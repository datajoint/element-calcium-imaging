{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "", "text": "Element Calcium Imaging \u00b6 DataJoint Element for functional calcium imaging with ScanImage , Scanbox , Nikon NIS , or PrairieView acquisition software and Suite2p or CaImAn analysis software. DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. Visit the Concepts page for more information on Element Calcium Imaging. To get started with building your data pipeline navigate to the Tutorials page.", "title": "Element Calcium Imaging"}, {"location": "#element-calcium-imaging", "text": "DataJoint Element for functional calcium imaging with ScanImage , Scanbox , Nikon NIS , or PrairieView acquisition software and Suite2p or CaImAn analysis software. DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. Visit the Concepts page for more information on Element Calcium Imaging. To get started with building your data pipeline navigate to the Tutorials page.", "title": "Element Calcium Imaging"}, {"location": "changelog/", "text": "Changelog \u00b6 Observes Semantic Versioning standard and Keep a Changelog convention. 0.4.2 - 2022-11-02 \u00b6 Bugfix - Add plotting package to the requirements to generate the figures Add - Scan date parser from nd2 files 0.4.1 - 2022-10-28 \u00b6 Update - Bump version to trigger PyPI release to revert updates from incorrect tag 0.4.0 - 2022-10-28 \u00b6 Add - New schema imaging_report to compute and store figures from results Add - Widget to display figures 0.3.0 - 2022-10-07 \u00b6 Add - Reader for Bruker PrairieView acquisition system 0.2.2 - 2022-09-28 \u00b6 Update - Minor table explanation edits Update - Query simplifications Update - Minor code refactoring 0.2.1 - 2022-09-12 \u00b6 Bugfix - fix errors in auto generating new ProcessingTask 0.2.0 - 2022-07-01 \u00b6 Add - Imaging module (imaging_preprocess.py) for pre-processing steps 0.1.0 - 2022-06-29 \u00b6 Add - Support for element-interface Add - Trigger Suite2p and CaImAn Add - Imaging module for no curation Add - Support for Nikon acquisition system Add - scan_datetime and scan_duration attributes Add - Estimate for scan duration Add - Citation section to README Update - Move background file to elements.datajoint.org Add - Adopt black formatting into code base 0.1.0b0 - 2021-05-07 \u00b6 Update - First beta release 0.1.0a4 - 2021-05-07 \u00b6 Update - Add workaround to handle DataJoint 0.13.* issue #914 0.1.0a3 - 2021-05-03 \u00b6 Add - GitHub Action release process Add - scan and imaging modules Add - Readers for ScanImage , ScanBox , Suite2p , CaImAn", "title": "Changelog"}, {"location": "changelog/#changelog", "text": "Observes Semantic Versioning standard and Keep a Changelog convention.", "title": "Changelog"}, {"location": "changelog/#042-2022-11-02", "text": "Bugfix - Add plotting package to the requirements to generate the figures Add - Scan date parser from nd2 files", "title": "0.4.2 - 2022-11-02"}, {"location": "changelog/#041-2022-10-28", "text": "Update - Bump version to trigger PyPI release to revert updates from incorrect tag", "title": "0.4.1 - 2022-10-28"}, {"location": "changelog/#040-2022-10-28", "text": "Add - New schema imaging_report to compute and store figures from results Add - Widget to display figures", "title": "0.4.0 - 2022-10-28"}, {"location": "changelog/#030-2022-10-07", "text": "Add - Reader for Bruker PrairieView acquisition system", "title": "0.3.0 - 2022-10-07"}, {"location": "changelog/#022-2022-09-28", "text": "Update - Minor table explanation edits Update - Query simplifications Update - Minor code refactoring", "title": "0.2.2 - 2022-09-28"}, {"location": "changelog/#021-2022-09-12", "text": "Bugfix - fix errors in auto generating new ProcessingTask", "title": "0.2.1 - 2022-09-12"}, {"location": "changelog/#020-2022-07-01", "text": "Add - Imaging module (imaging_preprocess.py) for pre-processing steps", "title": "0.2.0 - 2022-07-01"}, {"location": "changelog/#010-2022-06-29", "text": "Add - Support for element-interface Add - Trigger Suite2p and CaImAn Add - Imaging module for no curation Add - Support for Nikon acquisition system Add - scan_datetime and scan_duration attributes Add - Estimate for scan duration Add - Citation section to README Update - Move background file to elements.datajoint.org Add - Adopt black formatting into code base", "title": "0.1.0 - 2022-06-29"}, {"location": "changelog/#010b0-2021-05-07", "text": "Update - First beta release", "title": "0.1.0b0 - 2021-05-07"}, {"location": "changelog/#010a4-2021-05-07", "text": "Update - Add workaround to handle DataJoint 0.13.* issue #914", "title": "0.1.0a4 - 2021-05-07"}, {"location": "changelog/#010a3-2021-05-03", "text": "Add - GitHub Action release process Add - scan and imaging modules Add - Readers for ScanImage , ScanBox , Suite2p , CaImAn", "title": "0.1.0a3 - 2021-05-03"}, {"location": "citation/", "text": "Citation \u00b6 If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID). Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358 DataJoint Elements ( RRID:SCR_021894 ) - Element Calcium Imaging (version 0.4.2)", "title": "Citation"}, {"location": "citation/#citation", "text": "If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID). Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358 DataJoint Elements ( RRID:SCR_021894 ) - Element Calcium Imaging (version 0.4.2)", "title": "Citation"}, {"location": "concepts/", "text": "Concepts \u00b6 Multiphoton Calcium Imaging \u00b6 Over the past two decades, in vivo two-photon laser-scanning imaging of calcium signals has evolved into a mainstream modality for neurophysiology experiments to record population activity in intact neural circuits. The tools for signal acquisition and analysis continue to evolve but common patterns and elements of standardization have emerged. The preprocessing workflow for two-photon laser-scanning microscopy includes motion correction (rigid or non-rigid), cell segmentation, and calcium event extraction (sometimes described as \"deconvolution\" or \"spike inference\"). Some include raster artifact correction, cropping and stitching operations. Left to right: Raw scans, Motion corrected scans, Cell segmentation, Calcium events For a long time, most labs developed custom processing pipelines, sharing them with others as academic open-source projects. This has changed recently with the emerging of a few leaders as the standardization candidates for the initial preprocessing. CaImAn (Originally developed by Andrea Giovannucci, current support by FlatIron Institute: Eftychios A. Pnevmatikakis, Johannes Friedrich) Suite2p (Carsen Stringer and Marius Pachitariu at Janelia), 200+ users, active support Element Calcium Imaging encapsulates these packages to ease the management of data and its analysis. Key partnerships \u00b6 Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for two-photon Calcium imaging. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, pipeline design, associated tools, and interfaces. These teams include: MICrONS (Andreas Tolias Lab, BCM) - https://github.com/cajal BrainCoGs (Princeton) - https://github.com/BrainCOGS Moser Group (Kavli Institute/NTNU) - private repository Anne Churchland Lab (UCLA) Acquisition tools \u00b6 Hardware \u00b6 The primary acquisition systems are: Sutter Thorlabs Bruker Neurolabware We do not include Miniscopes in these estimates. In, all there are perhaps on the order of 3000 two-photon setups globally but their processing needs may need to be further segmented. Software \u00b6 ScanImage ThorImageLS Scanbox Nikon Vidrio\u2019s ScanImage is the data acquisition software for two types of home-built scanning two-photon systems, either based on Thorlabs and Sutter hardware. ScanImage has a free version and a licensed version. Thorlabs also provides their own acquisition software - ThorImageLS (probably half of the systems). Element Features \u00b6 Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Calcium Imaging Element with the repository hosted at https://github.com/datajoint/element-calcium-imaging. Major features of the Calcium Imaging Element include: Calcium-imaging scanning metadata, also compatible with mesoscale imaging and multi-ROI scanning mode Tables for all processing steps: motion correction, segmentation, cell spatial footprint, fluorescence trace extraction, spike inference and cell classification Store/track/manage different curations of the segmentation results Ingestion support for data acquired with ScanImage and Scanbox acquisition systems Ingestion support for processing outputs from both Suite2p and CaImAn analysis suites Sample data and complete test suite for quality assurance The processing workflow is typically performed on a per-scan basis, however, depending on the nature of the research questions, different labs may opt to perform processing/segmentation on a concatenated set of data from multiple scans. To this end, we have extended the Calcium Imaging Element and provided a design version capable of supporting a multi-scan processing scheme. Element Architecture \u00b6 Each node in the following diagram represents the analysis code in the workflow and the corresponding table in the database. Within the workflow, Element Calcium Imaging connects to upstream Elements including Lab, Animal, and Session. For more detailed documentation on each table, see the API docs for the respective schemas. The Element is composed of two main schemas, scan and imaging . To handle several use cases of this pipeline, we have designed two alternatives to imaging schemas, including imaging_no_curation and imaging_preprocess . imaging module - Multiple scans are acquired during each session and each scan is processed independently. imaging_no_curation module - Same as imaging module, without the Curation table. imaging_preprocess module - Same as imaging module. Additionally, pre-processing steps can be performed on each scan prior to processing with Suite2p or CaImAn. lab schema ( API docs ) \u00b6 Table Description Equipment Scanner metadata subject schema ( API docs ) \u00b6 Although not required, most choose to connect the Session table to a Subject table. Table Description Subject Basic information of the research subject session schema ( API docs ) \u00b6 Table Description Session Unique experimental session identifier scan schema ( API docs ) \u00b6 Table Description AcquisitionSoftware Software used in the acquisiton of the imaging scans Channel Recording Channel Scan A set of imaging scans perfomed in a single session ScanLocation Anatomical location of the region scanned ScanInfo Metadata of the imaging scan ScanInfo.Field Metadata of the fields imaged ScanInfo.ScanFile Path of the scan file imaging schema ( API docs ) \u00b6 Table Description ProcessingMethod Available analysis suites that can be used in processing of the imaging scans ProcessingParamSet All parameters required to process a calcium imaging scan CellCompartment Cell compartments that can be imaged MaskType Available labels for segmented masks ProcessingTask Task defined by a combination of Scan and ProcessingParamSet Processing The core table that executes a ProcessingTask Curation Curated results MotionCorrection Results of the motion correction procedure MotionCorrection.RigidMotionCorrection Details of the rigid motion correction performed on the imaging data MotionCorrection.NonRigidMotionCorrection Details of nonrigid motion correction performed on the imaging data MotionCorrection.NonRigidMotionCorrection.Block Results of non-rigid motion correction for each block MotionCorrection.Summary Summary images for each field and channel after motion corrections Segmentation Results of the segmentation Segmentation.Mask Masks identified in the segmentation procedure MaskClassificationMethod Method used in the mask classification procedure MaskClassification Result of the mask classification procedure MaskClassification.MaskType Type assigned to each mask Fluorescence Fluorescence measurements Fluorescence.Trace Fluorescence traces for each region of interest ActivityExtractionMethod Method used in activity extraction Activity Inferred neural activity Activity.Trace Inferred neural activity from fluorescence traces Roadmap \u00b6 Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will add the following features to this Element: Data quality metrics Data compression Deepinterpolation Data export to NWB Data publishing to DANDI", "title": "Concepts"}, {"location": "concepts/#concepts", "text": "", "title": "Concepts"}, {"location": "concepts/#multiphoton-calcium-imaging", "text": "Over the past two decades, in vivo two-photon laser-scanning imaging of calcium signals has evolved into a mainstream modality for neurophysiology experiments to record population activity in intact neural circuits. The tools for signal acquisition and analysis continue to evolve but common patterns and elements of standardization have emerged. The preprocessing workflow for two-photon laser-scanning microscopy includes motion correction (rigid or non-rigid), cell segmentation, and calcium event extraction (sometimes described as \"deconvolution\" or \"spike inference\"). Some include raster artifact correction, cropping and stitching operations. Left to right: Raw scans, Motion corrected scans, Cell segmentation, Calcium events For a long time, most labs developed custom processing pipelines, sharing them with others as academic open-source projects. This has changed recently with the emerging of a few leaders as the standardization candidates for the initial preprocessing. CaImAn (Originally developed by Andrea Giovannucci, current support by FlatIron Institute: Eftychios A. Pnevmatikakis, Johannes Friedrich) Suite2p (Carsen Stringer and Marius Pachitariu at Janelia), 200+ users, active support Element Calcium Imaging encapsulates these packages to ease the management of data and its analysis.", "title": "Multiphoton Calcium Imaging"}, {"location": "concepts/#key-partnerships", "text": "Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for two-photon Calcium imaging. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, pipeline design, associated tools, and interfaces. These teams include: MICrONS (Andreas Tolias Lab, BCM) - https://github.com/cajal BrainCoGs (Princeton) - https://github.com/BrainCOGS Moser Group (Kavli Institute/NTNU) - private repository Anne Churchland Lab (UCLA)", "title": "Key partnerships"}, {"location": "concepts/#acquisition-tools", "text": "", "title": "Acquisition tools"}, {"location": "concepts/#hardware", "text": "The primary acquisition systems are: Sutter Thorlabs Bruker Neurolabware We do not include Miniscopes in these estimates. In, all there are perhaps on the order of 3000 two-photon setups globally but their processing needs may need to be further segmented.", "title": "Hardware"}, {"location": "concepts/#software", "text": "ScanImage ThorImageLS Scanbox Nikon Vidrio\u2019s ScanImage is the data acquisition software for two types of home-built scanning two-photon systems, either based on Thorlabs and Sutter hardware. ScanImage has a free version and a licensed version. Thorlabs also provides their own acquisition software - ThorImageLS (probably half of the systems).", "title": "Software"}, {"location": "concepts/#element-features", "text": "Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Calcium Imaging Element with the repository hosted at https://github.com/datajoint/element-calcium-imaging. Major features of the Calcium Imaging Element include: Calcium-imaging scanning metadata, also compatible with mesoscale imaging and multi-ROI scanning mode Tables for all processing steps: motion correction, segmentation, cell spatial footprint, fluorescence trace extraction, spike inference and cell classification Store/track/manage different curations of the segmentation results Ingestion support for data acquired with ScanImage and Scanbox acquisition systems Ingestion support for processing outputs from both Suite2p and CaImAn analysis suites Sample data and complete test suite for quality assurance The processing workflow is typically performed on a per-scan basis, however, depending on the nature of the research questions, different labs may opt to perform processing/segmentation on a concatenated set of data from multiple scans. To this end, we have extended the Calcium Imaging Element and provided a design version capable of supporting a multi-scan processing scheme.", "title": "Element Features"}, {"location": "concepts/#element-architecture", "text": "Each node in the following diagram represents the analysis code in the workflow and the corresponding table in the database. Within the workflow, Element Calcium Imaging connects to upstream Elements including Lab, Animal, and Session. For more detailed documentation on each table, see the API docs for the respective schemas. The Element is composed of two main schemas, scan and imaging . To handle several use cases of this pipeline, we have designed two alternatives to imaging schemas, including imaging_no_curation and imaging_preprocess . imaging module - Multiple scans are acquired during each session and each scan is processed independently. imaging_no_curation module - Same as imaging module, without the Curation table. imaging_preprocess module - Same as imaging module. Additionally, pre-processing steps can be performed on each scan prior to processing with Suite2p or CaImAn.", "title": "Element Architecture"}, {"location": "concepts/#lab-schema-api-docs", "text": "Table Description Equipment Scanner metadata", "title": "lab schema (API docs)"}, {"location": "concepts/#subject-schema-api-docs", "text": "Although not required, most choose to connect the Session table to a Subject table. Table Description Subject Basic information of the research subject", "title": "subject schema (API docs)"}, {"location": "concepts/#session-schema-api-docs", "text": "Table Description Session Unique experimental session identifier", "title": "session schema (API docs)"}, {"location": "concepts/#scan-schema-api-docs", "text": "Table Description AcquisitionSoftware Software used in the acquisiton of the imaging scans Channel Recording Channel Scan A set of imaging scans perfomed in a single session ScanLocation Anatomical location of the region scanned ScanInfo Metadata of the imaging scan ScanInfo.Field Metadata of the fields imaged ScanInfo.ScanFile Path of the scan file", "title": "scan schema (API docs)"}, {"location": "concepts/#imaging-schema-api-docs", "text": "Table Description ProcessingMethod Available analysis suites that can be used in processing of the imaging scans ProcessingParamSet All parameters required to process a calcium imaging scan CellCompartment Cell compartments that can be imaged MaskType Available labels for segmented masks ProcessingTask Task defined by a combination of Scan and ProcessingParamSet Processing The core table that executes a ProcessingTask Curation Curated results MotionCorrection Results of the motion correction procedure MotionCorrection.RigidMotionCorrection Details of the rigid motion correction performed on the imaging data MotionCorrection.NonRigidMotionCorrection Details of nonrigid motion correction performed on the imaging data MotionCorrection.NonRigidMotionCorrection.Block Results of non-rigid motion correction for each block MotionCorrection.Summary Summary images for each field and channel after motion corrections Segmentation Results of the segmentation Segmentation.Mask Masks identified in the segmentation procedure MaskClassificationMethod Method used in the mask classification procedure MaskClassification Result of the mask classification procedure MaskClassification.MaskType Type assigned to each mask Fluorescence Fluorescence measurements Fluorescence.Trace Fluorescence traces for each region of interest ActivityExtractionMethod Method used in activity extraction Activity Inferred neural activity Activity.Trace Inferred neural activity from fluorescence traces", "title": "imaging schema (API docs)"}, {"location": "concepts/#roadmap", "text": "Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will add the following features to this Element: Data quality metrics Data compression Deepinterpolation Data export to NWB Data publishing to DANDI", "title": "Roadmap"}, {"location": "tutorials/", "text": "Tutorials \u00b6 Installation \u00b6 Installation of the Element requires an integrated development environment and database. Instructions to setup each of the components can be found on the User Instructions page. These instructions use the example workflow for Element Calcium Imaging , which can be modified for a user's specific experimental requirements. This example workflow uses four Elements (Lab, Animal, Session, and Calcium Imaging) to construct a complete pipeline, and is able to ingest experimental metadata and process calcium imaging scans. Videos \u00b6 Our YouTube tutorial gives an overview of the workflow files, notebooks, as well as core concepts related to calcium imaging analysis. To try out Elements notebooks in an online Jupyter environment with access to example data, visit CodeBook . (Calcium Imaging notebooks coming soon!) Notebooks \u00b6 Each of the notebooks in the workflow steps through ways to interact with the Element itself. 00-DataDownload highlights how to use DataJoint tools to download a sample model for trying out the Element. 01-Configure helps configure your local DataJoint installation to point to the correct database. 02-WorkflowStructure demonstrates the table architecture of the Element and key DataJoint basics for interacting with these tables. 03-Process steps through adding data to the tables and analyzing a calcium imaging scan. 04-Automate highlights the same steps as above, but utilizing all built-in automation tools. 05-Explore demonstrates the steps to fetch the results stored in the tables and plot them. 06-Drop provides the steps for dropping all the tables to start fresh. 07-DownStreamAnalysis demonstrates event- and trial-based analysis.", "title": "Tutorials"}, {"location": "tutorials/#tutorials", "text": "", "title": "Tutorials"}, {"location": "tutorials/#installation", "text": "Installation of the Element requires an integrated development environment and database. Instructions to setup each of the components can be found on the User Instructions page. These instructions use the example workflow for Element Calcium Imaging , which can be modified for a user's specific experimental requirements. This example workflow uses four Elements (Lab, Animal, Session, and Calcium Imaging) to construct a complete pipeline, and is able to ingest experimental metadata and process calcium imaging scans.", "title": "Installation"}, {"location": "tutorials/#videos", "text": "Our YouTube tutorial gives an overview of the workflow files, notebooks, as well as core concepts related to calcium imaging analysis. To try out Elements notebooks in an online Jupyter environment with access to example data, visit CodeBook . (Calcium Imaging notebooks coming soon!)", "title": "Videos"}, {"location": "tutorials/#notebooks", "text": "Each of the notebooks in the workflow steps through ways to interact with the Element itself. 00-DataDownload highlights how to use DataJoint tools to download a sample model for trying out the Element. 01-Configure helps configure your local DataJoint installation to point to the correct database. 02-WorkflowStructure demonstrates the table architecture of the Element and key DataJoint basics for interacting with these tables. 03-Process steps through adding data to the tables and analyzing a calcium imaging scan. 04-Automate highlights the same steps as above, but utilizing all built-in automation tools. 05-Explore demonstrates the steps to fetch the results stored in the tables and plot them. 06-Drop provides the steps for dropping all the tables to start fresh. 07-DownStreamAnalysis demonstrates event- and trial-based analysis.", "title": "Notebooks"}, {"location": "api/element_calcium_imaging/imaging/", "text": "Activity \u00b6 Bases: dj . Computed Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. ActivityExtractionMethod foreign key Primary key from ActivityExtractionMethod. Source code in element_calcium_imaging/imaging.py 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 @schema class Activity ( dj . Computed ): \"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Fluorescence (foreign key): Primary key from Fluorescence. ActivityExtractionMethod (foreign key): Primary key from ActivityExtractionMethod. \"\"\" definition = \"\"\"# Neural Activity -> Fluorescence -> ActivityExtractionMethod \"\"\" class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" @property def key_source ( self ): suite2p_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"suite2p\"' & 'extraction_method LIKE \"suite2p%\"' ) caiman_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"caiman\"' & 'extraction_method LIKE \"caiman%\"' ) return suite2p_key_source . proj () + caiman_key_source . proj () def make ( self , key ): \"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Trace \u00b6 Bases: dj . Part Trace(s) for each mask. Attributes: Name Type Description Activity foreign key Primary key from Activity. Fluorescence.Trace foreign key Fluorescence.Trace. activity_trace longblob Neural activity from fluoresence trace. Source code in element_calcium_imaging/imaging.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" make ( key ) \u00b6 Populate the Activity with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging.py 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 def make ( self , key ): \"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) ActivityExtractionMethod \u00b6 Bases: dj . Lookup Available activity extraction methods. Attributes: Name Type Description extraction_method str Extraction method. Source code in element_calcium_imaging/imaging.py 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 @schema class ActivityExtractionMethod ( dj . Lookup ): \"\"\"Available activity extraction methods. Attributes: extraction_method (str): Extraction method. \"\"\" definition = \"\"\"# Activity extraction method extraction_method: varchar(32) \"\"\" contents = zip ([ \"suite2p_deconvolution\" , \"caiman_deconvolution\" , \"caiman_dff\" ]) CellCompartment \u00b6 Bases: dj . Lookup Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: Name Type Description cell_compartment str Cell compartment. Source code in element_calcium_imaging/imaging.py 165 166 167 168 169 170 171 172 173 174 175 176 177 @schema class CellCompartment ( dj . Lookup ): \"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: cell_compartment (str): Cell compartment. \"\"\" definition = \"\"\"# Cell compartments cell_compartment: char(16) \"\"\" contents = zip ([ \"axon\" , \"soma\" , \"bouton\" ]) Curation \u00b6 Bases: dj . Manual Curated results If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir. Attributes: Name Type Description Processing foreign key Primary key from Processing. curation_id int Unique curation ID. curation_time datetime Time of generation of this set of curated results. curation_output_dir str Output directory of the curated results, relative to root data directory. manual_curation bool If True, manual curation has been performed on this result. curation_note str Notes about the curation task. Source code in element_calcium_imaging/imaging.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 @schema class Curation ( dj . Manual ): \"\"\"Curated results If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir. Attributes: Processing (foreign key): Primary key from Processing. curation_id (int): Unique curation ID. curation_time (datetime): Time of generation of this set of curated results. curation_output_dir (str): Output directory of the curated results, relative to root data directory. manual_curation (bool): If True, manual curation has been performed on this result. curation_note (str, optional): Notes about the curation task. \"\"\" definition = \"\"\"# Curation(s) results -> Processing curation_id: int --- curation_time: datetime # Time of generation of this set of curated results curation_output_dir: varchar(255) # Output directory of the curated results, relative to root data directory manual_curation: bool # Has manual curation been performed on this result? curation_note='': varchar(2000) \"\"\" def create1_from_processing_task ( self , key , is_curated = False , curation_note = \"\" ): \"\"\"Create a Curation entry for a given ProcessingTask key. Args: key (dict): Primary key set of an entry in the ProcessingTask table. is_curated (bool): When True, indicates a manual curation. curation_note (str): User's note on the specifics of the curation. \"\"\" if key not in Processing (): raise ValueError ( f \"No corresponding entry in Processing available for: { key } ;\" f \"Please run `Processing.populate(key)`\" ) output_dir = ( ProcessingTask & key ) . fetch1 ( \"processing_output_dir\" ) method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset curation_time = suite2p_dataset . creation_time elif method == \"caiman\" : caiman_dataset = imaging_dataset curation_time = caiman_dataset . creation_time else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : curation_time , \"curation_output_dir\" : output_dir , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) create1_from_processing_task ( key , is_curated = False , curation_note = '' ) \u00b6 Create a Curation entry for a given ProcessingTask key. Parameters: Name Type Description Default key dict Primary key set of an entry in the ProcessingTask table. required is_curated bool When True, indicates a manual curation. False curation_note str User's note on the specifics of the curation. '' Source code in element_calcium_imaging/imaging.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 def create1_from_processing_task ( self , key , is_curated = False , curation_note = \"\" ): \"\"\"Create a Curation entry for a given ProcessingTask key. Args: key (dict): Primary key set of an entry in the ProcessingTask table. is_curated (bool): When True, indicates a manual curation. curation_note (str): User's note on the specifics of the curation. \"\"\" if key not in Processing (): raise ValueError ( f \"No corresponding entry in Processing available for: { key } ;\" f \"Please run `Processing.populate(key)`\" ) output_dir = ( ProcessingTask & key ) . fetch1 ( \"processing_output_dir\" ) method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset curation_time = suite2p_dataset . creation_time elif method == \"caiman\" : caiman_dataset = imaging_dataset curation_time = caiman_dataset . creation_time else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : curation_time , \"curation_output_dir\" : output_dir , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) Fluorescence \u00b6 Bases: dj . Computed Fluorescence traces. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. Source code in element_calcium_imaging/imaging.py 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 @schema class Fluorescence ( dj . Computed ): \"\"\"Fluorescence traces. Attributes: Segmentation (foreign key): Primary key from Segmentation. \"\"\" definition = \"\"\"# Fluorescence traces before spike extraction or filtering -> Segmentation \"\"\" class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Trace \u00b6 Bases: dj . Part Traces obtained from segmented region of interests. Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. Segmentation.Mask foreign key Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') int The channel that this trace comes from. fluorescence longblob Fluorescence trace associated with this mask. neuropil_fluorescence longblob Neuropil fluorescence trace. Source code in element_calcium_imaging/imaging.py 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" make ( key ) \u00b6 Populate the Fluorescence with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging.py 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) MaskClassification \u00b6 Bases: dj . Computed Classes assigned to each mask. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. MaskClassificationMethod foreign key Primary key from MaskClassificationMethod. Source code in element_calcium_imaging/imaging.py 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 @schema class MaskClassification ( dj . Computed ): \"\"\"Classes assigned to each mask. Attributes: Segmentation (foreign key): Primary key from Segmentation. MaskClassificationMethod (foreign key): Primary key from MaskClassificationMethod. \"\"\" definition = \"\"\" -> Segmentation -> MaskClassificationMethod \"\"\" class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" def make ( self , key ): pass MaskType \u00b6 Bases: dj . Part Type assigned to each mask. Attributes: Name Type Description MaskClassification foreign key Primary key from MaskClassification. Segmentation.Mask foreign key Primary key from Segmentation.Mask. MaskType foreign key Primary key from MaskType. confidence float Confidence level of the mask classification. Source code in element_calcium_imaging/imaging.py 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" MaskClassificationMethod \u00b6 Bases: dj . Lookup Available mask classification methods. Attributes: Name Type Description mask_classification_method str Mask classification method. Source code in element_calcium_imaging/imaging.py 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 @schema class MaskClassificationMethod ( dj . Lookup ): \"\"\"Available mask classification methods. Attributes: mask_classification_method (str): Mask classification method. \"\"\" definition = \"\"\" mask_classification_method: varchar(48) \"\"\" contents = zip ([ \"suite2p_default_classifier\" , \"caiman_default_classifier\" ]) MaskType \u00b6 Bases: dj . Lookup Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: Name Type Description masky_type str Mask type. Source code in element_calcium_imaging/imaging.py 180 181 182 183 184 185 186 187 188 189 190 191 192 @schema class MaskType ( dj . Lookup ): \"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: masky_type (str): Mask type. \"\"\" definition = \"\"\"# Possible types of a segmented mask mask_type: varchar(16) \"\"\" contents = zip ([ \"soma\" , \"axon\" , \"dendrite\" , \"neuropil\" , \"artefact\" , \"unknown\" ]) MotionCorrection \u00b6 Bases: dj . Imported Results of motion correction shifts performed on the imaging data. Attributes: Name Type Description Curation foreign key Primary key from Curation. scan.Channel.proj(motion_correct_channel='channel') int Channel used for motion correction in this processing task. Source code in element_calcium_imaging/imaging.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 @schema class MotionCorrection ( dj . Imported ): \"\"\"Results of motion correction shifts performed on the imaging data. Attributes: Curation (foreign key): Primary key from Curation. scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for motion correction in this processing task. \"\"\" definition = \"\"\"# Results of motion correction -> Curation --- -> scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task \"\"\" class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Curation & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Block \u00b6 Bases: dj . Part FOV-tiled blocks used for non-rigid motion correction. Attributes: Name Type Description NonRigidMotionCorrection foreign key Primary key from NonRigidMotionCorrection. block_id int Unique block ID. block_y longblob # (y_start, y_end) in pixel of this block block_x longblob # (x_start, x_end) in pixel of this block block_z longblob # (z_start, z_end) in pixel of this block y_shifts longblob # (pixels) y motion correction shifts for every frame x_shifts longblob # (pixels) x motion correction shifts for every frame z_shifts=null longblob # (pixels) x motion correction shifts for every frame y_std float # (pixels) standard deviation of y shifts across all frames x_std float # (pixels) standard deviation of x shifts across all frames z_std=null float # (pixels) standard deviation of z shifts across all frames Source code in element_calcium_imaging/imaging.py 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" NonRigidMotionCorrection \u00b6 Bases: dj . Part Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob , null Mask with true for frames with outlier shifts (already corrected). block_height int Block height in pixels. block_width int Block width in pixels. block_depth int Block depth in pixels. block_count_y int Number of blocks tiled in the y direction. block_count_x int Number of blocks tiled in the x direction. block_count_z int Number of blocks tiled in the z direction. Source code in element_calcium_imaging/imaging.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" RigidMotionCorrection \u00b6 Bases: dj . Part Details of rigid motion correction performed on the imaging data. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob Mask with true for frames with outlier shifts (already corrected). y_shifts longblob y motion correction shifts (pixels). x_shifts longblob x motion correction shifts (pixels). z_shifts longblob z motion correction shifts (z-drift, pixels). y_std float standard deviation of y shifts across all frames (pixels). x_std float standard deviation of x shifts across all frames (pixels). z_std float standard deviation of z shifts across all frames (pixels). Source code in element_calcium_imaging/imaging.py 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" Summary \u00b6 Bases: dj . Part Summary images for each field and channel after corrections. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. scan.ScanInfo.Field foreign key Primary key from scan.ScanInfo.Field. ref_image longblob Image used as alignment template. average_image longblob Mean of registered frames. correlation_image longblob Correlation map (computed during cell detection). max_proj_image longblob Max of registered frames. Source code in element_calcium_imaging/imaging.py 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" make ( key ) \u00b6 Populate MotionCorrection with results parsed from analysis outputs Source code in element_calcium_imaging/imaging.py 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Curation & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Processing \u00b6 Bases: dj . Computed Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: Name Type Description ProcessingTask foreign key Primary key from ProcessingTask. processing_time datetime Process completion datetime. package_version str Version of the analysis package used in processing the data. Source code in element_calcium_imaging/imaging.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 @schema class Processing ( dj . Computed ): \"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: ProcessingTask (foreign key): Primary key from ProcessingTask. processing_time (datetime): Process completion datetime. package_version (str, optional): Version of the analysis package used in processing the data. \"\"\" definition = \"\"\" -> ProcessingTask --- processing_time : datetime # Time of generation of this set of processed, segmented results package_version='' : varchar(16) \"\"\" # Run processing only on Scan with ScanInfo inserted @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key ) key_source () property \u00b6 Limit the Processing to Scans that have their metadata ingested to the database. Source code in element_calcium_imaging/imaging.py 337 338 339 340 341 342 @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo make ( key ) \u00b6 Execute the calcium imaging analysis defined by the ProcessingTask. Source code in element_calcium_imaging/imaging.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key ) ProcessingMethod \u00b6 Bases: dj . Lookup Method, package, or analysis suite used for processing of calcium imaging data (e.g. Suite2p, CaImAn, etc.). Attributes: Name Type Description processing_method str Processing method. processing_method_desc str Processing method description. Source code in element_calcium_imaging/imaging.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @schema class ProcessingMethod ( dj . Lookup ): \"\"\"Method, package, or analysis suite used for processing of calcium imaging data (e.g. Suite2p, CaImAn, etc.). Attributes: processing_method (str): Processing method. processing_method_desc (str): Processing method description. \"\"\" definition = \"\"\"# Method for calcium imaging processing processing_method: char(8) --- processing_method_desc: varchar(1000) # Processing method description \"\"\" contents = [ ( \"suite2p\" , \"suite2p analysis suite\" ), ( \"caiman\" , \"caiman analysis suite\" ), ] ProcessingParamSet \u00b6 Bases: dj . Lookup Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: Name Type Description paramset_idx int Uniqiue parameter set ID. ProcessingMethod foreign key A primary key from ProcessingMethod. paramset_desc str Parameter set description. param_set_hash uuid A universally unique identifier for the parameter set. params longblob Parameter Set, a dictionary of all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 @schema class ProcessingParamSet ( dj . Lookup ): \"\"\"Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: paramset_idx (int): Uniqiue parameter set ID. ProcessingMethod (foreign key): A primary key from ProcessingMethod. paramset_desc (str): Parameter set description. param_set_hash (uuid): A universally unique identifier for the parameter set. params (longblob): Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" definition = \"\"\"# Processing Parameter Set paramset_idx: smallint # Uniqiue parameter set ID. --- -> ProcessingMethod paramset_desc: varchar(1280) # Parameter-set description param_set_hash: uuid # A universally unique identifier for the parameter set params: longblob # Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict ) insert_new_params ( processing_method , paramset_idx , paramset_desc , params ) classmethod \u00b6 Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: Name Type Description processing_method str Processing method/package used for processing of calcium imaging. paramset_idx int Uniqiue parameter set ID. paramset_desc str Parameter set description. params dict Parameter Set, all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict ) ProcessingTask \u00b6 Bases: dj . Manual A pairing of processing params and scans to be loaded or triggered This table defines a calcium imaging processing task for a combination of a Scan and a ProcessingParamSet entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: Name Type Description scan.Scan foreign key ProcessingParamSet foreign key processing_output_dir str task_mode str One of 'load' (load computed analysis results) or 'trigger' (trigger computation). Source code in element_calcium_imaging/imaging.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @schema class ProcessingTask ( dj . Manual ): \"\"\"A pairing of processing params and scans to be loaded or triggered This table defines a calcium imaging processing task for a combination of a `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: scan.Scan (foreign key): ProcessingParamSet (foreign key): processing_output_dir (str): task_mode (str): One of 'load' (load computed analysis results) or 'trigger' (trigger computation). \"\"\" definition = \"\"\"# Manual table for defining a processing task ready to be run -> scan.Scan -> ProcessingParamSet --- processing_output_dir: varchar(255) # Output directory of the processed scan relative to root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. Default False. mkdir (bool): If True, create the processing_output_dir directory. Default True. Returns: dir (str): A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) auto_generate_entries = generate generate ( scan_key , paramset_idx = 0 ) classmethod \u00b6 Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table. Parameters: Name Type Description Default scan_key dict Primary key from Scan table. required paramset_idx int Unique parameter set ID. 0 Source code in element_calcium_imaging/imaging.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) infer_output_dir ( key , relative = False , mkdir = False ) classmethod \u00b6 Infer an output directory for an entry in ProcessingTask table. Parameters: Name Type Description Default key dict Primary key from the ProcessingTask table. required relative bool If True, processing_output_dir is returned relative to imaging_root_dir. Default False. False mkdir bool If True, create the processing_output_dir directory. Default True. False Returns: Name Type Description dir str A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 Source code in element_calcium_imaging/imaging.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. Default False. mkdir (bool): If True, create the processing_output_dir directory. Default True. Returns: dir (str): A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir Segmentation \u00b6 Bases: dj . Computed Result of the Segmentation process. Attributes: Name Type Description Curation foreign key Primary key from Curation. Source code in element_calcium_imaging/imaging.py 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 @schema class Segmentation ( dj . Computed ): \"\"\"Result of the Segmentation process. Attributes: Curation (foreign key): Primary key from Curation. \"\"\" definition = \"\"\"# Different mask segmentations. -> Curation \"\"\" class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" ) Mask \u00b6 Bases: dj . Part Details of the masks identified from the Segmentation procedure. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. mask int Unique mask ID. scan.Channel.proj(segmentation_channel='channel') foreign key Channel used for segmentation. mask_npix int Number of pixels in ROIs. mask_center_x int Center x coordinate in pixel. mask_center_y int Center y coordinate in pixel. mask_center_z int Center z coordinate in pixel. mask_xpix longblob X coordinates in pixels. mask_ypix longblob Y coordinates in pixels. mask_zpix longblob Z coordinates in pixels. mask_weights longblob Weights of the mask at the indices above. Source code in element_calcium_imaging/imaging.py 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" make ( key ) \u00b6 Populate the Segmentation with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging.py 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" ) activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activate this schema. Parameters: Name Type Description Default imaging_schema_name str Schema name on the database server to activate the imaging module. required scan_schema_name str Schema name on the database server to activate the scan module. Omitted, if the scan module is already activated. None create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the imaging module: + all that are required by the scan module. None Dependencies: Upstream tables Session: A parent table to Scan, identifying a scanning session. Equipment: A parent table to Scan, identifying a scanning device. Source code in element_calcium_imaging/imaging.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None , ): \"\"\"Activate this schema. Args: imaging_schema_name (str): Schema name on the database server to activate the `imaging` module. scan_schema_name (str): Schema name on the database server to activate the `scan` module. Omitted, if the `scan` module is already activated. create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `imaging` module: + all that are required by the `scan` module. Dependencies: Upstream tables: + Session: A parent table to Scan, identifying a scanning session. + Equipment: A parent table to Scan, identifying a scanning device. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module scan . activate ( scan_schema_name , create_schema = create_schema , create_tables = create_tables , linking_module = linking_module , ) schema . activate ( imaging_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) imaging_report . activate ( f \" { imaging_schema_name } _report\" , imaging_schema_name ) get_loader_result ( key , table ) \u00b6 Retrieve the processed imaging results from a suite2p or caiman loader. Parameters: Name Type Description Default key dict The key to one entry of ProcessingTask or Curation required table dj . Table A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) required Raises: Type Description NotImplementedError If the processing_method is different than 'suite2p' or 'caiman'. Returns: Type Description A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) Source code in element_calcium_imaging/imaging.py 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 def get_loader_result ( key : dict , table : dj . Table ): \"\"\"Retrieve the processed imaging results from a suite2p or caiman loader. Args: key (dict): The `key` to one entry of ProcessingTask or Curation table (dj.Table): A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) Raises: NotImplementedError: If the processing_method is different than 'suite2p' or 'caiman'. Returns: A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) \"\"\" method , output_dir = ( ProcessingParamSet * table & key ) . fetch1 ( \"processing_method\" , _table_attribute_mapper [ table . __name__ ] ) output_path = find_full_path ( get_imaging_root_data_dir (), output_dir ) if method == \"suite2p\" : from element_interface import suite2p_loader loaded_dataset = suite2p_loader . Suite2p ( output_path ) elif method == \"caiman\" : from element_interface import caiman_loader loaded_dataset = caiman_loader . CaImAn ( output_path ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) return method , loaded_dataset", "title": "imaging.py"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity", "text": "Bases: dj . Computed Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. ActivityExtractionMethod foreign key Primary key from ActivityExtractionMethod. Source code in element_calcium_imaging/imaging.py 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 @schema class Activity ( dj . Computed ): \"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Fluorescence (foreign key): Primary key from Fluorescence. ActivityExtractionMethod (foreign key): Primary key from ActivityExtractionMethod. \"\"\" definition = \"\"\"# Neural Activity -> Fluorescence -> ActivityExtractionMethod \"\"\" class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" @property def key_source ( self ): suite2p_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"suite2p\"' & 'extraction_method LIKE \"suite2p%\"' ) caiman_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"caiman\"' & 'extraction_method LIKE \"caiman%\"' ) return suite2p_key_source . proj () + caiman_key_source . proj () def make ( self , key ): \"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "Activity"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity.Trace", "text": "Bases: dj . Part Trace(s) for each mask. Attributes: Name Type Description Activity foreign key Primary key from Activity. Fluorescence.Trace foreign key Fluorescence.Trace. activity_trace longblob Neural activity from fluoresence trace. Source code in element_calcium_imaging/imaging.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\"", "title": "Trace"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity.make", "text": "Populate the Activity with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging.py 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 def make ( self , key ): \"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ActivityExtractionMethod", "text": "Bases: dj . Lookup Available activity extraction methods. Attributes: Name Type Description extraction_method str Extraction method. Source code in element_calcium_imaging/imaging.py 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 @schema class ActivityExtractionMethod ( dj . Lookup ): \"\"\"Available activity extraction methods. Attributes: extraction_method (str): Extraction method. \"\"\" definition = \"\"\"# Activity extraction method extraction_method: varchar(32) \"\"\" contents = zip ([ \"suite2p_deconvolution\" , \"caiman_deconvolution\" , \"caiman_dff\" ])", "title": "ActivityExtractionMethod"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.CellCompartment", "text": "Bases: dj . Lookup Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: Name Type Description cell_compartment str Cell compartment. Source code in element_calcium_imaging/imaging.py 165 166 167 168 169 170 171 172 173 174 175 176 177 @schema class CellCompartment ( dj . Lookup ): \"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: cell_compartment (str): Cell compartment. \"\"\" definition = \"\"\"# Cell compartments cell_compartment: char(16) \"\"\" contents = zip ([ \"axon\" , \"soma\" , \"bouton\" ])", "title": "CellCompartment"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Curation", "text": "Bases: dj . Manual Curated results If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir. Attributes: Name Type Description Processing foreign key Primary key from Processing. curation_id int Unique curation ID. curation_time datetime Time of generation of this set of curated results. curation_output_dir str Output directory of the curated results, relative to root data directory. manual_curation bool If True, manual curation has been performed on this result. curation_note str Notes about the curation task. Source code in element_calcium_imaging/imaging.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 @schema class Curation ( dj . Manual ): \"\"\"Curated results If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir. Attributes: Processing (foreign key): Primary key from Processing. curation_id (int): Unique curation ID. curation_time (datetime): Time of generation of this set of curated results. curation_output_dir (str): Output directory of the curated results, relative to root data directory. manual_curation (bool): If True, manual curation has been performed on this result. curation_note (str, optional): Notes about the curation task. \"\"\" definition = \"\"\"# Curation(s) results -> Processing curation_id: int --- curation_time: datetime # Time of generation of this set of curated results curation_output_dir: varchar(255) # Output directory of the curated results, relative to root data directory manual_curation: bool # Has manual curation been performed on this result? curation_note='': varchar(2000) \"\"\" def create1_from_processing_task ( self , key , is_curated = False , curation_note = \"\" ): \"\"\"Create a Curation entry for a given ProcessingTask key. Args: key (dict): Primary key set of an entry in the ProcessingTask table. is_curated (bool): When True, indicates a manual curation. curation_note (str): User's note on the specifics of the curation. \"\"\" if key not in Processing (): raise ValueError ( f \"No corresponding entry in Processing available for: { key } ;\" f \"Please run `Processing.populate(key)`\" ) output_dir = ( ProcessingTask & key ) . fetch1 ( \"processing_output_dir\" ) method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset curation_time = suite2p_dataset . creation_time elif method == \"caiman\" : caiman_dataset = imaging_dataset curation_time = caiman_dataset . creation_time else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : curation_time , \"curation_output_dir\" : output_dir , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "Curation"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Curation.create1_from_processing_task", "text": "Create a Curation entry for a given ProcessingTask key. Parameters: Name Type Description Default key dict Primary key set of an entry in the ProcessingTask table. required is_curated bool When True, indicates a manual curation. False curation_note str User's note on the specifics of the curation. '' Source code in element_calcium_imaging/imaging.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 def create1_from_processing_task ( self , key , is_curated = False , curation_note = \"\" ): \"\"\"Create a Curation entry for a given ProcessingTask key. Args: key (dict): Primary key set of an entry in the ProcessingTask table. is_curated (bool): When True, indicates a manual curation. curation_note (str): User's note on the specifics of the curation. \"\"\" if key not in Processing (): raise ValueError ( f \"No corresponding entry in Processing available for: { key } ;\" f \"Please run `Processing.populate(key)`\" ) output_dir = ( ProcessingTask & key ) . fetch1 ( \"processing_output_dir\" ) method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset curation_time = suite2p_dataset . creation_time elif method == \"caiman\" : caiman_dataset = imaging_dataset curation_time = caiman_dataset . creation_time else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : curation_time , \"curation_output_dir\" : output_dir , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "create1_from_processing_task()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence", "text": "Bases: dj . Computed Fluorescence traces. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. Source code in element_calcium_imaging/imaging.py 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 @schema class Fluorescence ( dj . Computed ): \"\"\"Fluorescence traces. Attributes: Segmentation (foreign key): Primary key from Segmentation. \"\"\" definition = \"\"\"# Fluorescence traces before spike extraction or filtering -> Segmentation \"\"\" class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "Fluorescence"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence.Trace", "text": "Bases: dj . Part Traces obtained from segmented region of interests. Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. Segmentation.Mask foreign key Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') int The channel that this trace comes from. fluorescence longblob Fluorescence trace associated with this mask. neuropil_fluorescence longblob Neuropil fluorescence trace. Source code in element_calcium_imaging/imaging.py 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\"", "title": "Trace"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence.make", "text": "Populate the Fluorescence with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging.py 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassification", "text": "Bases: dj . Computed Classes assigned to each mask. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. MaskClassificationMethod foreign key Primary key from MaskClassificationMethod. Source code in element_calcium_imaging/imaging.py 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 @schema class MaskClassification ( dj . Computed ): \"\"\"Classes assigned to each mask. Attributes: Segmentation (foreign key): Primary key from Segmentation. MaskClassificationMethod (foreign key): Primary key from MaskClassificationMethod. \"\"\" definition = \"\"\" -> Segmentation -> MaskClassificationMethod \"\"\" class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" def make ( self , key ): pass", "title": "MaskClassification"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassification.MaskType", "text": "Bases: dj . Part Type assigned to each mask. Attributes: Name Type Description MaskClassification foreign key Primary key from MaskClassification. Segmentation.Mask foreign key Primary key from Segmentation.Mask. MaskType foreign key Primary key from MaskType. confidence float Confidence level of the mask classification. Source code in element_calcium_imaging/imaging.py 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\"", "title": "MaskType"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassificationMethod", "text": "Bases: dj . Lookup Available mask classification methods. Attributes: Name Type Description mask_classification_method str Mask classification method. Source code in element_calcium_imaging/imaging.py 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 @schema class MaskClassificationMethod ( dj . Lookup ): \"\"\"Available mask classification methods. Attributes: mask_classification_method (str): Mask classification method. \"\"\" definition = \"\"\" mask_classification_method: varchar(48) \"\"\" contents = zip ([ \"suite2p_default_classifier\" , \"caiman_default_classifier\" ])", "title": "MaskClassificationMethod"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskType", "text": "Bases: dj . Lookup Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: Name Type Description masky_type str Mask type. Source code in element_calcium_imaging/imaging.py 180 181 182 183 184 185 186 187 188 189 190 191 192 @schema class MaskType ( dj . Lookup ): \"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: masky_type (str): Mask type. \"\"\" definition = \"\"\"# Possible types of a segmented mask mask_type: varchar(16) \"\"\" contents = zip ([ \"soma\" , \"axon\" , \"dendrite\" , \"neuropil\" , \"artefact\" , \"unknown\" ])", "title": "MaskType"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection", "text": "Bases: dj . Imported Results of motion correction shifts performed on the imaging data. Attributes: Name Type Description Curation foreign key Primary key from Curation. scan.Channel.proj(motion_correct_channel='channel') int Channel used for motion correction in this processing task. Source code in element_calcium_imaging/imaging.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 @schema class MotionCorrection ( dj . Imported ): \"\"\"Results of motion correction shifts performed on the imaging data. Attributes: Curation (foreign key): Primary key from Curation. scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for motion correction in this processing task. \"\"\" definition = \"\"\"# Results of motion correction -> Curation --- -> scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task \"\"\" class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Curation & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "MotionCorrection"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.Block", "text": "Bases: dj . Part FOV-tiled blocks used for non-rigid motion correction. Attributes: Name Type Description NonRigidMotionCorrection foreign key Primary key from NonRigidMotionCorrection. block_id int Unique block ID. block_y longblob # (y_start, y_end) in pixel of this block block_x longblob # (x_start, x_end) in pixel of this block block_z longblob # (z_start, z_end) in pixel of this block y_shifts longblob # (pixels) y motion correction shifts for every frame x_shifts longblob # (pixels) x motion correction shifts for every frame z_shifts=null longblob # (pixels) x motion correction shifts for every frame y_std float # (pixels) standard deviation of y shifts across all frames x_std float # (pixels) standard deviation of x shifts across all frames z_std=null float # (pixels) standard deviation of z shifts across all frames Source code in element_calcium_imaging/imaging.py 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\"", "title": "Block"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.NonRigidMotionCorrection", "text": "Bases: dj . Part Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob , null Mask with true for frames with outlier shifts (already corrected). block_height int Block height in pixels. block_width int Block width in pixels. block_depth int Block depth in pixels. block_count_y int Number of blocks tiled in the y direction. block_count_x int Number of blocks tiled in the x direction. block_count_z int Number of blocks tiled in the z direction. Source code in element_calcium_imaging/imaging.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\"", "title": "NonRigidMotionCorrection"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.RigidMotionCorrection", "text": "Bases: dj . Part Details of rigid motion correction performed on the imaging data. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob Mask with true for frames with outlier shifts (already corrected). y_shifts longblob y motion correction shifts (pixels). x_shifts longblob x motion correction shifts (pixels). z_shifts longblob z motion correction shifts (z-drift, pixels). y_std float standard deviation of y shifts across all frames (pixels). x_std float standard deviation of x shifts across all frames (pixels). z_std float standard deviation of z shifts across all frames (pixels). Source code in element_calcium_imaging/imaging.py 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\"", "title": "RigidMotionCorrection"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.Summary", "text": "Bases: dj . Part Summary images for each field and channel after corrections. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. scan.ScanInfo.Field foreign key Primary key from scan.ScanInfo.Field. ref_image longblob Image used as alignment template. average_image longblob Mean of registered frames. correlation_image longblob Correlation map (computed during cell detection). max_proj_image longblob Max of registered frames. Source code in element_calcium_imaging/imaging.py 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\"", "title": "Summary"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.make", "text": "Populate MotionCorrection with results parsed from analysis outputs Source code in element_calcium_imaging/imaging.py 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Curation & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing", "text": "Bases: dj . Computed Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: Name Type Description ProcessingTask foreign key Primary key from ProcessingTask. processing_time datetime Process completion datetime. package_version str Version of the analysis package used in processing the data. Source code in element_calcium_imaging/imaging.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 @schema class Processing ( dj . Computed ): \"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: ProcessingTask (foreign key): Primary key from ProcessingTask. processing_time (datetime): Process completion datetime. package_version (str, optional): Version of the analysis package used in processing the data. \"\"\" definition = \"\"\" -> ProcessingTask --- processing_time : datetime # Time of generation of this set of processed, segmented results package_version='' : varchar(16) \"\"\" # Run processing only on Scan with ScanInfo inserted @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key )", "title": "Processing"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing.key_source", "text": "Limit the Processing to Scans that have their metadata ingested to the database. Source code in element_calcium_imaging/imaging.py 337 338 339 340 341 342 @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo", "title": "key_source()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing.make", "text": "Execute the calcium imaging analysis defined by the ProcessingTask. Source code in element_calcium_imaging/imaging.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key )", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingMethod", "text": "Bases: dj . Lookup Method, package, or analysis suite used for processing of calcium imaging data (e.g. Suite2p, CaImAn, etc.). Attributes: Name Type Description processing_method str Processing method. processing_method_desc str Processing method description. Source code in element_calcium_imaging/imaging.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @schema class ProcessingMethod ( dj . Lookup ): \"\"\"Method, package, or analysis suite used for processing of calcium imaging data (e.g. Suite2p, CaImAn, etc.). Attributes: processing_method (str): Processing method. processing_method_desc (str): Processing method description. \"\"\" definition = \"\"\"# Method for calcium imaging processing processing_method: char(8) --- processing_method_desc: varchar(1000) # Processing method description \"\"\" contents = [ ( \"suite2p\" , \"suite2p analysis suite\" ), ( \"caiman\" , \"caiman analysis suite\" ), ]", "title": "ProcessingMethod"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingParamSet", "text": "Bases: dj . Lookup Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: Name Type Description paramset_idx int Uniqiue parameter set ID. ProcessingMethod foreign key A primary key from ProcessingMethod. paramset_desc str Parameter set description. param_set_hash uuid A universally unique identifier for the parameter set. params longblob Parameter Set, a dictionary of all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 @schema class ProcessingParamSet ( dj . Lookup ): \"\"\"Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: paramset_idx (int): Uniqiue parameter set ID. ProcessingMethod (foreign key): A primary key from ProcessingMethod. paramset_desc (str): Parameter set description. param_set_hash (uuid): A universally unique identifier for the parameter set. params (longblob): Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" definition = \"\"\"# Processing Parameter Set paramset_idx: smallint # Uniqiue parameter set ID. --- -> ProcessingMethod paramset_desc: varchar(1280) # Parameter-set description param_set_hash: uuid # A universally unique identifier for the parameter set params: longblob # Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict )", "title": "ProcessingParamSet"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingParamSet.insert_new_params", "text": "Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: Name Type Description processing_method str Processing method/package used for processing of calcium imaging. paramset_idx int Uniqiue parameter set ID. paramset_desc str Parameter set description. params dict Parameter Set, all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict )", "title": "insert_new_params()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask", "text": "Bases: dj . Manual A pairing of processing params and scans to be loaded or triggered This table defines a calcium imaging processing task for a combination of a Scan and a ProcessingParamSet entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: Name Type Description scan.Scan foreign key ProcessingParamSet foreign key processing_output_dir str task_mode str One of 'load' (load computed analysis results) or 'trigger' (trigger computation). Source code in element_calcium_imaging/imaging.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 @schema class ProcessingTask ( dj . Manual ): \"\"\"A pairing of processing params and scans to be loaded or triggered This table defines a calcium imaging processing task for a combination of a `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: scan.Scan (foreign key): ProcessingParamSet (foreign key): processing_output_dir (str): task_mode (str): One of 'load' (load computed analysis results) or 'trigger' (trigger computation). \"\"\" definition = \"\"\"# Manual table for defining a processing task ready to be run -> scan.Scan -> ProcessingParamSet --- processing_output_dir: varchar(255) # Output directory of the processed scan relative to root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. Default False. mkdir (bool): If True, create the processing_output_dir directory. Default True. Returns: dir (str): A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) auto_generate_entries = generate", "title": "ProcessingTask"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask.generate", "text": "Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table. Parameters: Name Type Description Default scan_key dict Primary key from Scan table. required paramset_idx int Unique parameter set ID. 0 Source code in element_calcium_imaging/imaging.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } )", "title": "generate()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask.infer_output_dir", "text": "Infer an output directory for an entry in ProcessingTask table. Parameters: Name Type Description Default key dict Primary key from the ProcessingTask table. required relative bool If True, processing_output_dir is returned relative to imaging_root_dir. Default False. False mkdir bool If True, create the processing_output_dir directory. Default True. False Returns: Name Type Description dir str A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 Source code in element_calcium_imaging/imaging.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. Default False. mkdir (bool): If True, create the processing_output_dir directory. Default True. Returns: dir (str): A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir", "title": "infer_output_dir()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation", "text": "Bases: dj . Computed Result of the Segmentation process. Attributes: Name Type Description Curation foreign key Primary key from Curation. Source code in element_calcium_imaging/imaging.py 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 @schema class Segmentation ( dj . Computed ): \"\"\"Result of the Segmentation process. Attributes: Curation (foreign key): Primary key from Curation. \"\"\" definition = \"\"\"# Different mask segmentations. -> Curation \"\"\" class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" )", "title": "Segmentation"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation.Mask", "text": "Bases: dj . Part Details of the masks identified from the Segmentation procedure. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. mask int Unique mask ID. scan.Channel.proj(segmentation_channel='channel') foreign key Channel used for segmentation. mask_npix int Number of pixels in ROIs. mask_center_x int Center x coordinate in pixel. mask_center_y int Center y coordinate in pixel. mask_center_z int Center z coordinate in pixel. mask_xpix longblob X coordinates in pixels. mask_ypix longblob Y coordinates in pixels. mask_zpix longblob Z coordinates in pixels. mask_weights longblob Weights of the mask at the indices above. Source code in element_calcium_imaging/imaging.py 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\"", "title": "Mask"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation.make", "text": "Populate the Segmentation with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging.py 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" )", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.activate", "text": "Activate this schema. Parameters: Name Type Description Default imaging_schema_name str Schema name on the database server to activate the imaging module. required scan_schema_name str Schema name on the database server to activate the scan module. Omitted, if the scan module is already activated. None create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the imaging module: + all that are required by the scan module. None Dependencies: Upstream tables Session: A parent table to Scan, identifying a scanning session. Equipment: A parent table to Scan, identifying a scanning device. Source code in element_calcium_imaging/imaging.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None , ): \"\"\"Activate this schema. Args: imaging_schema_name (str): Schema name on the database server to activate the `imaging` module. scan_schema_name (str): Schema name on the database server to activate the `scan` module. Omitted, if the `scan` module is already activated. create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `imaging` module: + all that are required by the `scan` module. Dependencies: Upstream tables: + Session: A parent table to Scan, identifying a scanning session. + Equipment: A parent table to Scan, identifying a scanning device. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module scan . activate ( scan_schema_name , create_schema = create_schema , create_tables = create_tables , linking_module = linking_module , ) schema . activate ( imaging_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) imaging_report . activate ( f \" { imaging_schema_name } _report\" , imaging_schema_name )", "title": "activate()"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_loader_result", "text": "Retrieve the processed imaging results from a suite2p or caiman loader. Parameters: Name Type Description Default key dict The key to one entry of ProcessingTask or Curation required table dj . Table A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) required Raises: Type Description NotImplementedError If the processing_method is different than 'suite2p' or 'caiman'. Returns: Type Description A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) Source code in element_calcium_imaging/imaging.py 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 def get_loader_result ( key : dict , table : dj . Table ): \"\"\"Retrieve the processed imaging results from a suite2p or caiman loader. Args: key (dict): The `key` to one entry of ProcessingTask or Curation table (dj.Table): A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) Raises: NotImplementedError: If the processing_method is different than 'suite2p' or 'caiman'. Returns: A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) \"\"\" method , output_dir = ( ProcessingParamSet * table & key ) . fetch1 ( \"processing_method\" , _table_attribute_mapper [ table . __name__ ] ) output_path = find_full_path ( get_imaging_root_data_dir (), output_dir ) if method == \"suite2p\" : from element_interface import suite2p_loader loaded_dataset = suite2p_loader . Suite2p ( output_path ) elif method == \"caiman\" : from element_interface import caiman_loader loaded_dataset = caiman_loader . CaImAn ( output_path ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) return method , loaded_dataset", "title": "get_loader_result()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/", "text": "Activity \u00b6 Bases: dj . Computed Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. ActivityExtractionMethod foreign key Primary key from ActivityExtractionMethod. Source code in element_calcium_imaging/imaging_no_curation.py 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 @schema class Activity ( dj . Computed ): \"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Fluorescence (foreign key): Primary key from Fluorescence. ActivityExtractionMethod (foreign key): Primary key from ActivityExtractionMethod. \"\"\" definition = \"\"\"# Neural Activity -> Fluorescence -> ActivityExtractionMethod \"\"\" class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" @property def key_source ( self ): suite2p_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"suite2p\"' & 'extraction_method LIKE \"suite2p%\"' ) caiman_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"caiman\"' & 'extraction_method LIKE \"caiman%\"' ) return suite2p_key_source . proj () + caiman_key_source . proj () def make ( self , key ): \"\"\" Populate the Activity with the results parsed from analysis outputs. \"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Trace \u00b6 Bases: dj . Part Trace(s) for each mask. Attributes: Name Type Description Activity foreign key Primary key from Activity. Fluorescence.Trace foreign key Fluorescence.Trace. activity_trace longblob Neural activity from fluoresence trace. Source code in element_calcium_imaging/imaging_no_curation.py 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" make ( key ) \u00b6 Populate the Activity with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_no_curation.py 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 def make ( self , key ): \"\"\" Populate the Activity with the results parsed from analysis outputs. \"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) ActivityExtractionMethod \u00b6 Bases: dj . Lookup Available activity extraction methods. Attributes: Name Type Description extraction_method str Extraction method. Source code in element_calcium_imaging/imaging_no_curation.py 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 @schema class ActivityExtractionMethod ( dj . Lookup ): \"\"\"Available activity extraction methods. Attributes: extraction_method (str): Extraction method. \"\"\" definition = \"\"\"# Activity extraction method extraction_method: varchar(32) \"\"\" contents = zip ([ \"suite2p_deconvolution\" , \"caiman_deconvolution\" , \"caiman_dff\" ]) CellCompartment \u00b6 Bases: dj . Lookup Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: Name Type Description cell_compartment str Cell compartment. Source code in element_calcium_imaging/imaging_no_curation.py 164 165 166 167 168 169 170 171 172 173 174 175 176 @schema class CellCompartment ( dj . Lookup ): \"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: cell_compartment (str): Cell compartment. \"\"\" definition = \"\"\"# Cell compartments cell_compartment: char(16) \"\"\" contents = zip ([ \"axon\" , \"soma\" , \"bouton\" ]) Fluorescence \u00b6 Bases: dj . Computed Fluorescence traces. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. Source code in element_calcium_imaging/imaging_no_curation.py 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 @schema class Fluorescence ( dj . Computed ): \"\"\"Fluorescence traces. Attributes: Segmentation (foreign key): Primary key from Segmentation. \"\"\" definition = \"\"\"# Fluorescence traces before spike extraction or filtering -> Segmentation \"\"\" class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Trace \u00b6 Bases: dj . Part Traces obtained from segmented region of interests. Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. Segmentation.Mask foreign key Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') int The channel that this trace comes from. fluorescence longblob Fluorescence trace associated with this mask. neuropil_fluorescence longblob Neuropil fluorescence trace. Source code in element_calcium_imaging/imaging_no_curation.py 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" make ( key ) \u00b6 Populate the Fluorescence with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_no_curation.py 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) MaskClassification \u00b6 Bases: dj . Computed Classes assigned to each mask. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. MaskClassificationMethod foreign key Primary key from MaskClassificationMethod. Source code in element_calcium_imaging/imaging_no_curation.py 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 @schema class MaskClassification ( dj . Computed ): \"\"\"Classes assigned to each mask. Attributes: Segmentation (foreign key): Primary key from Segmentation. MaskClassificationMethod (foreign key): Primary key from MaskClassificationMethod. \"\"\" definition = \"\"\" -> Segmentation -> MaskClassificationMethod \"\"\" class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" def make ( self , key ): pass MaskType \u00b6 Bases: dj . Part Type assigned to each mask. Attributes: Name Type Description MaskClassification foreign key Primary key from MaskClassification. Segmentation.Mask foreign key Primary key from Segmentation.Mask. MaskType foreign key Primary key from MaskType. confidence float Confidence level of the mask classification. Source code in element_calcium_imaging/imaging_no_curation.py 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" MaskClassificationMethod \u00b6 Bases: dj . Lookup Available mask classification methods. Attributes: Name Type Description mask_classification_method str Mask classification method. Source code in element_calcium_imaging/imaging_no_curation.py 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 @schema class MaskClassificationMethod ( dj . Lookup ): \"\"\"Available mask classification methods. Attributes: mask_classification_method (str): Mask classification method. \"\"\" definition = \"\"\" mask_classification_method: varchar(48) \"\"\" contents = zip ([ \"suite2p_default_classifier\" , \"caiman_default_classifier\" ]) MaskType \u00b6 Bases: dj . Lookup Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: Name Type Description masky_type str Mask type. Source code in element_calcium_imaging/imaging_no_curation.py 179 180 181 182 183 184 185 186 187 188 189 190 191 @schema class MaskType ( dj . Lookup ): \"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: masky_type (str): Mask type. \"\"\" definition = \"\"\"# Possible types of a segmented mask mask_type: varchar(16) \"\"\" contents = zip ([ \"soma\" , \"axon\" , \"dendrite\" , \"neuropil\" , \"artefact\" , \"unknown\" ]) MotionCorrection \u00b6 Bases: dj . Imported Results of motion correction shifts performed on the imaging data. Attributes: Name Type Description Processing foreign key Primary key from Processing. scan.Channel.proj(motion_correct_channel='channel') int Channel used for motion correction in this processing task. Source code in element_calcium_imaging/imaging_no_curation.py 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 @schema class MotionCorrection ( dj . Imported ): \"\"\"Results of motion correction shifts performed on the imaging data. Attributes: Processing (foreign key): Primary key from Processing. scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for motion correction in this processing task. \"\"\" definition = \"\"\"# Results of motion correction -> Processing --- -> scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task \"\"\" class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y (longblob): y_start and y_end in pixels for this block block_x (longblob): x_start and x_end in pixels for this block block_z (longblob): z_start and z_end in pixels for this block y_shifts (longblob): y motion correction shifts for every frame in pixels x_shifts (longblob): x motion correction shifts for every frame in pixels z_shifts (longblob, optional): x motion correction shifts for every frame in pixels y_std (float): standard deviation of y shifts across all frames in pixels x_std (float): standard deviation of x shifts across all frames in pixels z_std (float, optional): standard deviation of z shifts across all frames in pixels \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Processing & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Block \u00b6 Bases: dj . Part FOV-tiled blocks used for non-rigid motion correction. Attributes: Name Type Description NonRigidMotionCorrection foreign key Primary key from NonRigidMotionCorrection. block_id int Unique block ID. block_y longblob y_start and y_end in pixels for this block block_x longblob x_start and x_end in pixels for this block block_z longblob z_start and z_end in pixels for this block y_shifts longblob y motion correction shifts for every frame in pixels x_shifts longblob x motion correction shifts for every frame in pixels z_shifts longblob x motion correction shifts for every frame in pixels y_std float standard deviation of y shifts across all frames in pixels x_std float standard deviation of x shifts across all frames in pixels z_std float standard deviation of z shifts across all frames in pixels Source code in element_calcium_imaging/imaging_no_curation.py 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y (longblob): y_start and y_end in pixels for this block block_x (longblob): x_start and x_end in pixels for this block block_z (longblob): z_start and z_end in pixels for this block y_shifts (longblob): y motion correction shifts for every frame in pixels x_shifts (longblob): x motion correction shifts for every frame in pixels z_shifts (longblob, optional): x motion correction shifts for every frame in pixels y_std (float): standard deviation of y shifts across all frames in pixels x_std (float): standard deviation of x shifts across all frames in pixels z_std (float, optional): standard deviation of z shifts across all frames in pixels \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" NonRigidMotionCorrection \u00b6 Bases: dj . Part Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob , null Mask with true for frames with outlier shifts (already corrected). block_height int Block height in pixels. block_width int Block width in pixels. block_depth int Block depth in pixels. block_count_y int Number of blocks tiled in the y direction. block_count_x int Number of blocks tiled in the x direction. block_count_z int Number of blocks tiled in the z direction. Source code in element_calcium_imaging/imaging_no_curation.py 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" RigidMotionCorrection \u00b6 Bases: dj . Part Details of rigid motion correction performed on the imaging data. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob Mask with true for frames with outlier shifts (already corrected). y_shifts longblob y motion correction shifts (pixels). x_shifts longblob x motion correction shifts (pixels). z_shifts longblob z motion correction shifts (z-drift, pixels). y_std float standard deviation of y shifts across all frames (pixels). x_std float standard deviation of x shifts across all frames (pixels). z_std float standard deviation of z shifts across all frames (pixels). Source code in element_calcium_imaging/imaging_no_curation.py 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" Summary \u00b6 Bases: dj . Part Summary images for each field and channel after corrections. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. scan.ScanInfo.Field foreign key Primary key from scan.ScanInfo.Field. ref_image longblob Image used as alignment template. average_image longblob Mean of registered frames. correlation_image longblob Correlation map (computed during cell detection). max_proj_image longblob Max of registered frames. Source code in element_calcium_imaging/imaging_no_curation.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" make ( key ) \u00b6 Populate MotionCorrection with results parsed from analysis outputs Source code in element_calcium_imaging/imaging_no_curation.py 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Processing & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Processing \u00b6 Bases: dj . Computed Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: Name Type Description ProcessingTask foreign key Primary key from ProcessingTask. processing_time datetime Process completion datetime. package_version str Version of the analysis package used in processing the data. Source code in element_calcium_imaging/imaging_no_curation.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 @schema class Processing ( dj . Computed ): \"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: ProcessingTask (foreign key): Primary key from ProcessingTask. processing_time (datetime): Process completion datetime. package_version (str, optional): Version of the analysis package used in processing the data. \"\"\" definition = \"\"\" -> ProcessingTask --- processing_time : datetime # Time of generation of this set of processed, segmented results package_version='' : varchar(16) \"\"\" # Run processing only on Scan with ScanInfo inserted @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key ) key_source () property \u00b6 Limit the Processing to Scans that have their metadata ingested to the database. Source code in element_calcium_imaging/imaging_no_curation.py 331 332 333 334 335 336 @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo make ( key ) \u00b6 Execute the calcium imaging analysis defined by the ProcessingTask. Source code in element_calcium_imaging/imaging_no_curation.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key ) ProcessingMethod \u00b6 Bases: dj . Lookup Method, package, or analysis suite used for processing of calcium imaging data (e.g. Suite2p, CaImAn, etc.). Attributes: Name Type Description processing_method str Processing method. processing_method_desc str Processing method description. Source code in element_calcium_imaging/imaging_no_curation.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @schema class ProcessingMethod ( dj . Lookup ): \"\"\"Method, package, or analysis suite used for processing of calcium imaging data (e.g. Suite2p, CaImAn, etc.). Attributes: processing_method (str): Processing method. processing_method_desc (str): Processing method description. \"\"\" definition = \"\"\"# Method for calcium imaging processing processing_method: char(8) --- processing_method_desc: varchar(1000) # Processing method description \"\"\" contents = [ ( \"suite2p\" , \"suite2p analysis suite\" ), ( \"caiman\" , \"caiman analysis suite\" ), ] ProcessingParamSet \u00b6 Bases: dj . Lookup Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: Name Type Description paramset_idx int Uniqiue parameter set ID. ProcessingMethod foreign key A primary key from ProcessingMethod. paramset_desc str Parameter set description. param_set_hash uuid A universally unique identifier for the parameter set. params longblob Parameter Set, a dictionary of all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_no_curation.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @schema class ProcessingParamSet ( dj . Lookup ): \"\"\"Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: paramset_idx (int): Uniqiue parameter set ID. ProcessingMethod (foreign key): A primary key from ProcessingMethod. paramset_desc (str): Parameter set description. param_set_hash (uuid): A universally unique identifier for the parameter set. params (longblob): Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" definition = \"\"\"# Processing Parameter Set paramset_idx: smallint # Uniqiue parameter set ID. --- -> ProcessingMethod paramset_desc: varchar(1280) # Parameter-set description param_set_hash: uuid # A universally unique identifier for the parameter set params: longblob # Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict ) insert_new_params ( processing_method , paramset_idx , paramset_desc , params ) classmethod \u00b6 Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: Name Type Description processing_method str Processing method/package used for processing of calcium imaging. paramset_idx int Uniqiue parameter set ID. paramset_desc str Parameter set description. params dict Parameter Set, all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_no_curation.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict ) ProcessingTask \u00b6 Bases: dj . Manual This table defines a calcium imaging processing task for a combination of a Scan and a ProcessingParamSet entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: Name Type Description scan.Scan foreign key ProcessingParamSet foreign key processing_output_dir str task_mode str One of 'load' (load computed analysis results) or 'trigger' (trigger computation). Source code in element_calcium_imaging/imaging_no_curation.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @schema class ProcessingTask ( dj . Manual ): \"\"\"This table defines a calcium imaging processing task for a combination of a `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: scan.Scan (foreign key): ProcessingParamSet (foreign key): processing_output_dir (str): task_mode (str): One of 'load' (load computed analysis results) or 'trigger' (trigger computation). \"\"\" definition = \"\"\"# Manual table for defining a processing task ready to be run -> scan.Scan -> ProcessingParamSet --- processing_output_dir: varchar(255) # Output directory of the processed scan relative to root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. mkdir (bool): If True, create the processing_output_dir directory. Returns: A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) auto_generate_entries = generate generate ( scan_key , paramset_idx = 0 ) classmethod \u00b6 Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Parameters: Name Type Description Default scan_key dict Primary key from Scan table. required paramset_idx int Unique parameter set ID. 0 Source code in element_calcium_imaging/imaging_no_curation.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) infer_output_dir ( key , relative = False , mkdir = False ) classmethod \u00b6 Infer an output directory for an entry in ProcessingTask table. Parameters: Name Type Description Default key dict Primary key from the ProcessingTask table. required relative bool If True, processing_output_dir is returned relative to imaging_root_dir. False mkdir bool If True, create the processing_output_dir directory. False Returns: Type Description A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 Source code in element_calcium_imaging/imaging_no_curation.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. mkdir (bool): If True, create the processing_output_dir directory. Returns: A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir Segmentation \u00b6 Bases: dj . Computed Result of the Segmentation process. Attributes: Name Type Description Processing foreign key Primary key from Processing. Source code in element_calcium_imaging/imaging_no_curation.py 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 @schema class Segmentation ( dj . Computed ): \"\"\"Result of the Segmentation process. Attributes: Processing (foreign key): Primary key from Processing. \"\"\" definition = \"\"\"# Different mask segmentations. -> Processing \"\"\" class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" ) Mask \u00b6 Bases: dj . Part Details of the masks identified from the Segmentation procedure. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. mask int Unique mask ID. scan.Channel.proj(segmentation_channel='channel') foreign key Channel used for segmentation. mask_npix int Number of pixels in ROIs. mask_center_x int Center x coordinate in pixel. mask_center_y int Center y coordinate in pixel. mask_center_z int Center z coordinate in pixel. mask_xpix longblob X coordinates in pixels. mask_ypix longblob Y coordinates in pixels. mask_zpix longblob Z coordinates in pixels. mask_weights longblob Weights of the mask at the indices above. Source code in element_calcium_imaging/imaging_no_curation.py 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" make ( key ) \u00b6 Populate the Segmentation with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_no_curation.py 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" ) activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activate this schema. Parameters: Name Type Description Default imaging_schema_name str Schema name on the database server to activate the imaging module. required scan_schema_name str Schema name on the database server to activate the scan module. Omitted, if the scan module is already activated. None create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the imaging module: + all that are required by the scan module. None Dependencies: Upstream tables Session: A parent table to Scan, identifying a scanning session. Equipment: A parent table to Scan, identifying a scanning device. Source code in element_calcium_imaging/imaging_no_curation.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None , ): \"\"\"Activate this schema. Args: imaging_schema_name (str): Schema name on the database server to activate the `imaging` module. scan_schema_name (str): Schema name on the database server to activate the `scan` module. Omitted, if the `scan` module is already activated. create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `imaging` module: + all that are required by the `scan` module. Dependencies: Upstream tables: + Session: A parent table to Scan, identifying a scanning session. + Equipment: A parent table to Scan, identifying a scanning device. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module scan . activate ( scan_schema_name , create_schema = create_schema , create_tables = create_tables , linking_module = linking_module , ) schema . activate ( imaging_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) imaging_report . activate ( f \" { imaging_schema_name } _report\" , imaging_schema_name ) get_loader_result ( key , table ) \u00b6 Retrieve the processed imaging results from a suite2p or caiman loader. Parameters: Name Type Description Default key dict The key to one entry of ProcessingTask or Curation required table dj . Table A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) required Raises: Type Description NotImplementedError If the processing_method is different than 'suite2p' or 'caiman'. Returns: Type Description A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) Source code in element_calcium_imaging/imaging_no_curation.py 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 def get_loader_result ( key : dict , table : dj . Table ): \"\"\"Retrieve the processed imaging results from a suite2p or caiman loader. Args: key (dict): The `key` to one entry of ProcessingTask or Curation table (dj.Table): A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) Raises: NotImplementedError: If the processing_method is different than 'suite2p' or 'caiman'. Returns: A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) \"\"\" method , output_dir = ( ProcessingParamSet * table & key ) . fetch1 ( \"processing_method\" , _table_attribute_mapper [ table . __name__ ] ) output_path = find_full_path ( get_imaging_root_data_dir (), output_dir ) if method == \"suite2p\" : from element_interface import suite2p_loader loaded_dataset = suite2p_loader . Suite2p ( output_path ) elif method == \"caiman\" : from element_interface import caiman_loader loaded_dataset = caiman_loader . CaImAn ( output_path ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) return method , loaded_dataset", "title": "imaging_no_curation.py"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity", "text": "Bases: dj . Computed Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. ActivityExtractionMethod foreign key Primary key from ActivityExtractionMethod. Source code in element_calcium_imaging/imaging_no_curation.py 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 @schema class Activity ( dj . Computed ): \"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Fluorescence (foreign key): Primary key from Fluorescence. ActivityExtractionMethod (foreign key): Primary key from ActivityExtractionMethod. \"\"\" definition = \"\"\"# Neural Activity -> Fluorescence -> ActivityExtractionMethod \"\"\" class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" @property def key_source ( self ): suite2p_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"suite2p\"' & 'extraction_method LIKE \"suite2p%\"' ) caiman_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"caiman\"' & 'extraction_method LIKE \"caiman%\"' ) return suite2p_key_source . proj () + caiman_key_source . proj () def make ( self , key ): \"\"\" Populate the Activity with the results parsed from analysis outputs. \"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "Activity"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity.Trace", "text": "Bases: dj . Part Trace(s) for each mask. Attributes: Name Type Description Activity foreign key Primary key from Activity. Fluorescence.Trace foreign key Fluorescence.Trace. activity_trace longblob Neural activity from fluoresence trace. Source code in element_calcium_imaging/imaging_no_curation.py 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\"", "title": "Trace"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity.make", "text": "Populate the Activity with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_no_curation.py 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 def make ( self , key ): \"\"\" Populate the Activity with the results parsed from analysis outputs. \"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ActivityExtractionMethod", "text": "Bases: dj . Lookup Available activity extraction methods. Attributes: Name Type Description extraction_method str Extraction method. Source code in element_calcium_imaging/imaging_no_curation.py 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 @schema class ActivityExtractionMethod ( dj . Lookup ): \"\"\"Available activity extraction methods. Attributes: extraction_method (str): Extraction method. \"\"\" definition = \"\"\"# Activity extraction method extraction_method: varchar(32) \"\"\" contents = zip ([ \"suite2p_deconvolution\" , \"caiman_deconvolution\" , \"caiman_dff\" ])", "title": "ActivityExtractionMethod"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.CellCompartment", "text": "Bases: dj . Lookup Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: Name Type Description cell_compartment str Cell compartment. Source code in element_calcium_imaging/imaging_no_curation.py 164 165 166 167 168 169 170 171 172 173 174 175 176 @schema class CellCompartment ( dj . Lookup ): \"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: cell_compartment (str): Cell compartment. \"\"\" definition = \"\"\"# Cell compartments cell_compartment: char(16) \"\"\" contents = zip ([ \"axon\" , \"soma\" , \"bouton\" ])", "title": "CellCompartment"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence", "text": "Bases: dj . Computed Fluorescence traces. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. Source code in element_calcium_imaging/imaging_no_curation.py 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 @schema class Fluorescence ( dj . Computed ): \"\"\"Fluorescence traces. Attributes: Segmentation (foreign key): Primary key from Segmentation. \"\"\" definition = \"\"\"# Fluorescence traces before spike extraction or filtering -> Segmentation \"\"\" class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "Fluorescence"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence.Trace", "text": "Bases: dj . Part Traces obtained from segmented region of interests. Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. Segmentation.Mask foreign key Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') int The channel that this trace comes from. fluorescence longblob Fluorescence trace associated with this mask. neuropil_fluorescence longblob Neuropil fluorescence trace. Source code in element_calcium_imaging/imaging_no_curation.py 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\"", "title": "Trace"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence.make", "text": "Populate the Fluorescence with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_no_curation.py 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassification", "text": "Bases: dj . Computed Classes assigned to each mask. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. MaskClassificationMethod foreign key Primary key from MaskClassificationMethod. Source code in element_calcium_imaging/imaging_no_curation.py 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 @schema class MaskClassification ( dj . Computed ): \"\"\"Classes assigned to each mask. Attributes: Segmentation (foreign key): Primary key from Segmentation. MaskClassificationMethod (foreign key): Primary key from MaskClassificationMethod. \"\"\" definition = \"\"\" -> Segmentation -> MaskClassificationMethod \"\"\" class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" def make ( self , key ): pass", "title": "MaskClassification"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassification.MaskType", "text": "Bases: dj . Part Type assigned to each mask. Attributes: Name Type Description MaskClassification foreign key Primary key from MaskClassification. Segmentation.Mask foreign key Primary key from Segmentation.Mask. MaskType foreign key Primary key from MaskType. confidence float Confidence level of the mask classification. Source code in element_calcium_imaging/imaging_no_curation.py 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\"", "title": "MaskType"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassificationMethod", "text": "Bases: dj . Lookup Available mask classification methods. Attributes: Name Type Description mask_classification_method str Mask classification method. Source code in element_calcium_imaging/imaging_no_curation.py 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 @schema class MaskClassificationMethod ( dj . Lookup ): \"\"\"Available mask classification methods. Attributes: mask_classification_method (str): Mask classification method. \"\"\" definition = \"\"\" mask_classification_method: varchar(48) \"\"\" contents = zip ([ \"suite2p_default_classifier\" , \"caiman_default_classifier\" ])", "title": "MaskClassificationMethod"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskType", "text": "Bases: dj . Lookup Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: Name Type Description masky_type str Mask type. Source code in element_calcium_imaging/imaging_no_curation.py 179 180 181 182 183 184 185 186 187 188 189 190 191 @schema class MaskType ( dj . Lookup ): \"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: masky_type (str): Mask type. \"\"\" definition = \"\"\"# Possible types of a segmented mask mask_type: varchar(16) \"\"\" contents = zip ([ \"soma\" , \"axon\" , \"dendrite\" , \"neuropil\" , \"artefact\" , \"unknown\" ])", "title": "MaskType"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection", "text": "Bases: dj . Imported Results of motion correction shifts performed on the imaging data. Attributes: Name Type Description Processing foreign key Primary key from Processing. scan.Channel.proj(motion_correct_channel='channel') int Channel used for motion correction in this processing task. Source code in element_calcium_imaging/imaging_no_curation.py 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 @schema class MotionCorrection ( dj . Imported ): \"\"\"Results of motion correction shifts performed on the imaging data. Attributes: Processing (foreign key): Primary key from Processing. scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for motion correction in this processing task. \"\"\" definition = \"\"\"# Results of motion correction -> Processing --- -> scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task \"\"\" class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y (longblob): y_start and y_end in pixels for this block block_x (longblob): x_start and x_end in pixels for this block block_z (longblob): z_start and z_end in pixels for this block y_shifts (longblob): y motion correction shifts for every frame in pixels x_shifts (longblob): x motion correction shifts for every frame in pixels z_shifts (longblob, optional): x motion correction shifts for every frame in pixels y_std (float): standard deviation of y shifts across all frames in pixels x_std (float): standard deviation of x shifts across all frames in pixels z_std (float, optional): standard deviation of z shifts across all frames in pixels \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Processing & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "MotionCorrection"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.Block", "text": "Bases: dj . Part FOV-tiled blocks used for non-rigid motion correction. Attributes: Name Type Description NonRigidMotionCorrection foreign key Primary key from NonRigidMotionCorrection. block_id int Unique block ID. block_y longblob y_start and y_end in pixels for this block block_x longblob x_start and x_end in pixels for this block block_z longblob z_start and z_end in pixels for this block y_shifts longblob y motion correction shifts for every frame in pixels x_shifts longblob x motion correction shifts for every frame in pixels z_shifts longblob x motion correction shifts for every frame in pixels y_std float standard deviation of y shifts across all frames in pixels x_std float standard deviation of x shifts across all frames in pixels z_std float standard deviation of z shifts across all frames in pixels Source code in element_calcium_imaging/imaging_no_curation.py 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y (longblob): y_start and y_end in pixels for this block block_x (longblob): x_start and x_end in pixels for this block block_z (longblob): z_start and z_end in pixels for this block y_shifts (longblob): y motion correction shifts for every frame in pixels x_shifts (longblob): x motion correction shifts for every frame in pixels z_shifts (longblob, optional): x motion correction shifts for every frame in pixels y_std (float): standard deviation of y shifts across all frames in pixels x_std (float): standard deviation of x shifts across all frames in pixels z_std (float, optional): standard deviation of z shifts across all frames in pixels \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\"", "title": "Block"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.NonRigidMotionCorrection", "text": "Bases: dj . Part Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob , null Mask with true for frames with outlier shifts (already corrected). block_height int Block height in pixels. block_width int Block width in pixels. block_depth int Block depth in pixels. block_count_y int Number of blocks tiled in the y direction. block_count_x int Number of blocks tiled in the x direction. block_count_z int Number of blocks tiled in the z direction. Source code in element_calcium_imaging/imaging_no_curation.py 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\"", "title": "NonRigidMotionCorrection"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.RigidMotionCorrection", "text": "Bases: dj . Part Details of rigid motion correction performed on the imaging data. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob Mask with true for frames with outlier shifts (already corrected). y_shifts longblob y motion correction shifts (pixels). x_shifts longblob x motion correction shifts (pixels). z_shifts longblob z motion correction shifts (z-drift, pixels). y_std float standard deviation of y shifts across all frames (pixels). x_std float standard deviation of x shifts across all frames (pixels). z_std float standard deviation of z shifts across all frames (pixels). Source code in element_calcium_imaging/imaging_no_curation.py 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\"", "title": "RigidMotionCorrection"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.Summary", "text": "Bases: dj . Part Summary images for each field and channel after corrections. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. scan.ScanInfo.Field foreign key Primary key from scan.ScanInfo.Field. ref_image longblob Image used as alignment template. average_image longblob Mean of registered frames. correlation_image longblob Correlation map (computed during cell detection). max_proj_image longblob Max of registered frames. Source code in element_calcium_imaging/imaging_no_curation.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\"", "title": "Summary"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.make", "text": "Populate MotionCorrection with results parsed from analysis outputs Source code in element_calcium_imaging/imaging_no_curation.py 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Processing & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing", "text": "Bases: dj . Computed Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: Name Type Description ProcessingTask foreign key Primary key from ProcessingTask. processing_time datetime Process completion datetime. package_version str Version of the analysis package used in processing the data. Source code in element_calcium_imaging/imaging_no_curation.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 @schema class Processing ( dj . Computed ): \"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: ProcessingTask (foreign key): Primary key from ProcessingTask. processing_time (datetime): Process completion datetime. package_version (str, optional): Version of the analysis package used in processing the data. \"\"\" definition = \"\"\" -> ProcessingTask --- processing_time : datetime # Time of generation of this set of processed, segmented results package_version='' : varchar(16) \"\"\" # Run processing only on Scan with ScanInfo inserted @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key )", "title": "Processing"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing.key_source", "text": "Limit the Processing to Scans that have their metadata ingested to the database. Source code in element_calcium_imaging/imaging_no_curation.py 331 332 333 334 335 336 @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo", "title": "key_source()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing.make", "text": "Execute the calcium imaging analysis defined by the ProcessingTask. Source code in element_calcium_imaging/imaging_no_curation.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key )", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingMethod", "text": "Bases: dj . Lookup Method, package, or analysis suite used for processing of calcium imaging data (e.g. Suite2p, CaImAn, etc.). Attributes: Name Type Description processing_method str Processing method. processing_method_desc str Processing method description. Source code in element_calcium_imaging/imaging_no_curation.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @schema class ProcessingMethod ( dj . Lookup ): \"\"\"Method, package, or analysis suite used for processing of calcium imaging data (e.g. Suite2p, CaImAn, etc.). Attributes: processing_method (str): Processing method. processing_method_desc (str): Processing method description. \"\"\" definition = \"\"\"# Method for calcium imaging processing processing_method: char(8) --- processing_method_desc: varchar(1000) # Processing method description \"\"\" contents = [ ( \"suite2p\" , \"suite2p analysis suite\" ), ( \"caiman\" , \"caiman analysis suite\" ), ]", "title": "ProcessingMethod"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingParamSet", "text": "Bases: dj . Lookup Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: Name Type Description paramset_idx int Uniqiue parameter set ID. ProcessingMethod foreign key A primary key from ProcessingMethod. paramset_desc str Parameter set description. param_set_hash uuid A universally unique identifier for the parameter set. params longblob Parameter Set, a dictionary of all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_no_curation.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @schema class ProcessingParamSet ( dj . Lookup ): \"\"\"Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: paramset_idx (int): Uniqiue parameter set ID. ProcessingMethod (foreign key): A primary key from ProcessingMethod. paramset_desc (str): Parameter set description. param_set_hash (uuid): A universally unique identifier for the parameter set. params (longblob): Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" definition = \"\"\"# Processing Parameter Set paramset_idx: smallint # Uniqiue parameter set ID. --- -> ProcessingMethod paramset_desc: varchar(1280) # Parameter-set description param_set_hash: uuid # A universally unique identifier for the parameter set params: longblob # Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict )", "title": "ProcessingParamSet"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingParamSet.insert_new_params", "text": "Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: Name Type Description processing_method str Processing method/package used for processing of calcium imaging. paramset_idx int Uniqiue parameter set ID. paramset_desc str Parameter set description. params dict Parameter Set, all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_no_curation.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict )", "title": "insert_new_params()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask", "text": "Bases: dj . Manual This table defines a calcium imaging processing task for a combination of a Scan and a ProcessingParamSet entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: Name Type Description scan.Scan foreign key ProcessingParamSet foreign key processing_output_dir str task_mode str One of 'load' (load computed analysis results) or 'trigger' (trigger computation). Source code in element_calcium_imaging/imaging_no_curation.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @schema class ProcessingTask ( dj . Manual ): \"\"\"This table defines a calcium imaging processing task for a combination of a `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: scan.Scan (foreign key): ProcessingParamSet (foreign key): processing_output_dir (str): task_mode (str): One of 'load' (load computed analysis results) or 'trigger' (trigger computation). \"\"\" definition = \"\"\"# Manual table for defining a processing task ready to be run -> scan.Scan -> ProcessingParamSet --- processing_output_dir: varchar(255) # Output directory of the processed scan relative to root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. mkdir (bool): If True, create the processing_output_dir directory. Returns: A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) auto_generate_entries = generate", "title": "ProcessingTask"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask.generate", "text": "Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Parameters: Name Type Description Default scan_key dict Primary key from Scan table. required paramset_idx int Unique parameter set ID. 0 Source code in element_calcium_imaging/imaging_no_curation.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } )", "title": "generate()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask.infer_output_dir", "text": "Infer an output directory for an entry in ProcessingTask table. Parameters: Name Type Description Default key dict Primary key from the ProcessingTask table. required relative bool If True, processing_output_dir is returned relative to imaging_root_dir. False mkdir bool If True, create the processing_output_dir directory. False Returns: Type Description A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 Source code in element_calcium_imaging/imaging_no_curation.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. mkdir (bool): If True, create the processing_output_dir directory. Returns: A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir", "title": "infer_output_dir()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation", "text": "Bases: dj . Computed Result of the Segmentation process. Attributes: Name Type Description Processing foreign key Primary key from Processing. Source code in element_calcium_imaging/imaging_no_curation.py 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 @schema class Segmentation ( dj . Computed ): \"\"\"Result of the Segmentation process. Attributes: Processing (foreign key): Primary key from Processing. \"\"\" definition = \"\"\"# Different mask segmentations. -> Processing \"\"\" class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" )", "title": "Segmentation"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation.Mask", "text": "Bases: dj . Part Details of the masks identified from the Segmentation procedure. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. mask int Unique mask ID. scan.Channel.proj(segmentation_channel='channel') foreign key Channel used for segmentation. mask_npix int Number of pixels in ROIs. mask_center_x int Center x coordinate in pixel. mask_center_y int Center y coordinate in pixel. mask_center_z int Center z coordinate in pixel. mask_xpix longblob X coordinates in pixels. mask_ypix longblob Y coordinates in pixels. mask_zpix longblob Z coordinates in pixels. mask_weights longblob Weights of the mask at the indices above. Source code in element_calcium_imaging/imaging_no_curation.py 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\"", "title": "Mask"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation.make", "text": "Populate the Segmentation with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_no_curation.py 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" )", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.activate", "text": "Activate this schema. Parameters: Name Type Description Default imaging_schema_name str Schema name on the database server to activate the imaging module. required scan_schema_name str Schema name on the database server to activate the scan module. Omitted, if the scan module is already activated. None create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the imaging module: + all that are required by the scan module. None Dependencies: Upstream tables Session: A parent table to Scan, identifying a scanning session. Equipment: A parent table to Scan, identifying a scanning device. Source code in element_calcium_imaging/imaging_no_curation.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None , ): \"\"\"Activate this schema. Args: imaging_schema_name (str): Schema name on the database server to activate the `imaging` module. scan_schema_name (str): Schema name on the database server to activate the `scan` module. Omitted, if the `scan` module is already activated. create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `imaging` module: + all that are required by the `scan` module. Dependencies: Upstream tables: + Session: A parent table to Scan, identifying a scanning session. + Equipment: A parent table to Scan, identifying a scanning device. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module scan . activate ( scan_schema_name , create_schema = create_schema , create_tables = create_tables , linking_module = linking_module , ) schema . activate ( imaging_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) imaging_report . activate ( f \" { imaging_schema_name } _report\" , imaging_schema_name )", "title": "activate()"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_loader_result", "text": "Retrieve the processed imaging results from a suite2p or caiman loader. Parameters: Name Type Description Default key dict The key to one entry of ProcessingTask or Curation required table dj . Table A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) required Raises: Type Description NotImplementedError If the processing_method is different than 'suite2p' or 'caiman'. Returns: Type Description A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) Source code in element_calcium_imaging/imaging_no_curation.py 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 def get_loader_result ( key : dict , table : dj . Table ): \"\"\"Retrieve the processed imaging results from a suite2p or caiman loader. Args: key (dict): The `key` to one entry of ProcessingTask or Curation table (dj.Table): A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) Raises: NotImplementedError: If the processing_method is different than 'suite2p' or 'caiman'. Returns: A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) \"\"\" method , output_dir = ( ProcessingParamSet * table & key ) . fetch1 ( \"processing_method\" , _table_attribute_mapper [ table . __name__ ] ) output_path = find_full_path ( get_imaging_root_data_dir (), output_dir ) if method == \"suite2p\" : from element_interface import suite2p_loader loaded_dataset = suite2p_loader . Suite2p ( output_path ) elif method == \"caiman\" : from element_interface import caiman_loader loaded_dataset = caiman_loader . CaImAn ( output_path ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) return method , loaded_dataset", "title": "get_loader_result()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/", "text": "Activity \u00b6 Bases: dj . Computed Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. ActivityExtractionMethod foreign key Primary key from ActivityExtractionMethod. Source code in element_calcium_imaging/imaging_preprocess.py 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 @schema class Activity ( dj . Computed ): \"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Fluorescence (foreign key): Primary key from Fluorescence. ActivityExtractionMethod (foreign key): Primary key from ActivityExtractionMethod. \"\"\" definition = \"\"\"# Neural Activity -> Fluorescence -> ActivityExtractionMethod \"\"\" class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" @property def key_source ( self ): suite2p_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"suite2p\"' & 'extraction_method LIKE \"suite2p%\"' ) caiman_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"caiman\"' & 'extraction_method LIKE \"caiman%\"' ) return suite2p_key_source . proj () + caiman_key_source . proj () def make ( self , key ): \"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Trace \u00b6 Bases: dj . Part Trace(s) for each mask. Attributes: Name Type Description Activity foreign key Primary key from Activity. Fluorescence.Trace foreign key Fluorescence.Trace. activity_trace longblob Neural activity from fluoresence trace. Source code in element_calcium_imaging/imaging_preprocess.py 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" make ( key ) \u00b6 Populate the Activity with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_preprocess.py 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 def make ( self , key ): \"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) ActivityExtractionMethod \u00b6 Bases: dj . Lookup Available activity extraction methods. Attributes: Name Type Description extraction_method str Extraction method. Source code in element_calcium_imaging/imaging_preprocess.py 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 @schema class ActivityExtractionMethod ( dj . Lookup ): \"\"\"Available activity extraction methods. Attributes: extraction_method (str): Extraction method. \"\"\" definition = \"\"\"# Activity extraction method extraction_method: varchar(32) \"\"\" contents = zip ([ \"suite2p_deconvolution\" , \"caiman_deconvolution\" , \"caiman_dff\" ]) CellCompartment \u00b6 Bases: dj . Lookup Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: Name Type Description cell_compartment str Cell compartment. Source code in element_calcium_imaging/imaging_preprocess.py 344 345 346 347 348 349 350 351 352 353 354 355 356 @schema class CellCompartment ( dj . Lookup ): \"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: cell_compartment (str): Cell compartment. \"\"\" definition = \"\"\"# Cell compartments cell_compartment: char(16) \"\"\" contents = zip ([ \"axon\" , \"soma\" , \"bouton\" ]) Curation \u00b6 Bases: dj . Manual Curated results. If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir. Attributes: Name Type Description Processing foreign key Primary key from Processing. curation_id int Unique curation ID. curation_time datetime Time of generation of this set of curated results. curation_output_dir str Output directory of the curated results, relative to root data directory. manual_curation bool If True, manual curation has been performed on this result. curation_note str Notes about the curation task. Source code in element_calcium_imaging/imaging_preprocess.py 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 @schema class Curation ( dj . Manual ): \"\"\"Curated results. If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir. Attributes: Processing (foreign key): Primary key from Processing. curation_id (int): Unique curation ID. curation_time (datetime): Time of generation of this set of curated results. curation_output_dir (str): Output directory of the curated results, relative to root data directory. manual_curation (bool): If True, manual curation has been performed on this result. curation_note (str, optional): Notes about the curation task. \"\"\" definition = \"\"\"# Curation(s) results -> Processing curation_id: int --- curation_time: datetime # Time of generation of this set of curated results curation_output_dir: varchar(255) # Output directory of the curated results, relative to root data directory manual_curation: bool # Has manual curation been performed on this result? curation_note='': varchar(2000) \"\"\" def create1_from_processing_task ( self , key , is_curated = False , curation_note = \"\" ): \"\"\"Create a Curation entry for a given ProcessingTask key. Args: key (dict): Primary key set of an entry in the ProcessingTask table. is_curated (bool): When True, indicates a manual curation. curation_note (str): User's note on the specifics of the curation. \"\"\" if key not in Processing (): raise ValueError ( f \"No corresponding entry in Processing available for: { key } ;\" f \"Please run `Processing.populate(key)`\" ) output_dir = ( ProcessingTask & key ) . fetch1 ( \"processing_output_dir\" ) method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset curation_time = suite2p_dataset . creation_time elif method == \"caiman\" : caiman_dataset = imaging_dataset curation_time = caiman_dataset . creation_time else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : curation_time , \"curation_output_dir\" : output_dir , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) create1_from_processing_task ( key , is_curated = False , curation_note = '' ) \u00b6 Create a Curation entry for a given ProcessingTask key. Parameters: Name Type Description Default key dict Primary key set of an entry in the ProcessingTask table. required is_curated bool When True, indicates a manual curation. False curation_note str User's note on the specifics of the curation. '' Source code in element_calcium_imaging/imaging_preprocess.py 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 def create1_from_processing_task ( self , key , is_curated = False , curation_note = \"\" ): \"\"\"Create a Curation entry for a given ProcessingTask key. Args: key (dict): Primary key set of an entry in the ProcessingTask table. is_curated (bool): When True, indicates a manual curation. curation_note (str): User's note on the specifics of the curation. \"\"\" if key not in Processing (): raise ValueError ( f \"No corresponding entry in Processing available for: { key } ;\" f \"Please run `Processing.populate(key)`\" ) output_dir = ( ProcessingTask & key ) . fetch1 ( \"processing_output_dir\" ) method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset curation_time = suite2p_dataset . creation_time elif method == \"caiman\" : caiman_dataset = imaging_dataset curation_time = caiman_dataset . creation_time else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : curation_time , \"curation_output_dir\" : output_dir , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) Fluorescence \u00b6 Bases: dj . Computed Fluorescence traces. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. Source code in element_calcium_imaging/imaging_preprocess.py 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 @schema class Fluorescence ( dj . Computed ): \"\"\"Fluorescence traces. Attributes: Segmentation (foreign key): Primary key from Segmentation. \"\"\" definition = \"\"\"# Fluorescence traces before spike extraction or filtering -> Segmentation \"\"\" class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Trace \u00b6 Bases: dj . Part Traces obtained from segmented region of interests. Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. Segmentation.Mask foreign key Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') int The channel that this trace comes from. fluorescence longblob Fluorescence trace associated with this mask. neuropil_fluorescence longblob Neuropil fluorescence trace. Source code in element_calcium_imaging/imaging_preprocess.py 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" make ( key ) \u00b6 Populate the Fluorescence with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_preprocess.py 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) MaskClassification \u00b6 Bases: dj . Computed Classes assigned to each mask. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. MaskClassificationMethod foreign key Primary key from MaskClassificationMethod. Source code in element_calcium_imaging/imaging_preprocess.py 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 @schema class MaskClassification ( dj . Computed ): \"\"\"Classes assigned to each mask. Attributes: Segmentation (foreign key): Primary key from Segmentation. MaskClassificationMethod (foreign key): Primary key from MaskClassificationMethod. \"\"\" definition = \"\"\" -> Segmentation -> MaskClassificationMethod \"\"\" class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" def make ( self , key ): pass MaskType \u00b6 Bases: dj . Part Type assigned to each mask. Attributes: Name Type Description MaskClassification foreign key Primary key from MaskClassification. Segmentation.Mask foreign key Primary key from Segmentation.Mask. MaskType foreign key Primary key from MaskType. confidence float Confidence level of the mask classification. Source code in element_calcium_imaging/imaging_preprocess.py 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" MaskClassificationMethod \u00b6 Bases: dj . Lookup Available mask classification methods. Attributes: Name Type Description mask_classification_method str Mask classification method. Source code in element_calcium_imaging/imaging_preprocess.py 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 @schema class MaskClassificationMethod ( dj . Lookup ): \"\"\"Available mask classification methods. Attributes: mask_classification_method (str): Mask classification method. \"\"\" definition = \"\"\" mask_classification_method: varchar(48) \"\"\" contents = zip ([ \"suite2p_default_classifier\" , \"caiman_default_classifier\" ]) MaskType \u00b6 Bases: dj . Lookup Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: Name Type Description masky_type str Mask type. Source code in element_calcium_imaging/imaging_preprocess.py 359 360 361 362 363 364 365 366 367 368 369 370 371 @schema class MaskType ( dj . Lookup ): \"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: masky_type (str): Mask type. \"\"\" definition = \"\"\"# Possible types of a segmented mask mask_type: varchar(16) \"\"\" contents = zip ([ \"soma\" , \"axon\" , \"dendrite\" , \"neuropil\" , \"artefact\" , \"unknown\" ]) MotionCorrection \u00b6 Bases: dj . Imported Results of motion correction shifts performed on the imaging data. Attributes: Name Type Description Curation foreign key Primary key from Curation. scan.Channel.proj(motion_correct_channel='channel') int Channel used for motion correction in this processing task. Source code in element_calcium_imaging/imaging_preprocess.py 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 @schema class MotionCorrection ( dj . Imported ): \"\"\"Results of motion correction shifts performed on the imaging data. Attributes: Curation (foreign key): Primary key from Curation. scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for motion correction in this processing task. \"\"\" definition = \"\"\"# Results of motion correction -> Curation --- -> scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task \"\"\" class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Curation & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Block \u00b6 Bases: dj . Part FOV-tiled blocks used for non-rigid motion correction. Attributes: Name Type Description NonRigidMotionCorrection foreign key Primary key from NonRigidMotionCorrection. block_id int Unique block ID. block_y longblob # (y_start, y_end) in pixel of this block block_x longblob # (x_start, x_end) in pixel of this block block_z longblob # (z_start, z_end) in pixel of this block y_shifts longblob # (pixels) y motion correction shifts for every frame x_shifts longblob # (pixels) x motion correction shifts for every frame z_shifts=null longblob # (pixels) x motion correction shifts for every frame y_std float # (pixels) standard deviation of y shifts across all frames x_std float # (pixels) standard deviation of x shifts across all frames z_std=null float # (pixels) standard deviation of z shifts across all frames Source code in element_calcium_imaging/imaging_preprocess.py 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" NonRigidMotionCorrection \u00b6 Bases: dj . Part Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob , null Mask with true for frames with outlier shifts (already corrected). block_height int Block height in pixels. block_width int Block width in pixels. block_depth int Block depth in pixels. block_count_y int Number of blocks tiled in the y direction. block_count_x int Number of blocks tiled in the x direction. block_count_z int Number of blocks tiled in the z direction. Source code in element_calcium_imaging/imaging_preprocess.py 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" RigidMotionCorrection \u00b6 Bases: dj . Part Details of rigid motion correction performed on the imaging data. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob Mask with true for frames with outlier shifts (already corrected). y_shifts longblob y motion correction shifts (pixels). x_shifts longblob x motion correction shifts (pixels). z_shifts longblob z motion correction shifts (z-drift, pixels). y_std float standard deviation of y shifts across all frames (pixels). x_std float standard deviation of x shifts across all frames (pixels). z_std float standard deviation of z shifts across all frames (pixels). Source code in element_calcium_imaging/imaging_preprocess.py 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" Summary \u00b6 Bases: dj . Part Summary images for each field and channel after corrections. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. scan.ScanInfo.Field foreign key Primary key from scan.ScanInfo.Field. ref_image longblob Image used as alignment template. average_image longblob Mean of registered frames. correlation_image longblob Correlation map (computed during cell detection). max_proj_image longblob Max of registered frames. Source code in element_calcium_imaging/imaging_preprocess.py 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" make ( key ) \u00b6 Populate MotionCorrection with results parsed from analysis outputs Source code in element_calcium_imaging/imaging_preprocess.py 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Curation & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) Preprocess \u00b6 Bases: dj . Imported Perform the computation of an entry (task) defined in the PreprocessTask table. If task_mode == \"none\" : no pre-processing performed If task_mode == \"trigger\" : Not implemented If task_mode == \"load\" : Not implemented Attributes: Name Type Description PreprocessTask foreign key preprocess_time datetime package_version str Version of the analysis package used in processing the data. Source code in element_calcium_imaging/imaging_preprocess.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 @schema class Preprocess ( dj . Imported ): \"\"\"Perform the computation of an entry (task) defined in the PreprocessTask table. + If `task_mode == \"none\"`: no pre-processing performed + If `task_mode == \"trigger\"`: Not implemented + If `task_mode == \"load\"`: Not implemented Attributes: PreprocessTask (foreign key): preprocess_time (datetime, optional): package_version (str, optional): Version of the analysis package used in processing the data. \"\"\" definition = \"\"\" -> PreprocessTask --- preprocess_time=null: datetime # Time of generation of pre-processing results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Execute the preprocessing analysis steps defined in PreprocessTask.\"\"\" task_mode , output_dir = ( PreprocessTask & key ) . fetch1 ( \"task_mode\" , \"preprocess_output_dir\" ) preprocess_output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) if task_mode == \"none\" : print ( f \"No pre-processing run on entry: { key } \" ) elif task_mode in [ \"load\" , \"trigger\" ]: raise NotImplementedError ( \"Pre-processing steps are not implemented.\" \"Please overwrite this `make` function with\" \"desired pre-processing steps.\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key ) make ( key ) \u00b6 Execute the preprocessing analysis steps defined in PreprocessTask. Source code in element_calcium_imaging/imaging_preprocess.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def make ( self , key ): \"\"\"Execute the preprocessing analysis steps defined in PreprocessTask.\"\"\" task_mode , output_dir = ( PreprocessTask & key ) . fetch1 ( \"task_mode\" , \"preprocess_output_dir\" ) preprocess_output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) if task_mode == \"none\" : print ( f \"No pre-processing run on entry: { key } \" ) elif task_mode in [ \"load\" , \"trigger\" ]: raise NotImplementedError ( \"Pre-processing steps are not implemented.\" \"Please overwrite this `make` function with\" \"desired pre-processing steps.\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key ) PreprocessMethod \u00b6 Bases: dj . Lookup Method(s) used for preprocessing of calcium imaging data. Attributes: Name Type Description preprocess_method str Preprocessing method. preprocess_method_desc str Processing method description. Source code in element_calcium_imaging/imaging_preprocess.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 @schema class PreprocessMethod ( dj . Lookup ): \"\"\"Method(s) used for preprocessing of calcium imaging data. Attributes: preprocess_method (str): Preprocessing method. preprocess_method_desc (str): Processing method description. \"\"\" definition = \"\"\" # Method/package used for pre-processing preprocess_method: varchar(16) --- preprocess_method_desc: varchar(1000) \"\"\" PreprocessParamSet \u00b6 Bases: dj . Lookup Parameter set used for the preprocessing of the calcium imaging scans. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: Name Type Description paramset_idx int Uniqiue parameter set ID. PreprocessMethod foreign key A primary key from PreprocessMethod. paramset_desc str Parameter set description. param_set_hash uuid A universally unique identifier for the parameter set. params longblob Parameter Set, a dictionary of all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_preprocess.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 @schema class PreprocessParamSet ( dj . Lookup ): \"\"\"Parameter set used for the preprocessing of the calcium imaging scans. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: paramset_idx (int): Uniqiue parameter set ID. PreprocessMethod (foreign key): A primary key from PreprocessMethod. paramset_desc (str): Parameter set description. param_set_hash (uuid): A universally unique identifier for the parameter set. params (longblob): Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" definition = \"\"\" # Parameter set used for pre-processing of calcium imaging data paramset_idx: smallint --- -> PreprocessMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , preprocess_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into PreprocessParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: preprocess_method (str): Method used for processing of calcium imaging scans. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters. \"\"\" param_dict = { \"preprocess_method\" : preprocess_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict ) insert_new_params ( preprocess_method , paramset_idx , paramset_desc , params ) classmethod \u00b6 Insert a parameter set into PreprocessParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: Name Type Description preprocess_method str Method used for processing of calcium imaging scans. paramset_idx int Uniqiue parameter set ID. paramset_desc str Parameter set description. params dict Parameter Set, all applicable parameters. Source code in element_calcium_imaging/imaging_preprocess.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 @classmethod def insert_new_params ( cls , preprocess_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into PreprocessParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: preprocess_method (str): Method used for processing of calcium imaging scans. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters. \"\"\" param_dict = { \"preprocess_method\" : preprocess_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict ) PreprocessParamSteps \u00b6 Bases: dj . Manual Ordered list of paramset_idx that will be run. When pre-processing is not performed, do not create an entry in Step Part table Attributes: Name Type Description preprocess_param_steps_id int preprocess_param_steps_name str preprocess_param_steps_desc str Source code in element_calcium_imaging/imaging_preprocess.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @schema class PreprocessParamSteps ( dj . Manual ): \"\"\"Ordered list of paramset_idx that will be run. When pre-processing is not performed, do not create an entry in `Step` Part table Attributes: preprocess_param_steps_id (int): preprocess_param_steps_name (str): preprocess_param_steps_desc (str): \"\"\" definition = \"\"\" preprocess_param_steps_id: smallint --- preprocess_param_steps_name: varchar(32) preprocess_param_steps_desc: varchar(128) \"\"\" class Step ( dj . Part ): \"\"\"ADD DEFINITION Attributes: PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps. step_number (int): PreprocessParamSet (foreign key): A primary key from PreprocessParamSet. \"\"\" definition = \"\"\" -> master step_number: smallint # Order of operations --- -> PreprocessParamSet \"\"\" Step \u00b6 Bases: dj . Part ADD DEFINITION Attributes: Name Type Description PreprocessParamSteps foreign key A primary key from PreprocessParamSteps. step_number int PreprocessParamSet foreign key A primary key from PreprocessParamSet. Source code in element_calcium_imaging/imaging_preprocess.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class Step ( dj . Part ): \"\"\"ADD DEFINITION Attributes: PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps. step_number (int): PreprocessParamSet (foreign key): A primary key from PreprocessParamSet. \"\"\" definition = \"\"\" -> master step_number: smallint # Order of operations --- -> PreprocessParamSet \"\"\" PreprocessTask \u00b6 Bases: dj . Manual This table defines a calcium imaging preprocessing task for a combination of a Scan and a PreprocessParamSteps entries, including all the inputs (scan, method, steps). The task defined here is then run in the downstream table Preprocess. This table supports definitions of both loading of pre-generated, results, triggering of new analysis, or skipping of preprocessing step. Attributes: Name Type Description Scan foreign key A primary key from Scan. PreprocessParamSteps foreign key A primary key from PreprocessParamSteps. preprocess_output_dir str Output directory for the results of preprocessing. task_mode str One of 'load' (load computed analysis results), 'trigger' (trigger computation), 'none' (no pre-processing). Default none. Source code in element_calcium_imaging/imaging_preprocess.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 @schema class PreprocessTask ( dj . Manual ): \"\"\"This table defines a calcium imaging preprocessing task for a combination of a `Scan` and a `PreprocessParamSteps` entries, including all the inputs (scan, method, steps). The task defined here is then run in the downstream table Preprocess. This table supports definitions of both loading of pre-generated, results, triggering of new analysis, or skipping of preprocessing step. Attributes: Scan (foreign key): A primary key from Scan. PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps. preprocess_output_dir (str): Output directory for the results of preprocessing. task_mode (str, optional): One of 'load' (load computed analysis results), 'trigger' (trigger computation), 'none' (no pre-processing). Default none. \"\"\" definition = \"\"\" # Manual table for defining a pre-processing task ready to be run -> scan.Scan -> PreprocessParamSteps --- preprocess_output_dir: varchar(255) # Pre-processing output directory relative # to the root data directory task_mode='none': enum('none','load', 'trigger') # 'none': no pre-processing # 'load': load analysis results # 'trigger': trigger computation \"\"\" Processing \u00b6 Bases: dj . Computed Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: Name Type Description ProcessingTask foreign key Primary key from ProcessingTask. processing_time datetime Process completion datetime. package_version str Version of the analysis package used in processing the data. Source code in element_calcium_imaging/imaging_preprocess.py 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 @schema class Processing ( dj . Computed ): \"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: ProcessingTask (foreign key): Primary key from ProcessingTask. processing_time (datetime): Process completion datetime. package_version (str, optional): Version of the analysis package used in processing the data. \"\"\" definition = \"\"\" -> ProcessingTask --- processing_time : datetime # Time of generation of this set of processed, segmented results package_version='' : varchar(16) \"\"\" # Run processing only on Scan with ScanInfo inserted @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) preprocess_paramsets = ( PreprocessParamSteps . Step () & dict ( preprocess_param_steps_id = key [ \"preprocess_param_steps_id\" ]) ) . fetch ( \"paramset_idx\" ) if len ( preprocess_paramsets ) == 0 : # No pre-processing steps were performed on the acquired dataset, so process the raw/acquired files. image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] else : preprocess_output_dir = ( PreprocessTask & key ) . fetch1 ( \"preprocess_output_dir\" ) preprocess_output_dir = find_full_path ( get_imaging_root_data_dir (), preprocess_output_dir ) if not preprocess_output_dir . exists (): raise FileNotFoundError ( f \"Pre-processed output directory not found ( { preprocess_output_dir } )\" ) image_files = list ( preprocess_output_dir . glob ( \"*.tif\" )) if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key ) key_source () property \u00b6 Limit the Processing to Scans that have their metadata ingested to the database. Source code in element_calcium_imaging/imaging_preprocess.py 511 512 513 514 515 516 @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo make ( key ) \u00b6 Execute the calcium imaging analysis defined by the ProcessingTask. Source code in element_calcium_imaging/imaging_preprocess.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) preprocess_paramsets = ( PreprocessParamSteps . Step () & dict ( preprocess_param_steps_id = key [ \"preprocess_param_steps_id\" ]) ) . fetch ( \"paramset_idx\" ) if len ( preprocess_paramsets ) == 0 : # No pre-processing steps were performed on the acquired dataset, so process the raw/acquired files. image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] else : preprocess_output_dir = ( PreprocessTask & key ) . fetch1 ( \"preprocess_output_dir\" ) preprocess_output_dir = find_full_path ( get_imaging_root_data_dir (), preprocess_output_dir ) if not preprocess_output_dir . exists (): raise FileNotFoundError ( f \"Pre-processed output directory not found ( { preprocess_output_dir } )\" ) image_files = list ( preprocess_output_dir . glob ( \"*.tif\" )) if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key ) ProcessingParamSet \u00b6 Bases: dj . Lookup Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: Name Type Description paramset_idx int Uniqiue parameter set ID. ProcessingMethod foreign key A primary key from ProcessingMethod. paramset_desc str Parameter set description. param_set_hash uuid A universally unique identifier for the parameter set. params longblob Parameter Set, a dictionary of all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_preprocess.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 @schema class ProcessingParamSet ( dj . Lookup ): \"\"\"Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: paramset_idx (int): Uniqiue parameter set ID. ProcessingMethod (foreign key): A primary key from ProcessingMethod. paramset_desc (str): Parameter set description. param_set_hash (uuid): A universally unique identifier for the parameter set. params (longblob): Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" definition = \"\"\"# Processing Parameter Set paramset_idx: smallint # Uniqiue parameter set ID. --- -> ProcessingMethod paramset_desc: varchar(1280) # Parameter-set description param_set_hash: uuid # A universally unique identifier for the parameter set params: longblob # Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict ) insert_new_params ( processing_method , paramset_idx , paramset_desc , params ) classmethod \u00b6 Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: Name Type Description processing_method str Processing method/package used for processing of calcium imaging. paramset_idx int Uniqiue parameter set ID. paramset_desc str Parameter set description. params dict Parameter Set, all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_preprocess.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict ) ProcessingTask \u00b6 Bases: dj . Manual This table defines a calcium imaging processing task for a combination of a Preprocess and a ProcessingParamSet entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: Name Type Description Preprocess foreign key ProcessingParamSet foreign key processing_output_dir str task_mode str One of 'load' (load computed analysis results) or 'trigger' (trigger computation). Source code in element_calcium_imaging/imaging_preprocess.py 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 @schema class ProcessingTask ( dj . Manual ): \"\"\"This table defines a calcium imaging processing task for a combination of a `Preprocess` and a `ProcessingParamSet` entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: Preprocess (foreign key): ProcessingParamSet (foreign key): processing_output_dir (str): task_mode (str): One of 'load' (load computed analysis results) or 'trigger' (trigger computation). \"\"\" definition = \"\"\"# Manual table for defining a processing task ready to be run -> Preprocess -> ProcessingParamSet --- processing_output_dir: varchar(255) # Output directory of the processed scan relative to root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. mkdir (bool): If True, create the processing_output_dir directory. Returns: A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) auto_generate_entries = generate generate ( scan_key , paramset_idx = 0 ) classmethod \u00b6 Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Parameters: Name Type Description Default scan_key dict Primary key from Scan table. required paramset_idx int Unique parameter set ID. 0 Source code in element_calcium_imaging/imaging_preprocess.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) infer_output_dir ( key , relative = False , mkdir = False ) classmethod \u00b6 Infer an output directory for an entry in ProcessingTask table. Parameters: Name Type Description Default key dict Primary key from the ProcessingTask table. required relative bool If True, processing_output_dir is returned relative to imaging_root_dir. False mkdir bool If True, create the processing_output_dir directory. False Returns: Type Description A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 Source code in element_calcium_imaging/imaging_preprocess.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. mkdir (bool): If True, create the processing_output_dir directory. Returns: A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir Segmentation \u00b6 Bases: dj . Computed Result of the Segmentation process. Attributes: Name Type Description Curation foreign key Primary key from Curation. Source code in element_calcium_imaging/imaging_preprocess.py 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 @schema class Segmentation ( dj . Computed ): \"\"\"Result of the Segmentation process. Attributes: Curation (foreign key): Primary key from Curation. \"\"\" definition = \"\"\"# Different mask segmentations. -> Curation \"\"\" class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" ) Mask \u00b6 Bases: dj . Part Details of the masks identified from the Segmentation procedure. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. mask int Unique mask ID. scan.Channel.proj(segmentation_channel='channel') foreign key Channel used for segmentation. mask_npix int Number of pixels in ROIs. mask_center_x int Center x coordinate in pixel. mask_center_y int Center y coordinate in pixel. mask_center_z int Center z coordinate in pixel. mask_xpix longblob X coordinates in pixels. mask_ypix longblob Y coordinates in pixels. mask_zpix longblob Z coordinates in pixels. mask_weights longblob Weights of the mask at the indices above. Source code in element_calcium_imaging/imaging_preprocess.py 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" make ( key ) \u00b6 Populate the Segmentation with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_preprocess.py 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" ) activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activate this schema. Parameters: Name Type Description Default imaging_schema_name str Schema name on the database server to activate the imaging module. required scan_schema_name str Schema name on the database server to activate the scan module. Omitted, if the scan module is already activated. None create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the imaging module: + all that are required by the scan module. None Dependencies: Upstream tables Session: A parent table to Scan, identifying a scanning session. Equipment: A parent table to Scan, identifying a scanning device. Source code in element_calcium_imaging/imaging_preprocess.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None , ): \"\"\"Activate this schema. Args: imaging_schema_name (str): Schema name on the database server to activate the `imaging` module. scan_schema_name (str): Schema name on the database server to activate the `scan` module. Omitted, if the `scan` module is already activated. create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `imaging` module: + all that are required by the `scan` module. Dependencies: Upstream tables: + Session: A parent table to Scan, identifying a scanning session. + Equipment: A parent table to Scan, identifying a scanning device. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module scan . activate ( scan_schema_name , create_schema = create_schema , create_tables = create_tables , linking_module = linking_module , ) schema . activate ( imaging_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) imaging_report . activate ( f \" { imaging_schema_name } _report\" , imaging_schema_name ) get_loader_result ( key , table ) \u00b6 Retrieve the processed imaging results from a suite2p or caiman loader. Parameters: Name Type Description Default key dict The key to one entry of ProcessingTask or Curation required table dj . Table A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) required Raises: Type Description NotImplementedError If the processing_method is different than 'suite2p' or 'caiman'. Returns: Type Description A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) Source code in element_calcium_imaging/imaging_preprocess.py 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 def get_loader_result ( key : dict , table : dj . Table ): \"\"\"Retrieve the processed imaging results from a suite2p or caiman loader. Args: key (dict): The `key` to one entry of ProcessingTask or Curation table (dj.Table): A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) Raises: NotImplementedError: If the processing_method is different than 'suite2p' or 'caiman'. Returns: A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) \"\"\" method , output_dir = ( ProcessingParamSet * table & key ) . fetch1 ( \"processing_method\" , _table_attribute_mapper [ table . __name__ ] ) output_path = find_full_path ( get_imaging_root_data_dir (), output_dir ) if method == \"suite2p\" : from element_interface import suite2p_loader loaded_dataset = suite2p_loader . Suite2p ( output_path ) elif method == \"caiman\" : from element_interface import caiman_loader loaded_dataset = caiman_loader . CaImAn ( output_path ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) return method , loaded_dataset", "title": "imaging_preprocess.py"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity", "text": "Bases: dj . Computed Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. ActivityExtractionMethod foreign key Primary key from ActivityExtractionMethod. Source code in element_calcium_imaging/imaging_preprocess.py 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 @schema class Activity ( dj . Computed ): \"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.). Attributes: Fluorescence (foreign key): Primary key from Fluorescence. ActivityExtractionMethod (foreign key): Primary key from ActivityExtractionMethod. \"\"\" definition = \"\"\"# Neural Activity -> Fluorescence -> ActivityExtractionMethod \"\"\" class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\" @property def key_source ( self ): suite2p_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"suite2p\"' & 'extraction_method LIKE \"suite2p%\"' ) caiman_key_source = ( Fluorescence * ActivityExtractionMethod * ProcessingParamSet . proj ( \"processing_method\" ) & 'processing_method = \"caiman\"' & 'extraction_method LIKE \"caiman%\"' ) return suite2p_key_source . proj () + caiman_key_source . proj () def make ( self , key ): \"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "Activity"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity.Trace", "text": "Bases: dj . Part Trace(s) for each mask. Attributes: Name Type Description Activity foreign key Primary key from Activity. Fluorescence.Trace foreign key Fluorescence.Trace. activity_trace longblob Neural activity from fluoresence trace. Source code in element_calcium_imaging/imaging_preprocess.py 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 class Trace ( dj . Part ): \"\"\"Trace(s) for each mask. Attributes: Activity (foreign key): Primary key from Activity. Fluorescence.Trace (foreign key): Fluorescence.Trace. activity_trace (longblob): Neural activity from fluoresence trace. \"\"\" definition = \"\"\" -> master -> Fluorescence.Trace --- activity_trace: longblob \"\"\"", "title": "Trace"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity.make", "text": "Populate the Activity with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_preprocess.py 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 def make ( self , key ): \"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : if key [ \"extraction_method\" ] == \"suite2p_deconvolution\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- spikes = [ dict ( key , mask = mask_idx , fluo_channel = 0 , activity_trace = spks , ) for mask_idx , spks in enumerate ( s for plane in suite2p_dataset . planes . values () for s in plane . spks ) ] self . insert1 ( key ) self . Trace . insert ( spikes ) elif method == \"caiman\" : caiman_dataset = imaging_dataset if key [ \"extraction_method\" ] in ( \"caiman_deconvolution\" , \"caiman_dff\" ): attr_mapper = { \"caiman_deconvolution\" : \"spikes\" , \"caiman_dff\" : \"dff\" } # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) self . insert1 ( key ) self . Trace . insert ( dict ( key , mask = mask [ \"mask_id\" ], fluo_channel = segmentation_channel , activity_trace = mask [ attr_mapper [ key [ \"extraction_method\" ]]], ) for mask in caiman_dataset . masks ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ActivityExtractionMethod", "text": "Bases: dj . Lookup Available activity extraction methods. Attributes: Name Type Description extraction_method str Extraction method. Source code in element_calcium_imaging/imaging_preprocess.py 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 @schema class ActivityExtractionMethod ( dj . Lookup ): \"\"\"Available activity extraction methods. Attributes: extraction_method (str): Extraction method. \"\"\" definition = \"\"\"# Activity extraction method extraction_method: varchar(32) \"\"\" contents = zip ([ \"suite2p_deconvolution\" , \"caiman_deconvolution\" , \"caiman_dff\" ])", "title": "ActivityExtractionMethod"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.CellCompartment", "text": "Bases: dj . Lookup Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: Name Type Description cell_compartment str Cell compartment. Source code in element_calcium_imaging/imaging_preprocess.py 344 345 346 347 348 349 350 351 352 353 354 355 356 @schema class CellCompartment ( dj . Lookup ): \"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.) Attributes: cell_compartment (str): Cell compartment. \"\"\" definition = \"\"\"# Cell compartments cell_compartment: char(16) \"\"\" contents = zip ([ \"axon\" , \"soma\" , \"bouton\" ])", "title": "CellCompartment"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Curation", "text": "Bases: dj . Manual Curated results. If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir. Attributes: Name Type Description Processing foreign key Primary key from Processing. curation_id int Unique curation ID. curation_time datetime Time of generation of this set of curated results. curation_output_dir str Output directory of the curated results, relative to root data directory. manual_curation bool If True, manual curation has been performed on this result. curation_note str Notes about the curation task. Source code in element_calcium_imaging/imaging_preprocess.py 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 @schema class Curation ( dj . Manual ): \"\"\"Curated results. If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir. Attributes: Processing (foreign key): Primary key from Processing. curation_id (int): Unique curation ID. curation_time (datetime): Time of generation of this set of curated results. curation_output_dir (str): Output directory of the curated results, relative to root data directory. manual_curation (bool): If True, manual curation has been performed on this result. curation_note (str, optional): Notes about the curation task. \"\"\" definition = \"\"\"# Curation(s) results -> Processing curation_id: int --- curation_time: datetime # Time of generation of this set of curated results curation_output_dir: varchar(255) # Output directory of the curated results, relative to root data directory manual_curation: bool # Has manual curation been performed on this result? curation_note='': varchar(2000) \"\"\" def create1_from_processing_task ( self , key , is_curated = False , curation_note = \"\" ): \"\"\"Create a Curation entry for a given ProcessingTask key. Args: key (dict): Primary key set of an entry in the ProcessingTask table. is_curated (bool): When True, indicates a manual curation. curation_note (str): User's note on the specifics of the curation. \"\"\" if key not in Processing (): raise ValueError ( f \"No corresponding entry in Processing available for: { key } ;\" f \"Please run `Processing.populate(key)`\" ) output_dir = ( ProcessingTask & key ) . fetch1 ( \"processing_output_dir\" ) method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset curation_time = suite2p_dataset . creation_time elif method == \"caiman\" : caiman_dataset = imaging_dataset curation_time = caiman_dataset . creation_time else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : curation_time , \"curation_output_dir\" : output_dir , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "Curation"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Curation.create1_from_processing_task", "text": "Create a Curation entry for a given ProcessingTask key. Parameters: Name Type Description Default key dict Primary key set of an entry in the ProcessingTask table. required is_curated bool When True, indicates a manual curation. False curation_note str User's note on the specifics of the curation. '' Source code in element_calcium_imaging/imaging_preprocess.py 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 def create1_from_processing_task ( self , key , is_curated = False , curation_note = \"\" ): \"\"\"Create a Curation entry for a given ProcessingTask key. Args: key (dict): Primary key set of an entry in the ProcessingTask table. is_curated (bool): When True, indicates a manual curation. curation_note (str): User's note on the specifics of the curation. \"\"\" if key not in Processing (): raise ValueError ( f \"No corresponding entry in Processing available for: { key } ;\" f \"Please run `Processing.populate(key)`\" ) output_dir = ( ProcessingTask & key ) . fetch1 ( \"processing_output_dir\" ) method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset curation_time = suite2p_dataset . creation_time elif method == \"caiman\" : caiman_dataset = imaging_dataset curation_time = caiman_dataset . creation_time else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : curation_time , \"curation_output_dir\" : output_dir , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "create1_from_processing_task()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence", "text": "Bases: dj . Computed Fluorescence traces. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. Source code in element_calcium_imaging/imaging_preprocess.py 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 @schema class Fluorescence ( dj . Computed ): \"\"\"Fluorescence traces. Attributes: Segmentation (foreign key): Primary key from Segmentation. \"\"\" definition = \"\"\"# Fluorescence traces before spike extraction or filtering -> Segmentation \"\"\" class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\" def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "Fluorescence"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence.Trace", "text": "Bases: dj . Part Traces obtained from segmented region of interests. Attributes: Name Type Description Fluorescence foreign key Primary key from Fluorescence. Segmentation.Mask foreign key Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') int The channel that this trace comes from. fluorescence longblob Fluorescence trace associated with this mask. neuropil_fluorescence longblob Neuropil fluorescence trace. Source code in element_calcium_imaging/imaging_preprocess.py 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 class Trace ( dj . Part ): \"\"\"Traces obtained from segmented region of interests. Attributes: Fluorescence (foreign key): Primary key from Fluorescence. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace comes from. fluorescence (longblob): Fluorescence trace associated with this mask. neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask -> scan.Channel.proj(fluo_channel='channel') # The channel that this trace comes from --- fluorescence : longblob # Fluorescence trace associated with this mask neuropil_fluorescence=null : longblob # Neuropil fluorescence trace \"\"\"", "title": "Trace"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence.make", "text": "Populate the Fluorescence with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_preprocess.py 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 def make ( self , key ): \"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- fluo_traces , fluo_chn2_traces = [], [] for s2p in suite2p_dataset . planes . values (): mask_count = len ( fluo_traces ) # increment mask id from all \"plane\" for mask_idx , ( f , fneu ) in enumerate ( zip ( s2p . F , s2p . Fneu )): fluo_traces . append ( { ** key , \"mask\" : mask_idx + mask_count , \"fluo_channel\" : 0 , \"fluorescence\" : f , \"neuropil_fluorescence\" : fneu , } ) if len ( s2p . F_chan2 ): mask_chn2_count = len ( fluo_chn2_traces ) # increment mask id from all planes for mask_idx , ( f2 , fneu2 ) in enumerate ( zip ( s2p . F_chan2 , s2p . Fneu_chan2 ) ): fluo_chn2_traces . append ( { ** key , \"mask\" : mask_idx + mask_chn2_count , \"fluo_channel\" : 1 , \"fluorescence\" : f2 , \"neuropil_fluorescence\" : fneu2 , } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces + fluo_chn2_traces ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) fluo_traces = [] for mask in caiman_dataset . masks : fluo_traces . append ( { ** key , \"mask\" : mask [ \"mask_id\" ], \"fluo_channel\" : segmentation_channel , \"fluorescence\" : mask [ \"inferred_trace\" ], } ) self . insert1 ( key ) self . Trace . insert ( fluo_traces ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassification", "text": "Bases: dj . Computed Classes assigned to each mask. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. MaskClassificationMethod foreign key Primary key from MaskClassificationMethod. Source code in element_calcium_imaging/imaging_preprocess.py 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 @schema class MaskClassification ( dj . Computed ): \"\"\"Classes assigned to each mask. Attributes: Segmentation (foreign key): Primary key from Segmentation. MaskClassificationMethod (foreign key): Primary key from MaskClassificationMethod. \"\"\" definition = \"\"\" -> Segmentation -> MaskClassificationMethod \"\"\" class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\" def make ( self , key ): pass", "title": "MaskClassification"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassification.MaskType", "text": "Bases: dj . Part Type assigned to each mask. Attributes: Name Type Description MaskClassification foreign key Primary key from MaskClassification. Segmentation.Mask foreign key Primary key from Segmentation.Mask. MaskType foreign key Primary key from MaskType. confidence float Confidence level of the mask classification. Source code in element_calcium_imaging/imaging_preprocess.py 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 class MaskType ( dj . Part ): \"\"\"Type assigned to each mask. Attributes: MaskClassification (foreign key): Primary key from MaskClassification. Segmentation.Mask (foreign key): Primary key from Segmentation.Mask. MaskType: Primary key from MaskType. confidence (float, optional): Confidence level of the mask classification. \"\"\" definition = \"\"\" -> master -> Segmentation.Mask --- -> MaskType confidence=null: float \"\"\"", "title": "MaskType"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassificationMethod", "text": "Bases: dj . Lookup Available mask classification methods. Attributes: Name Type Description mask_classification_method str Mask classification method. Source code in element_calcium_imaging/imaging_preprocess.py 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 @schema class MaskClassificationMethod ( dj . Lookup ): \"\"\"Available mask classification methods. Attributes: mask_classification_method (str): Mask classification method. \"\"\" definition = \"\"\" mask_classification_method: varchar(48) \"\"\" contents = zip ([ \"suite2p_default_classifier\" , \"caiman_default_classifier\" ])", "title": "MaskClassificationMethod"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskType", "text": "Bases: dj . Lookup Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: Name Type Description masky_type str Mask type. Source code in element_calcium_imaging/imaging_preprocess.py 359 360 361 362 363 364 365 366 367 368 369 370 371 @schema class MaskType ( dj . Lookup ): \"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil'). Attributes: masky_type (str): Mask type. \"\"\" definition = \"\"\"# Possible types of a segmented mask mask_type: varchar(16) \"\"\" contents = zip ([ \"soma\" , \"axon\" , \"dendrite\" , \"neuropil\" , \"artefact\" , \"unknown\" ])", "title": "MaskType"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection", "text": "Bases: dj . Imported Results of motion correction shifts performed on the imaging data. Attributes: Name Type Description Curation foreign key Primary key from Curation. scan.Channel.proj(motion_correct_channel='channel') int Channel used for motion correction in this processing task. Source code in element_calcium_imaging/imaging_preprocess.py 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 @schema class MotionCorrection ( dj . Imported ): \"\"\"Results of motion correction shifts performed on the imaging data. Attributes: Curation (foreign key): Primary key from Curation. scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for motion correction in this processing task. \"\"\" definition = \"\"\"# Results of motion correction -> Curation --- -> scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task \"\"\" class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\" class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\" def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Curation & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "MotionCorrection"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.Block", "text": "Bases: dj . Part FOV-tiled blocks used for non-rigid motion correction. Attributes: Name Type Description NonRigidMotionCorrection foreign key Primary key from NonRigidMotionCorrection. block_id int Unique block ID. block_y longblob # (y_start, y_end) in pixel of this block block_x longblob # (x_start, x_end) in pixel of this block block_z longblob # (z_start, z_end) in pixel of this block y_shifts longblob # (pixels) y motion correction shifts for every frame x_shifts longblob # (pixels) x motion correction shifts for every frame z_shifts=null longblob # (pixels) x motion correction shifts for every frame y_std float # (pixels) standard deviation of y shifts across all frames x_std float # (pixels) standard deviation of x shifts across all frames z_std=null float # (pixels) standard deviation of z shifts across all frames Source code in element_calcium_imaging/imaging_preprocess.py 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 class Block ( dj . Part ): \"\"\"FOV-tiled blocks used for non-rigid motion correction. Attributes: NonRigidMotionCorrection (foreign key): Primary key from NonRigidMotionCorrection. block_id (int): Unique block ID. block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\" definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction -> master.NonRigidMotionCorrection block_id : int --- block_y : longblob # (y_start, y_end) in pixel of this block block_x : longblob # (x_start, x_end) in pixel of this block block_z : longblob # (z_start, z_end) in pixel of this block y_shifts : longblob # (pixels) y motion correction shifts for every frame x_shifts : longblob # (pixels) x motion correction shifts for every frame z_shifts=null : longblob # (pixels) x motion correction shifts for every frame y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\"", "title": "Block"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.NonRigidMotionCorrection", "text": "Bases: dj . Part Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob , null Mask with true for frames with outlier shifts (already corrected). block_height int Block height in pixels. block_width int Block width in pixels. block_depth int Block depth in pixels. block_count_y int Number of blocks tiled in the y direction. block_count_x int Number of blocks tiled in the x direction. block_count_z int Number of blocks tiled in the z direction. Source code in element_calcium_imaging/imaging_preprocess.py 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 class NonRigidMotionCorrection ( dj . Part ): \"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob, null): Mask with true for frames with outlier shifts (already corrected). block_height (int): Block height in pixels. block_width (int): Block width in pixels. block_depth (int): Block depth in pixels. block_count_y (int): Number of blocks tiled in the y direction. block_count_x (int): Number of blocks tiled in the x direction. block_count_z (int): Number of blocks tiled in the z direction. \"\"\" definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) block_height : int # (pixels) block_width : int # (pixels) block_depth : int # (pixels) block_count_y : int # number of blocks tiled in the y direction block_count_x : int # number of blocks tiled in the x direction block_count_z : int # number of blocks tiled in the z direction \"\"\"", "title": "NonRigidMotionCorrection"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.RigidMotionCorrection", "text": "Bases: dj . Part Details of rigid motion correction performed on the imaging data. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. outlier_frames longblob Mask with true for frames with outlier shifts (already corrected). y_shifts longblob y motion correction shifts (pixels). x_shifts longblob x motion correction shifts (pixels). z_shifts longblob z motion correction shifts (z-drift, pixels). y_std float standard deviation of y shifts across all frames (pixels). x_std float standard deviation of x shifts across all frames (pixels). z_std float standard deviation of z shifts across all frames (pixels). Source code in element_calcium_imaging/imaging_preprocess.py 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 class RigidMotionCorrection ( dj . Part ): \"\"\"Details of rigid motion correction performed on the imaging data. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. outlier_frames (longblob): Mask with true for frames with outlier shifts (already corrected). y_shifts (longblob): y motion correction shifts (pixels). x_shifts (longblob): x motion correction shifts (pixels). z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels). y_std (float): standard deviation of y shifts across all frames (pixels). x_std (float): standard deviation of x shifts across all frames (pixels). z_std (float, optional): standard deviation of z shifts across all frames (pixels). \"\"\" definition = \"\"\"# Details of rigid motion correction performed on the imaging data -> master --- outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected) y_shifts : longblob # (pixels) y motion correction shifts x_shifts : longblob # (pixels) x motion correction shifts z_shifts=null : longblob # (pixels) z motion correction shifts (z-drift) y_std : float # (pixels) standard deviation of y shifts across all frames x_std : float # (pixels) standard deviation of x shifts across all frames z_std=null : float # (pixels) standard deviation of z shifts across all frames \"\"\"", "title": "RigidMotionCorrection"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.Summary", "text": "Bases: dj . Part Summary images for each field and channel after corrections. Attributes: Name Type Description MotionCorrection foreign key Primary key from MotionCorrection. scan.ScanInfo.Field foreign key Primary key from scan.ScanInfo.Field. ref_image longblob Image used as alignment template. average_image longblob Mean of registered frames. correlation_image longblob Correlation map (computed during cell detection). max_proj_image longblob Max of registered frames. Source code in element_calcium_imaging/imaging_preprocess.py 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 class Summary ( dj . Part ): \"\"\"Summary images for each field and channel after corrections. Attributes: MotionCorrection (foreign key): Primary key from MotionCorrection. scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field. ref_image (longblob): Image used as alignment template. average_image (longblob): Mean of registered frames. correlation_image (longblob, optional): Correlation map (computed during cell detection). max_proj_image (longblob, optional): Max of registered frames. \"\"\" definition = \"\"\"# Summary images for each field and channel after corrections -> master -> scan.ScanInfo.Field --- ref_image : longblob # image used as alignment template average_image : longblob # mean of registered frames correlation_image=null : longblob # correlation map (computed during cell detection) max_proj_image=null : longblob # max of registered frames \"\"\"", "title": "Summary"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.make", "text": "Populate MotionCorrection with results parsed from analysis outputs Source code in element_calcium_imaging/imaging_preprocess.py 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 def make ( self , key ): \"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) field_keys , _ = ( scan . ScanInfo . Field & key ) . fetch ( \"KEY\" , \"field_z\" , order_by = \"field_z\" ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset motion_correct_channel = suite2p_dataset . planes [ 0 ] . alignment_channel # ---- iterate through all s2p plane outputs ---- rigid_correction , nonrigid_correction , nonrigid_blocks = {}, {}, {} summary_images = [] for idx , ( plane , s2p ) in enumerate ( suite2p_dataset . planes . items ()): # -- rigid motion correction -- if idx == 0 : rigid_correction = { ** key , \"y_shifts\" : s2p . ops [ \"yoff\" ], \"x_shifts\" : s2p . ops [ \"xoff\" ], \"z_shifts\" : np . full_like ( s2p . ops [ \"xoff\" ], 0 ), \"y_std\" : np . nanstd ( s2p . ops [ \"yoff\" ]), \"x_std\" : np . nanstd ( s2p . ops [ \"xoff\" ]), \"z_std\" : np . nan , \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : rigid_correction [ \"y_shifts\" ] = np . vstack ( [ rigid_correction [ \"y_shifts\" ], s2p . ops [ \"yoff\" ]] ) rigid_correction [ \"y_std\" ] = np . nanstd ( rigid_correction [ \"y_shifts\" ] . flatten () ) rigid_correction [ \"x_shifts\" ] = np . vstack ( [ rigid_correction [ \"x_shifts\" ], s2p . ops [ \"xoff\" ]] ) rigid_correction [ \"x_std\" ] = np . nanstd ( rigid_correction [ \"x_shifts\" ] . flatten () ) rigid_correction [ \"outlier_frames\" ] = np . logical_or ( rigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) # -- non-rigid motion correction -- if s2p . ops [ \"nonrigid\" ]: if idx == 0 : nonrigid_correction = { ** key , \"block_height\" : s2p . ops [ \"block_size\" ][ 0 ], \"block_width\" : s2p . ops [ \"block_size\" ][ 1 ], \"block_depth\" : 1 , \"block_count_y\" : s2p . ops [ \"nblocks\" ][ 0 ], \"block_count_x\" : s2p . ops [ \"nblocks\" ][ 1 ], \"block_count_z\" : len ( suite2p_dataset . planes ), \"outlier_frames\" : s2p . ops [ \"badframes\" ], } else : nonrigid_correction [ \"outlier_frames\" ] = np . logical_or ( nonrigid_correction [ \"outlier_frames\" ], s2p . ops [ \"badframes\" ] ) for b_id , ( b_y , b_x , bshift_y , bshift_x ) in enumerate ( zip ( s2p . ops [ \"xblock\" ], s2p . ops [ \"yblock\" ], s2p . ops [ \"yoff1\" ] . T , s2p . ops [ \"xoff1\" ] . T , ) ): if b_id in nonrigid_blocks : nonrigid_blocks [ b_id ][ \"y_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"y_shifts\" ], bshift_y ] ) nonrigid_blocks [ b_id ][ \"y_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"y_shifts\" ] . flatten () ) nonrigid_blocks [ b_id ][ \"x_shifts\" ] = np . vstack ( [ nonrigid_blocks [ b_id ][ \"x_shifts\" ], bshift_x ] ) nonrigid_blocks [ b_id ][ \"x_std\" ] = np . nanstd ( nonrigid_blocks [ b_id ][ \"x_shifts\" ] . flatten () ) else : nonrigid_blocks [ b_id ] = { ** key , \"block_id\" : b_id , \"block_y\" : b_y , \"block_x\" : b_x , \"block_z\" : np . full_like ( b_x , plane ), \"y_shifts\" : bshift_y , \"x_shifts\" : bshift_x , \"z_shifts\" : np . full ( ( len ( suite2p_dataset . planes ), len ( bshift_x )), 0 ), \"y_std\" : np . nanstd ( bshift_y ), \"x_std\" : np . nanstd ( bshift_x ), \"z_std\" : np . nan , } # -- summary images -- motion_correction_key = ( scan . ScanInfo . Field * Curation & key & field_keys [ plane ] ) . fetch1 ( \"KEY\" ) summary_images . append ( { ** motion_correction_key , \"ref_image\" : s2p . ref_image , \"average_image\" : s2p . mean_image , \"correlation_image\" : s2p . correlation_map , \"max_proj_image\" : s2p . max_proj_image , } ) self . insert1 ({ ** key , \"motion_correct_channel\" : motion_correct_channel }) if rigid_correction : self . RigidMotionCorrection . insert1 ( rigid_correction ) if nonrigid_correction : self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks . values ()) self . Summary . insert ( summary_images ) elif method == \"caiman\" : caiman_dataset = imaging_dataset self . insert1 ( { ** key , \"motion_correct_channel\" : caiman_dataset . alignment_channel } ) is3D = caiman_dataset . params . motion [ \"is3D\" ] if not caiman_dataset . params . motion [ \"pw_rigid\" ]: # -- rigid motion correction -- rigid_correction = { ** key , \"x_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], \"y_shifts\" : caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ], 0 ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 0 ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 1 ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"shifts_rig\" ][:, 2 ]) if is3D else np . nan ), \"outlier_frames\" : None , } self . RigidMotionCorrection . insert1 ( rigid_correction ) else : # -- non-rigid motion correction -- nonrigid_correction = { ** key , \"block_height\" : ( caiman_dataset . params . motion [ \"strides\" ][ 0 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 0 ] ), \"block_width\" : ( caiman_dataset . params . motion [ \"strides\" ][ 1 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 1 ] ), \"block_depth\" : ( caiman_dataset . params . motion [ \"strides\" ][ 2 ] + caiman_dataset . params . motion [ \"overlaps\" ][ 2 ] if is3D else 1 ), \"block_count_x\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 0 ]) ), \"block_count_y\" : len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][:, 2 ]) ), \"block_count_z\" : ( len ( set ( caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ :, 4 ] ) ) if is3D else 1 ), \"outlier_frames\" : None , } nonrigid_blocks = [] for b_id in range ( len ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ 0 , :]) ): nonrigid_blocks . append ( { ** key , \"block_id\" : b_id , \"block_x\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), \"block_y\" : np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 2 : 4 ] ), \"block_z\" : ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 4 : 6 ] ) if is3D else np . full_like ( np . arange ( * caiman_dataset . motion_correction [ \"coord_shifts_els\" ][ b_id , 0 : 2 ] ), 0 , ) ), \"x_shifts\" : caiman_dataset . motion_correction [ \"x_shifts_els\" ][:, b_id ], \"y_shifts\" : caiman_dataset . motion_correction [ \"y_shifts_els\" ][:, b_id ], \"z_shifts\" : ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] if is3D else np . full_like ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ], 0 , ) ), \"x_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"x_shifts_els\" ][ :, b_id ] ), \"y_std\" : np . nanstd ( caiman_dataset . motion_correction [ \"y_shifts_els\" ][ :, b_id ] ), \"z_std\" : ( np . nanstd ( caiman_dataset . motion_correction [ \"z_shifts_els\" ][ :, b_id ] ) if is3D else np . nan ), } ) self . NonRigidMotionCorrection . insert1 ( nonrigid_correction ) self . Block . insert ( nonrigid_blocks ) # -- summary images -- summary_images = [ { ** key , ** fkey , \"ref_image\" : ref_image , \"average_image\" : ave_img , \"correlation_image\" : corr_img , \"max_proj_image\" : max_img , } for fkey , ref_image , ave_img , corr_img , max_img in zip ( field_keys , caiman_dataset . motion_correction [ \"reference_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"reference_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"average_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"average_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"correlation_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"correlation_image\" ][ ... ][ np . newaxis , ... ], caiman_dataset . motion_correction [ \"max_image\" ] . transpose ( 2 , 0 , 1 ) if is3D else caiman_dataset . motion_correction [ \"max_image\" ][ ... ][ np . newaxis , ... ], ) ] self . Summary . insert ( summary_images ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ))", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Preprocess", "text": "Bases: dj . Imported Perform the computation of an entry (task) defined in the PreprocessTask table. If task_mode == \"none\" : no pre-processing performed If task_mode == \"trigger\" : Not implemented If task_mode == \"load\" : Not implemented Attributes: Name Type Description PreprocessTask foreign key preprocess_time datetime package_version str Version of the analysis package used in processing the data. Source code in element_calcium_imaging/imaging_preprocess.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 @schema class Preprocess ( dj . Imported ): \"\"\"Perform the computation of an entry (task) defined in the PreprocessTask table. + If `task_mode == \"none\"`: no pre-processing performed + If `task_mode == \"trigger\"`: Not implemented + If `task_mode == \"load\"`: Not implemented Attributes: PreprocessTask (foreign key): preprocess_time (datetime, optional): package_version (str, optional): Version of the analysis package used in processing the data. \"\"\" definition = \"\"\" -> PreprocessTask --- preprocess_time=null: datetime # Time of generation of pre-processing results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Execute the preprocessing analysis steps defined in PreprocessTask.\"\"\" task_mode , output_dir = ( PreprocessTask & key ) . fetch1 ( \"task_mode\" , \"preprocess_output_dir\" ) preprocess_output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) if task_mode == \"none\" : print ( f \"No pre-processing run on entry: { key } \" ) elif task_mode in [ \"load\" , \"trigger\" ]: raise NotImplementedError ( \"Pre-processing steps are not implemented.\" \"Please overwrite this `make` function with\" \"desired pre-processing steps.\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key )", "title": "Preprocess"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Preprocess.make", "text": "Execute the preprocessing analysis steps defined in PreprocessTask. Source code in element_calcium_imaging/imaging_preprocess.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def make ( self , key ): \"\"\"Execute the preprocessing analysis steps defined in PreprocessTask.\"\"\" task_mode , output_dir = ( PreprocessTask & key ) . fetch1 ( \"task_mode\" , \"preprocess_output_dir\" ) preprocess_output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) if task_mode == \"none\" : print ( f \"No pre-processing run on entry: { key } \" ) elif task_mode in [ \"load\" , \"trigger\" ]: raise NotImplementedError ( \"Pre-processing steps are not implemented.\" \"Please overwrite this `make` function with\" \"desired pre-processing steps.\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key )", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessMethod", "text": "Bases: dj . Lookup Method(s) used for preprocessing of calcium imaging data. Attributes: Name Type Description preprocess_method str Preprocessing method. preprocess_method_desc str Processing method description. Source code in element_calcium_imaging/imaging_preprocess.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 @schema class PreprocessMethod ( dj . Lookup ): \"\"\"Method(s) used for preprocessing of calcium imaging data. Attributes: preprocess_method (str): Preprocessing method. preprocess_method_desc (str): Processing method description. \"\"\" definition = \"\"\" # Method/package used for pre-processing preprocess_method: varchar(16) --- preprocess_method_desc: varchar(1000) \"\"\"", "title": "PreprocessMethod"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSet", "text": "Bases: dj . Lookup Parameter set used for the preprocessing of the calcium imaging scans. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: Name Type Description paramset_idx int Uniqiue parameter set ID. PreprocessMethod foreign key A primary key from PreprocessMethod. paramset_desc str Parameter set description. param_set_hash uuid A universally unique identifier for the parameter set. params longblob Parameter Set, a dictionary of all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_preprocess.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 @schema class PreprocessParamSet ( dj . Lookup ): \"\"\"Parameter set used for the preprocessing of the calcium imaging scans. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: paramset_idx (int): Uniqiue parameter set ID. PreprocessMethod (foreign key): A primary key from PreprocessMethod. paramset_desc (str): Parameter set description. param_set_hash (uuid): A universally unique identifier for the parameter set. params (longblob): Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" definition = \"\"\" # Parameter set used for pre-processing of calcium imaging data paramset_idx: smallint --- -> PreprocessMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , preprocess_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into PreprocessParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: preprocess_method (str): Method used for processing of calcium imaging scans. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters. \"\"\" param_dict = { \"preprocess_method\" : preprocess_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict )", "title": "PreprocessParamSet"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSet.insert_new_params", "text": "Insert a parameter set into PreprocessParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: Name Type Description preprocess_method str Method used for processing of calcium imaging scans. paramset_idx int Uniqiue parameter set ID. paramset_desc str Parameter set description. params dict Parameter Set, all applicable parameters. Source code in element_calcium_imaging/imaging_preprocess.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 @classmethod def insert_new_params ( cls , preprocess_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into PreprocessParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: preprocess_method (str): Method used for processing of calcium imaging scans. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters. \"\"\" param_dict = { \"preprocess_method\" : preprocess_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict )", "title": "insert_new_params()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSteps", "text": "Bases: dj . Manual Ordered list of paramset_idx that will be run. When pre-processing is not performed, do not create an entry in Step Part table Attributes: Name Type Description preprocess_param_steps_id int preprocess_param_steps_name str preprocess_param_steps_desc str Source code in element_calcium_imaging/imaging_preprocess.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @schema class PreprocessParamSteps ( dj . Manual ): \"\"\"Ordered list of paramset_idx that will be run. When pre-processing is not performed, do not create an entry in `Step` Part table Attributes: preprocess_param_steps_id (int): preprocess_param_steps_name (str): preprocess_param_steps_desc (str): \"\"\" definition = \"\"\" preprocess_param_steps_id: smallint --- preprocess_param_steps_name: varchar(32) preprocess_param_steps_desc: varchar(128) \"\"\" class Step ( dj . Part ): \"\"\"ADD DEFINITION Attributes: PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps. step_number (int): PreprocessParamSet (foreign key): A primary key from PreprocessParamSet. \"\"\" definition = \"\"\" -> master step_number: smallint # Order of operations --- -> PreprocessParamSet \"\"\"", "title": "PreprocessParamSteps"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSteps.Step", "text": "Bases: dj . Part ADD DEFINITION Attributes: Name Type Description PreprocessParamSteps foreign key A primary key from PreprocessParamSteps. step_number int PreprocessParamSet foreign key A primary key from PreprocessParamSet. Source code in element_calcium_imaging/imaging_preprocess.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class Step ( dj . Part ): \"\"\"ADD DEFINITION Attributes: PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps. step_number (int): PreprocessParamSet (foreign key): A primary key from PreprocessParamSet. \"\"\" definition = \"\"\" -> master step_number: smallint # Order of operations --- -> PreprocessParamSet \"\"\"", "title": "Step"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessTask", "text": "Bases: dj . Manual This table defines a calcium imaging preprocessing task for a combination of a Scan and a PreprocessParamSteps entries, including all the inputs (scan, method, steps). The task defined here is then run in the downstream table Preprocess. This table supports definitions of both loading of pre-generated, results, triggering of new analysis, or skipping of preprocessing step. Attributes: Name Type Description Scan foreign key A primary key from Scan. PreprocessParamSteps foreign key A primary key from PreprocessParamSteps. preprocess_output_dir str Output directory for the results of preprocessing. task_mode str One of 'load' (load computed analysis results), 'trigger' (trigger computation), 'none' (no pre-processing). Default none. Source code in element_calcium_imaging/imaging_preprocess.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 @schema class PreprocessTask ( dj . Manual ): \"\"\"This table defines a calcium imaging preprocessing task for a combination of a `Scan` and a `PreprocessParamSteps` entries, including all the inputs (scan, method, steps). The task defined here is then run in the downstream table Preprocess. This table supports definitions of both loading of pre-generated, results, triggering of new analysis, or skipping of preprocessing step. Attributes: Scan (foreign key): A primary key from Scan. PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps. preprocess_output_dir (str): Output directory for the results of preprocessing. task_mode (str, optional): One of 'load' (load computed analysis results), 'trigger' (trigger computation), 'none' (no pre-processing). Default none. \"\"\" definition = \"\"\" # Manual table for defining a pre-processing task ready to be run -> scan.Scan -> PreprocessParamSteps --- preprocess_output_dir: varchar(255) # Pre-processing output directory relative # to the root data directory task_mode='none': enum('none','load', 'trigger') # 'none': no pre-processing # 'load': load analysis results # 'trigger': trigger computation \"\"\"", "title": "PreprocessTask"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing", "text": "Bases: dj . Computed Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: Name Type Description ProcessingTask foreign key Primary key from ProcessingTask. processing_time datetime Process completion datetime. package_version str Version of the analysis package used in processing the data. Source code in element_calcium_imaging/imaging_preprocess.py 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 @schema class Processing ( dj . Computed ): \"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted. Attributes: ProcessingTask (foreign key): Primary key from ProcessingTask. processing_time (datetime): Process completion datetime. package_version (str, optional): Version of the analysis package used in processing the data. \"\"\" definition = \"\"\" -> ProcessingTask --- processing_time : datetime # Time of generation of this set of processed, segmented results package_version='' : varchar(16) \"\"\" # Run processing only on Scan with ScanInfo inserted @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) preprocess_paramsets = ( PreprocessParamSteps . Step () & dict ( preprocess_param_steps_id = key [ \"preprocess_param_steps_id\" ]) ) . fetch ( \"paramset_idx\" ) if len ( preprocess_paramsets ) == 0 : # No pre-processing steps were performed on the acquired dataset, so process the raw/acquired files. image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] else : preprocess_output_dir = ( PreprocessTask & key ) . fetch1 ( \"preprocess_output_dir\" ) preprocess_output_dir = find_full_path ( get_imaging_root_data_dir (), preprocess_output_dir ) if not preprocess_output_dir . exists (): raise FileNotFoundError ( f \"Pre-processed output directory not found ( { preprocess_output_dir } )\" ) image_files = list ( preprocess_output_dir . glob ( \"*.tif\" )) if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key )", "title": "Processing"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing.key_source", "text": "Limit the Processing to Scans that have their metadata ingested to the database. Source code in element_calcium_imaging/imaging_preprocess.py 511 512 513 514 515 516 @property def key_source ( self ): \"\"\"Limit the Processing to Scans that have their metadata ingested to the database.\"\"\" return ProcessingTask & scan . ScanInfo", "title": "key_source()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing.make", "text": "Execute the calcium imaging analysis defined by the ProcessingTask. Source code in element_calcium_imaging/imaging_preprocess.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 def make ( self , key ): \"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\" task_mode , output_dir = ( ProcessingTask & key ) . fetch1 ( \"task_mode\" , \"processing_output_dir\" ) output_dir = find_full_path ( get_imaging_root_data_dir (), output_dir ) . as_posix () if not output_dir : output_dir = ProcessingTask . infer_output_dir ( key , relative = True , mkdir = True ) # update processing_output_dir ProcessingTask . update1 ( { ** key , \"processing_output_dir\" : output_dir . as_posix ()} ) if task_mode == \"load\" : method , imaging_dataset = get_loader_result ( key , ProcessingTask ) if method == \"suite2p\" : if ( scan . ScanInfo & key ) . fetch1 ( \"nrois\" ) > 0 : raise NotImplementedError ( f \"Suite2p ingestion error - Unable to handle\" f \" ScanImage multi-ROI scanning mode yet\" ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : caiman_dataset = imaging_dataset key = { ** key , \"processing_time\" : caiman_dataset . creation_time } else : raise NotImplementedError ( \"Unknown method: {} \" . format ( method )) elif task_mode == \"trigger\" : method = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"processing_method\" ) preprocess_paramsets = ( PreprocessParamSteps . Step () & dict ( preprocess_param_steps_id = key [ \"preprocess_param_steps_id\" ]) ) . fetch ( \"paramset_idx\" ) if len ( preprocess_paramsets ) == 0 : # No pre-processing steps were performed on the acquired dataset, so process the raw/acquired files. image_files = ( scan . ScanInfo . ScanFile & key ) . fetch ( \"file_path\" ) image_files = [ find_full_path ( get_imaging_root_data_dir (), image_file ) for image_file in image_files ] else : preprocess_output_dir = ( PreprocessTask & key ) . fetch1 ( \"preprocess_output_dir\" ) preprocess_output_dir = find_full_path ( get_imaging_root_data_dir (), preprocess_output_dir ) if not preprocess_output_dir . exists (): raise FileNotFoundError ( f \"Pre-processed output directory not found ( { preprocess_output_dir } )\" ) image_files = list ( preprocess_output_dir . glob ( \"*.tif\" )) if method == \"suite2p\" : import suite2p suite2p_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) suite2p_params [ \"save_path0\" ] = output_dir suite2p_params [ \"fs\" ], suite2p_params [ \"nplanes\" ], suite2p_params [ \"nchannels\" ] = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" , \"nchannels\" ) input_format = pathlib . Path ( image_files [ 0 ]) . suffix suite2p_params [ \"input_format\" ] = input_format [ 1 :] suite2p_paths = { \"data_path\" : [ image_files [ 0 ] . parent . as_posix ()], \"tiff_list\" : [ f . as_posix () for f in image_files ], } suite2p . run_s2p ( ops = suite2p_params , db = suite2p_paths ) # Run suite2p _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) suite2p_dataset = imaging_dataset key = { ** key , \"processing_time\" : suite2p_dataset . creation_time } elif method == \"caiman\" : from element_interface.run_caiman import run_caiman caiman_params = ( ProcessingTask * ProcessingParamSet & key ) . fetch1 ( \"params\" ) sampling_rate , ndepths = ( scan . ScanInfo & key ) . fetch1 ( \"fps\" , \"ndepths\" ) is3D = bool ( ndepths > 1 ) if is3D : raise NotImplementedError ( \"Caiman pipeline is not yet capable of analyzing 3D scans.\" ) run_caiman ( file_paths = [ f . as_posix () for f in image_files ], parameters = caiman_params , sampling_rate = sampling_rate , output_dir = output_dir , is3D = is3D , ) _ , imaging_dataset = get_loader_result ( key , ProcessingTask ) caiman_dataset = imaging_dataset key [ \"processing_time\" ] = caiman_dataset . creation_time else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ( key )", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingParamSet", "text": "Bases: dj . Lookup Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: Name Type Description paramset_idx int Uniqiue parameter set ID. ProcessingMethod foreign key A primary key from ProcessingMethod. paramset_desc str Parameter set description. param_set_hash uuid A universally unique identifier for the parameter set. params longblob Parameter Set, a dictionary of all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_preprocess.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 @schema class ProcessingParamSet ( dj . Lookup ): \"\"\"Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters. A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries. Attributes: paramset_idx (int): Uniqiue parameter set ID. ProcessingMethod (foreign key): A primary key from ProcessingMethod. paramset_desc (str): Parameter set description. param_set_hash (uuid): A universally unique identifier for the parameter set. params (longblob): Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" definition = \"\"\"# Processing Parameter Set paramset_idx: smallint # Uniqiue parameter set ID. --- -> ProcessingMethod paramset_desc: varchar(1280) # Parameter-set description param_set_hash: uuid # A universally unique identifier for the parameter set params: longblob # Parameter Set, a dictionary of all applicable parameters to the analysis suite. \"\"\" @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict )", "title": "ProcessingParamSet"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingParamSet.insert_new_params", "text": "Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: Name Type Description processing_method str Processing method/package used for processing of calcium imaging. paramset_idx int Uniqiue parameter set ID. paramset_desc str Parameter set description. params dict Parameter Set, all applicable parameters to the analysis suite. Source code in element_calcium_imaging/imaging_preprocess.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Insert a parameter set into ProcessingParamSet table. This function automizes the parameter set hashing and avoids insertion of an existing parameter set. Attributes: processing_method (str): Processing method/package used for processing of calcium imaging. paramset_idx (int): Uniqiue parameter set ID. paramset_desc (str): Parameter set description. params (dict): Parameter Set, all applicable parameters to the analysis suite. \"\"\" param_dict = { \"processing_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } q_param = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if q_param : # If the specified param-set already exists pname = q_param . fetch1 ( \"paramset_idx\" ) if pname == paramset_idx : # If the existed set has the same name: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set already exists - name: {} \" . format ( pname ) ) else : cls . insert1 ( param_dict )", "title": "insert_new_params()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask", "text": "Bases: dj . Manual This table defines a calcium imaging processing task for a combination of a Preprocess and a ProcessingParamSet entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: Name Type Description Preprocess foreign key ProcessingParamSet foreign key processing_output_dir str task_mode str One of 'load' (load computed analysis results) or 'trigger' (trigger computation). Source code in element_calcium_imaging/imaging_preprocess.py 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 @schema class ProcessingTask ( dj . Manual ): \"\"\"This table defines a calcium imaging processing task for a combination of a `Preprocess` and a `ProcessingParamSet` entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods Attributes: Preprocess (foreign key): ProcessingParamSet (foreign key): processing_output_dir (str): task_mode (str): One of 'load' (load computed analysis results) or 'trigger' (trigger computation). \"\"\" definition = \"\"\"# Manual table for defining a processing task ready to be run -> Preprocess -> ProcessingParamSet --- processing_output_dir: varchar(255) # Output directory of the processed scan relative to root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. mkdir (bool): If True, create the processing_output_dir directory. Returns: A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } ) auto_generate_entries = generate", "title": "ProcessingTask"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask.generate", "text": "Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Parameters: Name Type Description Default scan_key dict Primary key from Scan table. required paramset_idx int Unique parameter set ID. 0 Source code in element_calcium_imaging/imaging_preprocess.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 @classmethod def generate ( cls , scan_key , paramset_idx = 0 ): \"\"\"Generate a default ProcessingTask entry for a particular Scan using an existing parameter set in the ProcessingParamSet table. Args: scan_key (dict): Primary key from Scan table. paramset_idx (int): Unique parameter set ID. \"\"\" key = { ** scan_key , \"paramset_idx\" : paramset_idx } output_dir = cls . infer_output_dir ( key , relative = False , mkdir = True ) method = ( ProcessingParamSet & { \"paramset_idx\" : paramset_idx }) . fetch1 ( \"processing_method\" ) try : if method == \"suite2p\" : from element_interface import suite2p_loader suite2p_loader . Suite2p ( output_dir ) elif method == \"caiman\" : from element_interface import caiman_loader caiman_loader . CaImAn ( output_dir ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method ) ) except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"processing_output_dir\" : output_dir , \"task_mode\" : task_mode , } )", "title": "generate()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask.infer_output_dir", "text": "Infer an output directory for an entry in ProcessingTask table. Parameters: Name Type Description Default key dict Primary key from the ProcessingTask table. required relative bool If True, processing_output_dir is returned relative to imaging_root_dir. False mkdir bool If True, create the processing_output_dir directory. False Returns: Type Description A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 Source code in element_calcium_imaging/imaging_preprocess.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer an output directory for an entry in ProcessingTask table. Args: key (dict): Primary key from the ProcessingTask table. relative (bool): If True, processing_output_dir is returned relative to imaging_root_dir. mkdir (bool): If True, create the processing_output_dir directory. Returns: A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0 \"\"\" image_locators = { \"NIS\" : get_nd2_files , \"ScanImage\" : get_scan_image_files , \"Scanbox\" : get_scan_box_files , } image_locator = image_locators [( scan . Scan & key ) . fetch1 ( \"acq_software\" )] scan_dir = find_full_path ( get_imaging_root_data_dir (), image_locator ( key )[ 0 ] ) . parent root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_dir ) method = ( ( ProcessingParamSet & key ) . fetch1 ( \"processing_method\" ) . replace ( \".\" , \"-\" ) ) processed_dir = pathlib . Path ( get_processed_root_data_dir ()) output_dir = ( processed_dir / scan_dir . relative_to ( root_dir ) / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir", "title": "infer_output_dir()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation", "text": "Bases: dj . Computed Result of the Segmentation process. Attributes: Name Type Description Curation foreign key Primary key from Curation. Source code in element_calcium_imaging/imaging_preprocess.py 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 @schema class Segmentation ( dj . Computed ): \"\"\"Result of the Segmentation process. Attributes: Curation (foreign key): Primary key from Curation. \"\"\" definition = \"\"\"# Different mask segmentations. -> Curation \"\"\" class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\" def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" )", "title": "Segmentation"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation.Mask", "text": "Bases: dj . Part Details of the masks identified from the Segmentation procedure. Attributes: Name Type Description Segmentation foreign key Primary key from Segmentation. mask int Unique mask ID. scan.Channel.proj(segmentation_channel='channel') foreign key Channel used for segmentation. mask_npix int Number of pixels in ROIs. mask_center_x int Center x coordinate in pixel. mask_center_y int Center y coordinate in pixel. mask_center_z int Center z coordinate in pixel. mask_xpix longblob X coordinates in pixels. mask_ypix longblob Y coordinates in pixels. mask_zpix longblob Z coordinates in pixels. mask_weights longblob Weights of the mask at the indices above. Source code in element_calcium_imaging/imaging_preprocess.py 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 class Mask ( dj . Part ): \"\"\"Details of the masks identified from the Segmentation procedure. Attributes: Segmentation (foreign key): Primary key from Segmentation. mask (int): Unique mask ID. scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel used for segmentation. mask_npix (int): Number of pixels in ROIs. mask_center_x (int): Center x coordinate in pixel. mask_center_y (int): Center y coordinate in pixel. mask_center_z (int): Center z coordinate in pixel. mask_xpix (longblob): X coordinates in pixels. mask_ypix (longblob): Y coordinates in pixels. mask_zpix (longblob): Z coordinates in pixels. mask_weights (longblob): Weights of the mask at the indices above. \"\"\" definition = \"\"\" # A mask produced by segmentation. -> master mask : smallint --- -> scan.Channel.proj(segmentation_channel='channel') # channel used for segmentation mask_npix : int # number of pixels in ROIs mask_center_x : int # center x coordinate in pixel mask_center_y : int # center y coordinate in pixel mask_center_z : int # center z coordinate in pixel mask_xpix : longblob # x coordinates in pixels mask_ypix : longblob # y coordinates in pixels mask_zpix : longblob # z coordinates in pixels mask_weights : longblob # weights of the mask at the indices above \"\"\"", "title": "Mask"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation.make", "text": "Populate the Segmentation with the results parsed from analysis outputs. Source code in element_calcium_imaging/imaging_preprocess.py 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 def make ( self , key ): \"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\" method , imaging_dataset = get_loader_result ( key , Curation ) if method == \"suite2p\" : suite2p_dataset = imaging_dataset # ---- iterate through all s2p plane outputs ---- masks , cells = [], [] for plane , s2p in suite2p_dataset . planes . items (): mask_count = len ( masks ) # increment mask id from all \"plane\" for mask_idx , ( is_cell , cell_prob , mask_stat ) in enumerate ( zip ( s2p . iscell , s2p . cell_prob , s2p . stat ) ): masks . append ( { ** key , \"mask\" : mask_idx + mask_count , \"segmentation_channel\" : s2p . segmentation_channel , \"mask_npix\" : mask_stat [ \"npix\" ], \"mask_center_x\" : mask_stat [ \"med\" ][ 1 ], \"mask_center_y\" : mask_stat [ \"med\" ][ 0 ], \"mask_center_z\" : mask_stat . get ( \"iplane\" , plane ), \"mask_xpix\" : mask_stat [ \"xpix\" ], \"mask_ypix\" : mask_stat [ \"ypix\" ], \"mask_zpix\" : np . full ( mask_stat [ \"npix\" ], mask_stat . get ( \"iplane\" , plane ) ), \"mask_weights\" : mask_stat [ \"lam\" ], } ) if is_cell : cells . append ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" , \"mask\" : mask_idx + mask_count , \"mask_type\" : \"soma\" , \"confidence\" : cell_prob , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"suite2p_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) elif method == \"caiman\" : caiman_dataset = imaging_dataset # infer \"segmentation_channel\" - from params if available, else from caiman loader params = ( ProcessingParamSet * ProcessingTask & key ) . fetch1 ( \"params\" ) segmentation_channel = params . get ( \"segmentation_channel\" , caiman_dataset . segmentation_channel ) masks , cells = [], [] for mask in caiman_dataset . masks : masks . append ( { ** key , \"segmentation_channel\" : segmentation_channel , \"mask\" : mask [ \"mask_id\" ], \"mask_npix\" : mask [ \"mask_npix\" ], \"mask_center_x\" : mask [ \"mask_center_x\" ], \"mask_center_y\" : mask [ \"mask_center_y\" ], \"mask_center_z\" : mask [ \"mask_center_z\" ], \"mask_xpix\" : mask [ \"mask_xpix\" ], \"mask_ypix\" : mask [ \"mask_ypix\" ], \"mask_zpix\" : mask [ \"mask_zpix\" ], \"mask_weights\" : mask [ \"mask_weights\" ], } ) if caiman_dataset . cnmf . estimates . idx_components is not None : if mask [ \"mask_id\" ] in caiman_dataset . cnmf . estimates . idx_components : cells . append ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" , \"mask\" : mask [ \"mask_id\" ], \"mask_type\" : \"soma\" , } ) self . insert1 ( key ) self . Mask . insert ( masks , ignore_extra_fields = True ) if cells : MaskClassification . insert1 ( { ** key , \"mask_classification_method\" : \"caiman_default_classifier\" }, allow_direct_insert = True , ) MaskClassification . MaskType . insert ( cells , ignore_extra_fields = True , allow_direct_insert = True ) else : raise NotImplementedError ( f \"Unknown/unimplemented method: { method } \" )", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.activate", "text": "Activate this schema. Parameters: Name Type Description Default imaging_schema_name str Schema name on the database server to activate the imaging module. required scan_schema_name str Schema name on the database server to activate the scan module. Omitted, if the scan module is already activated. None create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the imaging module: + all that are required by the scan module. None Dependencies: Upstream tables Session: A parent table to Scan, identifying a scanning session. Equipment: A parent table to Scan, identifying a scanning device. Source code in element_calcium_imaging/imaging_preprocess.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def activate ( imaging_schema_name , scan_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None , ): \"\"\"Activate this schema. Args: imaging_schema_name (str): Schema name on the database server to activate the `imaging` module. scan_schema_name (str): Schema name on the database server to activate the `scan` module. Omitted, if the `scan` module is already activated. create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `imaging` module: + all that are required by the `scan` module. Dependencies: Upstream tables: + Session: A parent table to Scan, identifying a scanning session. + Equipment: A parent table to Scan, identifying a scanning device. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module scan . activate ( scan_schema_name , create_schema = create_schema , create_tables = create_tables , linking_module = linking_module , ) schema . activate ( imaging_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) imaging_report . activate ( f \" { imaging_schema_name } _report\" , imaging_schema_name )", "title": "activate()"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_loader_result", "text": "Retrieve the processed imaging results from a suite2p or caiman loader. Parameters: Name Type Description Default key dict The key to one entry of ProcessingTask or Curation required table dj . Table A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) required Raises: Type Description NotImplementedError If the processing_method is different than 'suite2p' or 'caiman'. Returns: Type Description A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) Source code in element_calcium_imaging/imaging_preprocess.py 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 def get_loader_result ( key : dict , table : dj . Table ): \"\"\"Retrieve the processed imaging results from a suite2p or caiman loader. Args: key (dict): The `key` to one entry of ProcessingTask or Curation table (dj.Table): A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation) Raises: NotImplementedError: If the processing_method is different than 'suite2p' or 'caiman'. Returns: A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn, see element-interface for more information on the loaders.) \"\"\" method , output_dir = ( ProcessingParamSet * table & key ) . fetch1 ( \"processing_method\" , _table_attribute_mapper [ table . __name__ ] ) output_path = find_full_path ( get_imaging_root_data_dir (), output_dir ) if method == \"suite2p\" : from element_interface import suite2p_loader loaded_dataset = suite2p_loader . Suite2p ( output_path ) elif method == \"caiman\" : from element_interface import caiman_loader loaded_dataset = caiman_loader . CaImAn ( output_path ) else : raise NotImplementedError ( \"Unknown/unimplemented method: {} \" . format ( method )) return method , loaded_dataset", "title": "get_loader_result()"}, {"location": "api/element_calcium_imaging/imaging_report/", "text": "ScanLevelReport \u00b6 Bases: dj . Computed Scan level report with figures. Attributes: Name Type Description imaging.Segmentation foreign key Primary key from imaging.Segmentation. cell_overlayed_image longblob Plotly figure object showing the segmented cells on the average image. Source code in element_calcium_imaging/imaging_report.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @schema class ScanLevelReport ( dj . Computed ): \"\"\"Scan level report with figures. Attributes: imaging.Segmentation (foreign key): Primary key from imaging.Segmentation. cell_overlayed_image (longblob): Plotly figure object showing the segmented cells on the average image. \"\"\" definition = \"\"\" -> imaging.Segmentation --- cell_overlayed_image: longblob \"\"\" def make ( self , key ): \"\"\"Compute and ingest the plotly figure objects.\"\"\" image_fig = cell_plot . plot_cell_overlayed_image ( imaging , key ) self . insert1 ({ ** key , \"cell_overlayed_image\" : image_fig . to_json ()}) make ( key ) \u00b6 Compute and ingest the plotly figure objects. Source code in element_calcium_imaging/imaging_report.py 51 52 53 54 55 def make ( self , key ): \"\"\"Compute and ingest the plotly figure objects.\"\"\" image_fig = cell_plot . plot_cell_overlayed_image ( imaging , key ) self . insert1 ({ ** key , \"cell_overlayed_image\" : image_fig . to_json ()}) TraceReport \u00b6 Bases: dj . Computed Figures of traces. Attributes: Name Type Description imaging.Segmentation.Mask foreign key Primary key from imaging.Segmentation.Mask. cell_traces longblob Plotly figure object showing the cell traces. Source code in element_calcium_imaging/imaging_report.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @schema class TraceReport ( dj . Computed ): \"\"\"Figures of traces. Attributes: imaging.Segmentation.Mask (foreign key): Primary key from imaging.Segmentation.Mask. cell_traces (longblob): Plotly figure object showing the cell traces. \"\"\" definition = \"\"\" -> imaging.Segmentation.Mask --- cell_traces: longblob \"\"\" def make ( self , key ): \"\"\"Compute and ingest the plotly figure objects.\"\"\" trace_fig = cell_plot . plot_cell_traces ( imaging , key ) self . insert1 ({ ** key , \"cell_traces\" : trace_fig . to_json ()}) make ( key ) \u00b6 Compute and ingest the plotly figure objects. Source code in element_calcium_imaging/imaging_report.py 74 75 76 77 78 def make ( self , key ): \"\"\"Compute and ingest the plotly figure objects.\"\"\" trace_fig = cell_plot . plot_cell_traces ( imaging , key ) self . insert1 ({ ** key , \"cell_traces\" : trace_fig . to_json ()}) activate ( schema_name , imaging_schema_name , * , create_schema = True , create_tables = True ) \u00b6 Activate this schema. Parameters: Name Type Description Default schema_name str Schema name on the database server to activate the imaging_report schema required imaging_schema_name str Schema name of the activated imaging element for which this imaging_report schema will be downstream from required create_schema When True (default), create schema in the database if it does not yet exist. True create_tables When True (default), create tables in the database if they do not yet exist. True Source code in element_calcium_imaging/imaging_report.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def activate ( schema_name , imaging_schema_name , * , create_schema = True , create_tables = True ): \"\"\"Activate this schema. Args: schema_name (str): Schema name on the database server to activate the `imaging_report` schema imaging_schema_name (str): Schema name of the activated imaging element for which this imaging_report schema will be downstream from create_schema: When True (default), create schema in the database if it does not yet exist. create_tables: When True (default), create tables in the database if they do not yet exist. \"\"\" global imaging imaging = dj . create_virtual_module ( \"imaging\" , imaging_schema_name ) schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = imaging . __dict__ , )", "title": "imaging_report.py"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.ScanLevelReport", "text": "Bases: dj . Computed Scan level report with figures. Attributes: Name Type Description imaging.Segmentation foreign key Primary key from imaging.Segmentation. cell_overlayed_image longblob Plotly figure object showing the segmented cells on the average image. Source code in element_calcium_imaging/imaging_report.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @schema class ScanLevelReport ( dj . Computed ): \"\"\"Scan level report with figures. Attributes: imaging.Segmentation (foreign key): Primary key from imaging.Segmentation. cell_overlayed_image (longblob): Plotly figure object showing the segmented cells on the average image. \"\"\" definition = \"\"\" -> imaging.Segmentation --- cell_overlayed_image: longblob \"\"\" def make ( self , key ): \"\"\"Compute and ingest the plotly figure objects.\"\"\" image_fig = cell_plot . plot_cell_overlayed_image ( imaging , key ) self . insert1 ({ ** key , \"cell_overlayed_image\" : image_fig . to_json ()})", "title": "ScanLevelReport"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.ScanLevelReport.make", "text": "Compute and ingest the plotly figure objects. Source code in element_calcium_imaging/imaging_report.py 51 52 53 54 55 def make ( self , key ): \"\"\"Compute and ingest the plotly figure objects.\"\"\" image_fig = cell_plot . plot_cell_overlayed_image ( imaging , key ) self . insert1 ({ ** key , \"cell_overlayed_image\" : image_fig . to_json ()})", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.TraceReport", "text": "Bases: dj . Computed Figures of traces. Attributes: Name Type Description imaging.Segmentation.Mask foreign key Primary key from imaging.Segmentation.Mask. cell_traces longblob Plotly figure object showing the cell traces. Source code in element_calcium_imaging/imaging_report.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @schema class TraceReport ( dj . Computed ): \"\"\"Figures of traces. Attributes: imaging.Segmentation.Mask (foreign key): Primary key from imaging.Segmentation.Mask. cell_traces (longblob): Plotly figure object showing the cell traces. \"\"\" definition = \"\"\" -> imaging.Segmentation.Mask --- cell_traces: longblob \"\"\" def make ( self , key ): \"\"\"Compute and ingest the plotly figure objects.\"\"\" trace_fig = cell_plot . plot_cell_traces ( imaging , key ) self . insert1 ({ ** key , \"cell_traces\" : trace_fig . to_json ()})", "title": "TraceReport"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.TraceReport.make", "text": "Compute and ingest the plotly figure objects. Source code in element_calcium_imaging/imaging_report.py 74 75 76 77 78 def make ( self , key ): \"\"\"Compute and ingest the plotly figure objects.\"\"\" trace_fig = cell_plot . plot_cell_traces ( imaging , key ) self . insert1 ({ ** key , \"cell_traces\" : trace_fig . to_json ()})", "title": "make()"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.activate", "text": "Activate this schema. Parameters: Name Type Description Default schema_name str Schema name on the database server to activate the imaging_report schema required imaging_schema_name str Schema name of the activated imaging element for which this imaging_report schema will be downstream from required create_schema When True (default), create schema in the database if it does not yet exist. True create_tables When True (default), create tables in the database if they do not yet exist. True Source code in element_calcium_imaging/imaging_report.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def activate ( schema_name , imaging_schema_name , * , create_schema = True , create_tables = True ): \"\"\"Activate this schema. Args: schema_name (str): Schema name on the database server to activate the `imaging_report` schema imaging_schema_name (str): Schema name of the activated imaging element for which this imaging_report schema will be downstream from create_schema: When True (default), create schema in the database if it does not yet exist. create_tables: When True (default), create tables in the database if they do not yet exist. \"\"\" global imaging imaging = dj . create_virtual_module ( \"imaging\" , imaging_schema_name ) schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = imaging . __dict__ , )", "title": "activate()"}, {"location": "api/element_calcium_imaging/scan/", "text": "AcquisitionSoftware \u00b6 Bases: dj . Lookup A list of acquisition softwares supported by the Element. Required to define a scan. Attributes: Name Type Description acq_software str Acquistion software Source code in element_calcium_imaging/scan.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"A list of acquisition softwares supported by the Element. Required to define a scan. Attributes: acq_software (str): Acquistion software \"\"\" definition = \"\"\" # Acquisition softwares acq_software: varchar(24) \"\"\" contents = zip ([ \"ScanImage\" , \"Scanbox\" , \"NIS\" , \"PrairieView\" ]) Channel \u00b6 Bases: dj . Lookup Recording channels for the imaging wavelengths. Attributes: Name Type Description channel int Channel index Source code in element_calcium_imaging/scan.py 169 170 171 172 173 174 175 176 177 178 179 180 @schema class Channel ( dj . Lookup ): \"\"\"Recording channels for the imaging wavelengths. Attributes: channel (int): Channel index \"\"\" definition = \"\"\" # A recording channel channel : tinyint # 0-based indexing \"\"\" contents = zip ( range ( 5 )) Scan \u00b6 Bases: dj . Manual Scan defined by a measurement done using a scanner and an acquisition software. The details of the scanning data is placed in other tables, including, ScanLocation, ScanInfo, and ScanInfo's part tables. Attributes: Name Type Description Session foreign key A primary key from Session. scan_id int Unique Scan ID. Equipment foreign key A primary key from Equipment. AcquisitionSoftware foreign key A primary key from AcquisitonSoftware. scan_notes str Notes of the experimenter regarding the scan. Source code in element_calcium_imaging/scan.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @schema class Scan ( dj . Manual ): \"\"\"Scan defined by a measurement done using a scanner and an acquisition software. The details of the scanning data is placed in other tables, including, ScanLocation, ScanInfo, and ScanInfo's part tables. Attributes: Session (foreign key): A primary key from Session. scan_id (int): Unique Scan ID. Equipment (foreign key, optional): A primary key from Equipment. AcquisitionSoftware (foreign key): A primary key from AcquisitonSoftware. scan_notes (str, optional): Notes of the experimenter regarding the scan. \"\"\" definition = \"\"\" -> Session scan_id: int --- -> [nullable] Equipment -> AcquisitionSoftware scan_notes='' : varchar(4095) \"\"\" ScanInfo \u00b6 Bases: dj . Imported Information about the scan extracted from the recorded files. Attributes: Name Type Description Scan foreign key A primary key from Scan. nfields int Number of fields. nchannels int Number of channels. ndepths int Number of scanning depths (planes). nframes int Number of recorded frames. nrois int Number of ROIs (see scanimage's multi ROI imaging). x float ScanImage's 0 point in the motor coordinate system (um). y float ScanImage's 0 point in the motor coordinate system (um). z float ScanImage's 0 point in the motor coordinate system (um). fps float) Frames per second (Hz) - Volumetric Scan Rate. bidirectional bool True = bidirectional scanning. usecs_per_line float Microseconds per scan line. fill_fraction float Raster scan temporal fill fraction (see scanimage) scan_datetime datetime Datetime of the scan. scan_duration float Duration of the scan (s). bidirectional_z bool True = bidirectional z-scan. Source code in element_calcium_imaging/scan.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 @schema class ScanInfo ( dj . Imported ): \"\"\" Information about the scan extracted from the recorded files. Attributes: Scan (foreign key): A primary key from Scan. nfields (int): Number of fields. nchannels (int): Number of channels. ndepths (int): Number of scanning depths (planes). nframes (int): Number of recorded frames. nrois (int): Number of ROIs (see scanimage's multi ROI imaging). x (float, optional): ScanImage's 0 point in the motor coordinate system (um). y (float, optional): ScanImage's 0 point in the motor coordinate system (um). z (float, optional): ScanImage's 0 point in the motor coordinate system (um). fps (float) : Frames per second (Hz) - Volumetric Scan Rate. bidirectional (bool): True = bidirectional scanning. usecs_per_line (float, optional): Microseconds per scan line. fill_fraction (float, optional): Raster scan temporal fill fraction (see scanimage) scan_datetime (datetime, optional): Datetime of the scan. scan_duration (float, optional): Duration of the scan (s). bidirectional_z (bool, optional): True = bidirectional z-scan. \"\"\" definition = \"\"\" # General data about the reso/meso scans from header -> Scan --- nfields : tinyint # number of fields nchannels : tinyint # number of channels ndepths : int # Number of scanning depths (planes) nframes : int # number of recorded frames nrois : tinyint # number of ROIs (see scanimage's multi ROI imaging) x=null : float # (um) ScanImage's 0 point in the motor coordinate system y=null : float # (um) ScanImage's 0 point in the motor coordinate system z=null : float # (um) ScanImage's 0 point in the motor coordinate system fps : float # (Hz) frames per second - Volumetric Scan Rate bidirectional : boolean # true = bidirectional scanning usecs_per_line=null : float # microseconds per scan line fill_fraction=null : float # raster scan temporal fill fraction (see scanimage) scan_datetime=null : datetime # datetime of the scan scan_duration=null : float # (seconds) duration of the scan bidirectional_z=null : boolean # true = bidirectional z-scan \"\"\" class Field ( dj . Part ): \"\"\"Stores field information of scan, including its coordinates, size, pixel pitch, etc. Attributes: ScanInfo (foreign key): A primary key from ScanInfo. field_idx (int): Unique field index. px_height (int): Image height in pixels. px_width (int): Image width in pixels. um_height (float, optional): Image height in microns. um_width (float, optional): Image width in microns. field_x (float, optional): X coordinate of the center of field in the motor coordinate system (um). field_y (float, optional): Y coordinate of the center of field in the motor coordinate system (um). field_z (float, optional): Relative depth of field (um). delay_image (longblob, optional): Delay between the start of the scan and pixels in this field (ms). roi (int, optional): The scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans. \"\"\" definition = \"\"\" # field-specific scan information -> master field_idx : int --- px_height : smallint # height in pixels px_width : smallint # width in pixels um_height=null : float # height in microns um_width=null : float # width in microns field_x=null : float # (um) center of field in the motor coordinate system field_y=null : float # (um) center of field in the motor coordinate system field_z=null : float # (um) relative depth of field delay_image=null : longblob # (ms) delay between the start of the scan and pixels in this field roi=null : int # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans \"\"\" class ScanFile ( dj . Part ): \"\"\"Filepath of the scan relative to root data directory. Attributes: ScanInfo (foreign key): A primary key from ScanInfo. file_path (str): Path of the scan file relative to the root data directory. \"\"\" definition = \"\"\" -> master file_path: varchar(255) # Filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populate the ScanInfo with the information parsed from image files.\"\"\" acq_software = ( Scan & key ) . fetch1 ( \"acq_software\" ) if acq_software == \"ScanImage\" : import scanreader # Read the scan scan_filepaths = get_scan_image_files ( key ) scan = scanreader . read_scan ( scan_filepaths ) # Insert in ScanInfo x_zero = ( scan . motor_position_at_zero [ 0 ] if scan . motor_position_at_zero else None ) y_zero = ( scan . motor_position_at_zero [ 1 ] if scan . motor_position_at_zero else None ) z_zero = ( scan . motor_position_at_zero [ 2 ] if scan . motor_position_at_zero else None ) self . insert1 ( dict ( key , nfields = scan . num_fields , nchannels = scan . num_channels , nframes = scan . num_frames , ndepths = scan . num_scanning_depths , x = x_zero , y = y_zero , z = z_zero , fps = scan . fps , bidirectional = scan . is_bidirectional , usecs_per_line = scan . seconds_per_line * 1e6 , fill_fraction = scan . temporal_fill_fraction , nrois = scan . num_rois if scan . is_multiROI else 0 , scan_duration = scan . num_frames / scan . fps , ) ) # Insert Field(s) if scan . is_multiROI : self . Field . insert ( [ dict ( key , field_idx = field_id , px_height = scan . field_heights [ field_id ], px_width = scan . field_widths [ field_id ], um_height = scan . field_heights_in_microns [ field_id ], um_width = scan . field_widths_in_microns [ field_id ], field_x = x_zero + scan . _degrees_to_microns ( scan . fields [ field_id ] . x ) if x_zero else None , field_y = y_zero + scan . _degrees_to_microns ( scan . fields [ field_id ] . y ) if y_zero else None , field_z = z_zero + scan . fields [ field_id ] . depth if z_zero else None , delay_image = scan . field_offsets [ field_id ], roi = scan . field_rois [ field_id ][ 0 ], ) for field_id in range ( scan . num_fields ) ] ) else : self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = scan . image_height , px_width = scan . image_width , um_height = getattr ( scan , \"image_height_in_microns\" , None ), um_width = getattr ( scan , \"image_width_in_microns\" , None ), field_x = x_zero if x_zero else None , field_y = y_zero if y_zero else None , field_z = z_zero + scan . scanning_depths [ plane_idx ] if z_zero else None , delay_image = scan . field_offsets [ plane_idx ], ) for plane_idx in range ( scan . num_scanning_depths ) ] ) elif acq_software == \"Scanbox\" : import sbxreader # Read the scan scan_filepaths = get_scan_box_files ( key ) sbx_meta = sbxreader . sbx_get_metadata ( scan_filepaths [ 0 ]) sbx_matinfo = sbxreader . sbx_get_info ( scan_filepaths [ 0 ]) is_multiROI = bool ( sbx_matinfo . mesoscope . enabled ) # currently not handling \"multiROI\" ingestion if is_multiROI : raise NotImplementedError ( \"Loading routine not implemented for Scanbox multiROI scan mode\" ) # Insert in ScanInfo x_zero , y_zero , z_zero = sbx_meta [ \"stage_pos\" ] self . insert1 ( dict ( key , nfields = sbx_meta [ \"num_fields\" ] if is_multiROI else sbx_meta [ \"num_planes\" ], nchannels = sbx_meta [ \"num_channels\" ], nframes = sbx_meta [ \"num_frames\" ], ndepths = sbx_meta [ \"num_planes\" ], x = x_zero , y = y_zero , z = z_zero , fps = sbx_meta [ \"frame_rate\" ], bidirectional = sbx_meta == \"bidirectional\" , nrois = sbx_meta [ \"num_rois\" ] if is_multiROI else 0 , scan_duration = ( sbx_meta [ \"num_frames\" ] / sbx_meta [ \"frame_rate\" ]), ) ) # Insert Field(s) if not is_multiROI : px_width , px_height = sbx_meta [ \"frame_size\" ] self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = px_height , px_width = px_width , um_height = px_height * sbx_meta [ \"um_per_pixel_y\" ] if sbx_meta [ \"um_per_pixel_y\" ] else None , um_width = px_width * sbx_meta [ \"um_per_pixel_x\" ] if sbx_meta [ \"um_per_pixel_x\" ] else None , field_x = x_zero , field_y = y_zero , field_z = z_zero + sbx_meta [ \"etl_pos\" ][ plane_idx ], ) for plane_idx in range ( sbx_meta [ \"num_planes\" ]) ] ) elif acq_software == \"NIS\" : import nd2 # Read the scan scan_filepaths = get_nd2_files ( key ) nd2_file = nd2 . ND2File ( scan_filepaths [ 0 ]) is_multiROI = False # MultiROI to be implemented later # Frame per second try : fps = 1000 / nd2_file . experiment [ 0 ] . parameters . periods [ 0 ] . periodDiff . avg except : fps = 1000 / nd2_file . experiment [ 0 ] . parameters . periodDiff . avg # Estimate ND2 file scan duration def estimate_nd2_scan_duration ( nd2_scan_obj ): # Calculates scan duration for Nikon images ti = ( nd2_scan_obj . frame_metadata ( 0 ) . channels [ 0 ] . time . absoluteJulianDayNumber ) # Initial frame's JD. tf = ( nd2_scan_obj . frame_metadata ( nd2_scan_obj . shape [ 0 ] - 1 ) . channels [ 0 ] . time . absoluteJulianDayNumber ) # Final frame's JD. return ( tf - ti ) * 86400 + 1 / fps scan_duration = sum ( estimate_nd2_scan_duration ( nd2 . ND2File ( f )) for f in scan_filepaths ) try : scan_datetime = nd2_file . text_info [ \"date\" ] scan_datetime = datetime . strptime ( scan_datetime , \"%m/ %d /%Y %H:%M:%S %p\" if re . search (( \"AM|PM\" ), scan_datetime ) else \"%m/ %d /%Y %H:%M:%S\" , ) scan_datetime = datetime . strftime ( scan_datetime , \"%Y-%m- %d %H:%M:%S\" ) except : scan_datetime = None # Insert in ScanInfo self . insert1 ( dict ( key , nfields = nd2_file . sizes . get ( \"P\" , 1 ), nchannels = nd2_file . attributes . channelCount , nframes = nd2_file . metadata . contents . frameCount , ndepths = nd2_file . sizes . get ( \"Z\" , 1 ), x = None , y = None , z = None , fps = fps , bidirectional = bool ( nd2_file . custom_data [ \"GrabberCameraSettingsV1_0\" ][ \"GrabberCameraSettings\" ][ \"PropertiesQuality\" ][ \"ScanDirection\" ] ), nrois = 0 , scan_datetime = scan_datetime , scan_duration = scan_duration , ) ) # MultiROI to be implemented later # Insert in Field if not is_multiROI : self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = nd2_file . attributes . heightPx , px_width = nd2_file . attributes . widthPx , um_height = nd2_file . attributes . heightPx * nd2_file . voxel_size () . y , um_width = nd2_file . attributes . widthPx * nd2_file . voxel_size () . x , field_x = None , field_y = None , field_z = None , ) for plane_idx in range ( nd2_file . sizes . get ( \"Z\" , 1 )) ] ) elif acq_software == \"PrairieView\" : from element_interface import prairieviewreader scan_filepaths = get_prairieview_files ( key ) pvscan_info = prairieviewreader . get_pv_metadata ( scan_filepaths [ 0 ]) self . insert1 ( dict ( key , nfields = pvscan_info [ \"num_fields\" ], nchannels = pvscan_info [ \"num_channels\" ], ndepths = pvscan_info [ \"num_planes\" ], nframes = pvscan_info [ \"num_frames\" ], nrois = pvscan_info [ \"num_rois\" ], x = pvscan_info [ \"x_pos\" ], y = pvscan_info [ \"y_pos\" ], z = pvscan_info [ \"z_pos\" ], fps = pvscan_info [ \"frame_rate\" ], bidirectional = pvscan_info [ \"bidirectional\" ], bidirectional_z = pvscan_info [ \"bidirectional_z\" ], usecs_per_line = pvscan_info [ \"usecs_per_line\" ], scan_datetime = pvscan_info [ \"scan_datetime\" ], scan_duration = pvscan_info [ \"scan_duration\" ], ) ) self . Field . insert ( dict ( key , field_idx = plane_idx , px_height = pvscan_info [ \"height_in_pixels\" ], px_width = pvscan_info [ \"width_in_pixels\" ], um_height = pvscan_info [ \"height_in_um\" ], um_width = pvscan_info [ \"width_in_um\" ], field_x = pvscan_info [ \"fieldX\" ], field_y = pvscan_info [ \"fieldY\" ], field_z = pvscan_info [ \"fieldZ\" ][ plane_idx ], ) for plane_idx in range ( pvscan_info [ \"num_planes\" ]) ) else : raise NotImplementedError ( f \"Loading routine not implemented for { acq_software } \" \"acquisition software\" ) # Insert file(s) root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_filepaths [ 0 ]) scan_files = [ pathlib . Path ( f ) . relative_to ( root_dir ) . as_posix () for f in scan_filepaths ] self . ScanFile . insert ([{ ** key , \"file_path\" : f } for f in scan_files ]) Field \u00b6 Bases: dj . Part Stores field information of scan, including its coordinates, size, pixel pitch, etc. Attributes: Name Type Description ScanInfo foreign key A primary key from ScanInfo. field_idx int Unique field index. px_height int Image height in pixels. px_width int Image width in pixels. um_height float Image height in microns. um_width float Image width in microns. field_x float X coordinate of the center of field in the motor coordinate system (um). field_y float Y coordinate of the center of field in the motor coordinate system (um). field_z float Relative depth of field (um). delay_image longblob Delay between the start of the scan and pixels in this field (ms). roi int The scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans. Source code in element_calcium_imaging/scan.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 class Field ( dj . Part ): \"\"\"Stores field information of scan, including its coordinates, size, pixel pitch, etc. Attributes: ScanInfo (foreign key): A primary key from ScanInfo. field_idx (int): Unique field index. px_height (int): Image height in pixels. px_width (int): Image width in pixels. um_height (float, optional): Image height in microns. um_width (float, optional): Image width in microns. field_x (float, optional): X coordinate of the center of field in the motor coordinate system (um). field_y (float, optional): Y coordinate of the center of field in the motor coordinate system (um). field_z (float, optional): Relative depth of field (um). delay_image (longblob, optional): Delay between the start of the scan and pixels in this field (ms). roi (int, optional): The scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans. \"\"\" definition = \"\"\" # field-specific scan information -> master field_idx : int --- px_height : smallint # height in pixels px_width : smallint # width in pixels um_height=null : float # height in microns um_width=null : float # width in microns field_x=null : float # (um) center of field in the motor coordinate system field_y=null : float # (um) center of field in the motor coordinate system field_z=null : float # (um) relative depth of field delay_image=null : longblob # (ms) delay between the start of the scan and pixels in this field roi=null : int # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans \"\"\" ScanFile \u00b6 Bases: dj . Part Filepath of the scan relative to root data directory. Attributes: Name Type Description ScanInfo foreign key A primary key from ScanInfo. file_path str Path of the scan file relative to the root data directory. Source code in element_calcium_imaging/scan.py 306 307 308 309 310 311 312 313 314 315 316 317 class ScanFile ( dj . Part ): \"\"\"Filepath of the scan relative to root data directory. Attributes: ScanInfo (foreign key): A primary key from ScanInfo. file_path (str): Path of the scan file relative to the root data directory. \"\"\" definition = \"\"\" -> master file_path: varchar(255) # Filepath relative to root data directory \"\"\" make ( key ) \u00b6 Populate the ScanInfo with the information parsed from image files. Source code in element_calcium_imaging/scan.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 def make ( self , key ): \"\"\"Populate the ScanInfo with the information parsed from image files.\"\"\" acq_software = ( Scan & key ) . fetch1 ( \"acq_software\" ) if acq_software == \"ScanImage\" : import scanreader # Read the scan scan_filepaths = get_scan_image_files ( key ) scan = scanreader . read_scan ( scan_filepaths ) # Insert in ScanInfo x_zero = ( scan . motor_position_at_zero [ 0 ] if scan . motor_position_at_zero else None ) y_zero = ( scan . motor_position_at_zero [ 1 ] if scan . motor_position_at_zero else None ) z_zero = ( scan . motor_position_at_zero [ 2 ] if scan . motor_position_at_zero else None ) self . insert1 ( dict ( key , nfields = scan . num_fields , nchannels = scan . num_channels , nframes = scan . num_frames , ndepths = scan . num_scanning_depths , x = x_zero , y = y_zero , z = z_zero , fps = scan . fps , bidirectional = scan . is_bidirectional , usecs_per_line = scan . seconds_per_line * 1e6 , fill_fraction = scan . temporal_fill_fraction , nrois = scan . num_rois if scan . is_multiROI else 0 , scan_duration = scan . num_frames / scan . fps , ) ) # Insert Field(s) if scan . is_multiROI : self . Field . insert ( [ dict ( key , field_idx = field_id , px_height = scan . field_heights [ field_id ], px_width = scan . field_widths [ field_id ], um_height = scan . field_heights_in_microns [ field_id ], um_width = scan . field_widths_in_microns [ field_id ], field_x = x_zero + scan . _degrees_to_microns ( scan . fields [ field_id ] . x ) if x_zero else None , field_y = y_zero + scan . _degrees_to_microns ( scan . fields [ field_id ] . y ) if y_zero else None , field_z = z_zero + scan . fields [ field_id ] . depth if z_zero else None , delay_image = scan . field_offsets [ field_id ], roi = scan . field_rois [ field_id ][ 0 ], ) for field_id in range ( scan . num_fields ) ] ) else : self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = scan . image_height , px_width = scan . image_width , um_height = getattr ( scan , \"image_height_in_microns\" , None ), um_width = getattr ( scan , \"image_width_in_microns\" , None ), field_x = x_zero if x_zero else None , field_y = y_zero if y_zero else None , field_z = z_zero + scan . scanning_depths [ plane_idx ] if z_zero else None , delay_image = scan . field_offsets [ plane_idx ], ) for plane_idx in range ( scan . num_scanning_depths ) ] ) elif acq_software == \"Scanbox\" : import sbxreader # Read the scan scan_filepaths = get_scan_box_files ( key ) sbx_meta = sbxreader . sbx_get_metadata ( scan_filepaths [ 0 ]) sbx_matinfo = sbxreader . sbx_get_info ( scan_filepaths [ 0 ]) is_multiROI = bool ( sbx_matinfo . mesoscope . enabled ) # currently not handling \"multiROI\" ingestion if is_multiROI : raise NotImplementedError ( \"Loading routine not implemented for Scanbox multiROI scan mode\" ) # Insert in ScanInfo x_zero , y_zero , z_zero = sbx_meta [ \"stage_pos\" ] self . insert1 ( dict ( key , nfields = sbx_meta [ \"num_fields\" ] if is_multiROI else sbx_meta [ \"num_planes\" ], nchannels = sbx_meta [ \"num_channels\" ], nframes = sbx_meta [ \"num_frames\" ], ndepths = sbx_meta [ \"num_planes\" ], x = x_zero , y = y_zero , z = z_zero , fps = sbx_meta [ \"frame_rate\" ], bidirectional = sbx_meta == \"bidirectional\" , nrois = sbx_meta [ \"num_rois\" ] if is_multiROI else 0 , scan_duration = ( sbx_meta [ \"num_frames\" ] / sbx_meta [ \"frame_rate\" ]), ) ) # Insert Field(s) if not is_multiROI : px_width , px_height = sbx_meta [ \"frame_size\" ] self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = px_height , px_width = px_width , um_height = px_height * sbx_meta [ \"um_per_pixel_y\" ] if sbx_meta [ \"um_per_pixel_y\" ] else None , um_width = px_width * sbx_meta [ \"um_per_pixel_x\" ] if sbx_meta [ \"um_per_pixel_x\" ] else None , field_x = x_zero , field_y = y_zero , field_z = z_zero + sbx_meta [ \"etl_pos\" ][ plane_idx ], ) for plane_idx in range ( sbx_meta [ \"num_planes\" ]) ] ) elif acq_software == \"NIS\" : import nd2 # Read the scan scan_filepaths = get_nd2_files ( key ) nd2_file = nd2 . ND2File ( scan_filepaths [ 0 ]) is_multiROI = False # MultiROI to be implemented later # Frame per second try : fps = 1000 / nd2_file . experiment [ 0 ] . parameters . periods [ 0 ] . periodDiff . avg except : fps = 1000 / nd2_file . experiment [ 0 ] . parameters . periodDiff . avg # Estimate ND2 file scan duration def estimate_nd2_scan_duration ( nd2_scan_obj ): # Calculates scan duration for Nikon images ti = ( nd2_scan_obj . frame_metadata ( 0 ) . channels [ 0 ] . time . absoluteJulianDayNumber ) # Initial frame's JD. tf = ( nd2_scan_obj . frame_metadata ( nd2_scan_obj . shape [ 0 ] - 1 ) . channels [ 0 ] . time . absoluteJulianDayNumber ) # Final frame's JD. return ( tf - ti ) * 86400 + 1 / fps scan_duration = sum ( estimate_nd2_scan_duration ( nd2 . ND2File ( f )) for f in scan_filepaths ) try : scan_datetime = nd2_file . text_info [ \"date\" ] scan_datetime = datetime . strptime ( scan_datetime , \"%m/ %d /%Y %H:%M:%S %p\" if re . search (( \"AM|PM\" ), scan_datetime ) else \"%m/ %d /%Y %H:%M:%S\" , ) scan_datetime = datetime . strftime ( scan_datetime , \"%Y-%m- %d %H:%M:%S\" ) except : scan_datetime = None # Insert in ScanInfo self . insert1 ( dict ( key , nfields = nd2_file . sizes . get ( \"P\" , 1 ), nchannels = nd2_file . attributes . channelCount , nframes = nd2_file . metadata . contents . frameCount , ndepths = nd2_file . sizes . get ( \"Z\" , 1 ), x = None , y = None , z = None , fps = fps , bidirectional = bool ( nd2_file . custom_data [ \"GrabberCameraSettingsV1_0\" ][ \"GrabberCameraSettings\" ][ \"PropertiesQuality\" ][ \"ScanDirection\" ] ), nrois = 0 , scan_datetime = scan_datetime , scan_duration = scan_duration , ) ) # MultiROI to be implemented later # Insert in Field if not is_multiROI : self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = nd2_file . attributes . heightPx , px_width = nd2_file . attributes . widthPx , um_height = nd2_file . attributes . heightPx * nd2_file . voxel_size () . y , um_width = nd2_file . attributes . widthPx * nd2_file . voxel_size () . x , field_x = None , field_y = None , field_z = None , ) for plane_idx in range ( nd2_file . sizes . get ( \"Z\" , 1 )) ] ) elif acq_software == \"PrairieView\" : from element_interface import prairieviewreader scan_filepaths = get_prairieview_files ( key ) pvscan_info = prairieviewreader . get_pv_metadata ( scan_filepaths [ 0 ]) self . insert1 ( dict ( key , nfields = pvscan_info [ \"num_fields\" ], nchannels = pvscan_info [ \"num_channels\" ], ndepths = pvscan_info [ \"num_planes\" ], nframes = pvscan_info [ \"num_frames\" ], nrois = pvscan_info [ \"num_rois\" ], x = pvscan_info [ \"x_pos\" ], y = pvscan_info [ \"y_pos\" ], z = pvscan_info [ \"z_pos\" ], fps = pvscan_info [ \"frame_rate\" ], bidirectional = pvscan_info [ \"bidirectional\" ], bidirectional_z = pvscan_info [ \"bidirectional_z\" ], usecs_per_line = pvscan_info [ \"usecs_per_line\" ], scan_datetime = pvscan_info [ \"scan_datetime\" ], scan_duration = pvscan_info [ \"scan_duration\" ], ) ) self . Field . insert ( dict ( key , field_idx = plane_idx , px_height = pvscan_info [ \"height_in_pixels\" ], px_width = pvscan_info [ \"width_in_pixels\" ], um_height = pvscan_info [ \"height_in_um\" ], um_width = pvscan_info [ \"width_in_um\" ], field_x = pvscan_info [ \"fieldX\" ], field_y = pvscan_info [ \"fieldY\" ], field_z = pvscan_info [ \"fieldZ\" ][ plane_idx ], ) for plane_idx in range ( pvscan_info [ \"num_planes\" ]) ) else : raise NotImplementedError ( f \"Loading routine not implemented for { acq_software } \" \"acquisition software\" ) # Insert file(s) root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_filepaths [ 0 ]) scan_files = [ pathlib . Path ( f ) . relative_to ( root_dir ) . as_posix () for f in scan_filepaths ] self . ScanFile . insert ([{ ** key , \"file_path\" : f } for f in scan_files ]) ScanLocation \u00b6 Bases: dj . Manual Anatomical location of the scanned region in the brain Attributes: Name Type Description Scan foreign key A primary key from Scan. Locaton foreign key A primary key from Location. Source code in element_calcium_imaging/scan.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 @schema class ScanLocation ( dj . Manual ): \"\"\"Anatomical location of the scanned region in the brain Attributes: Scan (foreign key): A primary key from Scan. Locaton (foreign key): A primary key from Location. \"\"\" definition = \"\"\"Anatomical location -> Scan --- -> Location \"\"\" activate ( scan_schema_name , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activate this schema. Parameters: Name Type Description Default scan_schema_name str Schema name on the database server to activate the scan module required create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the scan module. None Dependencies: Upstream tables Session: Parent table to Scan, typically identifying a recording session Equipment: Reference table for Scan, specifying the equipment used for the acquisition of this scan. Location: Reference table for ScanLocation, specifying the scanned regions's anatomical location in the brain. Source code in element_calcium_imaging/scan.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def activate ( scan_schema_name , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\"Activate this schema. Args: scan_schema_name (str): Schema name on the database server to activate the `scan` module create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `scan` module. Dependencies: Upstream tables: + Session: Parent table to Scan, typically identifying a recording session + Equipment: Reference table for Scan, specifying the equipment used for the acquisition of this scan. + Location: Reference table for ScanLocation, specifying the scanned regions's anatomical location in the brain. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module schema . activate ( scan_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) get_imaging_root_data_dir () \u00b6 Return imaging root data director(y/ies) Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations). Returns: Name Type Description dirs list A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies). Source code in element_calcium_imaging/scan.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def get_imaging_root_data_dir () -> list : \"\"\"Return imaging root data director(y/ies) Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations). Returns: dirs (list): A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies). \"\"\" root_directories = _linking_module . get_imaging_root_data_dir () if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ root_directories ] if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): root_directories . append ( _linking_module . get_processed_root_data_dir ()) return root_directories get_nd2_files ( scan_key ) \u00b6 Retrieve the list of Nikon files (*.nd2) associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key of a Scan entry. required Returns: Type Description list A list of Nikon files' full file-paths. Source code in element_calcium_imaging/scan.py 126 127 128 129 130 131 132 133 134 135 def get_nd2_files ( scan_key : dict ) -> list : \"\"\"Retrieve the list of Nikon files (*.nd2) associated with a given Scan. Args: scan_key: Primary key of a Scan entry. Returns: A list of Nikon files' full file-paths. \"\"\" return _linking_module . get_nd2_files ( scan_key ) get_prairieview_files ( scan_key ) \u00b6 Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key of a Scan entry. required Returns: Type Description list A list of Bruker PrairieView files' full file-paths. Source code in element_calcium_imaging/scan.py 138 139 140 141 142 143 144 145 146 147 def get_prairieview_files ( scan_key : dict ) -> list : \"\"\"Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan. Args: scan_key: Primary key of a Scan entry. Returns: A list of Bruker PrairieView files' full file-paths. \"\"\" return _linking_module . get_prairieview_files ( scan_key ) get_processed_root_data_dir () \u00b6 Retrieve the root directory for all processed data. All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations). Returns: Name Type Description dir str | pathlib . Path Absolute path of the pocessed imaging root data directory. Source code in element_calcium_imaging/scan.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def get_processed_root_data_dir () -> Union [ str , pathlib . Path ]: \"\"\"Retrieve the root directory for all processed data. All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations). Returns: dir (str| pathlib.Path): Absolute path of the pocessed imaging root data directory. \"\"\" if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): return _linking_module . get_processed_root_data_dir () else : return get_imaging_root_data_dir ()[ 0 ] get_scan_box_files ( scan_key ) \u00b6 Retrieve the list of Scanbox files (*.sbx) associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key of a Scan entry. required Returns: Type Description list A list of Scanbox files' full file-paths. Source code in element_calcium_imaging/scan.py 114 115 116 117 118 119 120 121 122 123 def get_scan_box_files ( scan_key : dict ) -> list : \"\"\"Retrieve the list of Scanbox files (*.sbx) associated with a given Scan. Args: scan_key: Primary key of a Scan entry. Returns: A list of Scanbox files' full file-paths. \"\"\" return _linking_module . get_scan_box_files ( scan_key ) get_scan_image_files ( scan_key ) \u00b6 Retrieve the list of ScanImage files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key of a Scan entry. required Returns: Type Description list A list of ScanImage files' full file-paths. Source code in element_calcium_imaging/scan.py 102 103 104 105 106 107 108 109 110 111 def get_scan_image_files ( scan_key : dict ) -> list : \"\"\"Retrieve the list of ScanImage files associated with a given Scan. Args: scan_key: Primary key of a Scan entry. Returns: A list of ScanImage files' full file-paths. \"\"\" return _linking_module . get_scan_image_files ( scan_key )", "title": "scan.py"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.AcquisitionSoftware", "text": "Bases: dj . Lookup A list of acquisition softwares supported by the Element. Required to define a scan. Attributes: Name Type Description acq_software str Acquistion software Source code in element_calcium_imaging/scan.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"A list of acquisition softwares supported by the Element. Required to define a scan. Attributes: acq_software (str): Acquistion software \"\"\" definition = \"\"\" # Acquisition softwares acq_software: varchar(24) \"\"\" contents = zip ([ \"ScanImage\" , \"Scanbox\" , \"NIS\" , \"PrairieView\" ])", "title": "AcquisitionSoftware"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.Channel", "text": "Bases: dj . Lookup Recording channels for the imaging wavelengths. Attributes: Name Type Description channel int Channel index Source code in element_calcium_imaging/scan.py 169 170 171 172 173 174 175 176 177 178 179 180 @schema class Channel ( dj . Lookup ): \"\"\"Recording channels for the imaging wavelengths. Attributes: channel (int): Channel index \"\"\" definition = \"\"\" # A recording channel channel : tinyint # 0-based indexing \"\"\" contents = zip ( range ( 5 ))", "title": "Channel"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.Scan", "text": "Bases: dj . Manual Scan defined by a measurement done using a scanner and an acquisition software. The details of the scanning data is placed in other tables, including, ScanLocation, ScanInfo, and ScanInfo's part tables. Attributes: Name Type Description Session foreign key A primary key from Session. scan_id int Unique Scan ID. Equipment foreign key A primary key from Equipment. AcquisitionSoftware foreign key A primary key from AcquisitonSoftware. scan_notes str Notes of the experimenter regarding the scan. Source code in element_calcium_imaging/scan.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @schema class Scan ( dj . Manual ): \"\"\"Scan defined by a measurement done using a scanner and an acquisition software. The details of the scanning data is placed in other tables, including, ScanLocation, ScanInfo, and ScanInfo's part tables. Attributes: Session (foreign key): A primary key from Session. scan_id (int): Unique Scan ID. Equipment (foreign key, optional): A primary key from Equipment. AcquisitionSoftware (foreign key): A primary key from AcquisitonSoftware. scan_notes (str, optional): Notes of the experimenter regarding the scan. \"\"\" definition = \"\"\" -> Session scan_id: int --- -> [nullable] Equipment -> AcquisitionSoftware scan_notes='' : varchar(4095) \"\"\"", "title": "Scan"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo", "text": "Bases: dj . Imported Information about the scan extracted from the recorded files. Attributes: Name Type Description Scan foreign key A primary key from Scan. nfields int Number of fields. nchannels int Number of channels. ndepths int Number of scanning depths (planes). nframes int Number of recorded frames. nrois int Number of ROIs (see scanimage's multi ROI imaging). x float ScanImage's 0 point in the motor coordinate system (um). y float ScanImage's 0 point in the motor coordinate system (um). z float ScanImage's 0 point in the motor coordinate system (um). fps float) Frames per second (Hz) - Volumetric Scan Rate. bidirectional bool True = bidirectional scanning. usecs_per_line float Microseconds per scan line. fill_fraction float Raster scan temporal fill fraction (see scanimage) scan_datetime datetime Datetime of the scan. scan_duration float Duration of the scan (s). bidirectional_z bool True = bidirectional z-scan. Source code in element_calcium_imaging/scan.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 @schema class ScanInfo ( dj . Imported ): \"\"\" Information about the scan extracted from the recorded files. Attributes: Scan (foreign key): A primary key from Scan. nfields (int): Number of fields. nchannels (int): Number of channels. ndepths (int): Number of scanning depths (planes). nframes (int): Number of recorded frames. nrois (int): Number of ROIs (see scanimage's multi ROI imaging). x (float, optional): ScanImage's 0 point in the motor coordinate system (um). y (float, optional): ScanImage's 0 point in the motor coordinate system (um). z (float, optional): ScanImage's 0 point in the motor coordinate system (um). fps (float) : Frames per second (Hz) - Volumetric Scan Rate. bidirectional (bool): True = bidirectional scanning. usecs_per_line (float, optional): Microseconds per scan line. fill_fraction (float, optional): Raster scan temporal fill fraction (see scanimage) scan_datetime (datetime, optional): Datetime of the scan. scan_duration (float, optional): Duration of the scan (s). bidirectional_z (bool, optional): True = bidirectional z-scan. \"\"\" definition = \"\"\" # General data about the reso/meso scans from header -> Scan --- nfields : tinyint # number of fields nchannels : tinyint # number of channels ndepths : int # Number of scanning depths (planes) nframes : int # number of recorded frames nrois : tinyint # number of ROIs (see scanimage's multi ROI imaging) x=null : float # (um) ScanImage's 0 point in the motor coordinate system y=null : float # (um) ScanImage's 0 point in the motor coordinate system z=null : float # (um) ScanImage's 0 point in the motor coordinate system fps : float # (Hz) frames per second - Volumetric Scan Rate bidirectional : boolean # true = bidirectional scanning usecs_per_line=null : float # microseconds per scan line fill_fraction=null : float # raster scan temporal fill fraction (see scanimage) scan_datetime=null : datetime # datetime of the scan scan_duration=null : float # (seconds) duration of the scan bidirectional_z=null : boolean # true = bidirectional z-scan \"\"\" class Field ( dj . Part ): \"\"\"Stores field information of scan, including its coordinates, size, pixel pitch, etc. Attributes: ScanInfo (foreign key): A primary key from ScanInfo. field_idx (int): Unique field index. px_height (int): Image height in pixels. px_width (int): Image width in pixels. um_height (float, optional): Image height in microns. um_width (float, optional): Image width in microns. field_x (float, optional): X coordinate of the center of field in the motor coordinate system (um). field_y (float, optional): Y coordinate of the center of field in the motor coordinate system (um). field_z (float, optional): Relative depth of field (um). delay_image (longblob, optional): Delay between the start of the scan and pixels in this field (ms). roi (int, optional): The scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans. \"\"\" definition = \"\"\" # field-specific scan information -> master field_idx : int --- px_height : smallint # height in pixels px_width : smallint # width in pixels um_height=null : float # height in microns um_width=null : float # width in microns field_x=null : float # (um) center of field in the motor coordinate system field_y=null : float # (um) center of field in the motor coordinate system field_z=null : float # (um) relative depth of field delay_image=null : longblob # (ms) delay between the start of the scan and pixels in this field roi=null : int # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans \"\"\" class ScanFile ( dj . Part ): \"\"\"Filepath of the scan relative to root data directory. Attributes: ScanInfo (foreign key): A primary key from ScanInfo. file_path (str): Path of the scan file relative to the root data directory. \"\"\" definition = \"\"\" -> master file_path: varchar(255) # Filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populate the ScanInfo with the information parsed from image files.\"\"\" acq_software = ( Scan & key ) . fetch1 ( \"acq_software\" ) if acq_software == \"ScanImage\" : import scanreader # Read the scan scan_filepaths = get_scan_image_files ( key ) scan = scanreader . read_scan ( scan_filepaths ) # Insert in ScanInfo x_zero = ( scan . motor_position_at_zero [ 0 ] if scan . motor_position_at_zero else None ) y_zero = ( scan . motor_position_at_zero [ 1 ] if scan . motor_position_at_zero else None ) z_zero = ( scan . motor_position_at_zero [ 2 ] if scan . motor_position_at_zero else None ) self . insert1 ( dict ( key , nfields = scan . num_fields , nchannels = scan . num_channels , nframes = scan . num_frames , ndepths = scan . num_scanning_depths , x = x_zero , y = y_zero , z = z_zero , fps = scan . fps , bidirectional = scan . is_bidirectional , usecs_per_line = scan . seconds_per_line * 1e6 , fill_fraction = scan . temporal_fill_fraction , nrois = scan . num_rois if scan . is_multiROI else 0 , scan_duration = scan . num_frames / scan . fps , ) ) # Insert Field(s) if scan . is_multiROI : self . Field . insert ( [ dict ( key , field_idx = field_id , px_height = scan . field_heights [ field_id ], px_width = scan . field_widths [ field_id ], um_height = scan . field_heights_in_microns [ field_id ], um_width = scan . field_widths_in_microns [ field_id ], field_x = x_zero + scan . _degrees_to_microns ( scan . fields [ field_id ] . x ) if x_zero else None , field_y = y_zero + scan . _degrees_to_microns ( scan . fields [ field_id ] . y ) if y_zero else None , field_z = z_zero + scan . fields [ field_id ] . depth if z_zero else None , delay_image = scan . field_offsets [ field_id ], roi = scan . field_rois [ field_id ][ 0 ], ) for field_id in range ( scan . num_fields ) ] ) else : self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = scan . image_height , px_width = scan . image_width , um_height = getattr ( scan , \"image_height_in_microns\" , None ), um_width = getattr ( scan , \"image_width_in_microns\" , None ), field_x = x_zero if x_zero else None , field_y = y_zero if y_zero else None , field_z = z_zero + scan . scanning_depths [ plane_idx ] if z_zero else None , delay_image = scan . field_offsets [ plane_idx ], ) for plane_idx in range ( scan . num_scanning_depths ) ] ) elif acq_software == \"Scanbox\" : import sbxreader # Read the scan scan_filepaths = get_scan_box_files ( key ) sbx_meta = sbxreader . sbx_get_metadata ( scan_filepaths [ 0 ]) sbx_matinfo = sbxreader . sbx_get_info ( scan_filepaths [ 0 ]) is_multiROI = bool ( sbx_matinfo . mesoscope . enabled ) # currently not handling \"multiROI\" ingestion if is_multiROI : raise NotImplementedError ( \"Loading routine not implemented for Scanbox multiROI scan mode\" ) # Insert in ScanInfo x_zero , y_zero , z_zero = sbx_meta [ \"stage_pos\" ] self . insert1 ( dict ( key , nfields = sbx_meta [ \"num_fields\" ] if is_multiROI else sbx_meta [ \"num_planes\" ], nchannels = sbx_meta [ \"num_channels\" ], nframes = sbx_meta [ \"num_frames\" ], ndepths = sbx_meta [ \"num_planes\" ], x = x_zero , y = y_zero , z = z_zero , fps = sbx_meta [ \"frame_rate\" ], bidirectional = sbx_meta == \"bidirectional\" , nrois = sbx_meta [ \"num_rois\" ] if is_multiROI else 0 , scan_duration = ( sbx_meta [ \"num_frames\" ] / sbx_meta [ \"frame_rate\" ]), ) ) # Insert Field(s) if not is_multiROI : px_width , px_height = sbx_meta [ \"frame_size\" ] self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = px_height , px_width = px_width , um_height = px_height * sbx_meta [ \"um_per_pixel_y\" ] if sbx_meta [ \"um_per_pixel_y\" ] else None , um_width = px_width * sbx_meta [ \"um_per_pixel_x\" ] if sbx_meta [ \"um_per_pixel_x\" ] else None , field_x = x_zero , field_y = y_zero , field_z = z_zero + sbx_meta [ \"etl_pos\" ][ plane_idx ], ) for plane_idx in range ( sbx_meta [ \"num_planes\" ]) ] ) elif acq_software == \"NIS\" : import nd2 # Read the scan scan_filepaths = get_nd2_files ( key ) nd2_file = nd2 . ND2File ( scan_filepaths [ 0 ]) is_multiROI = False # MultiROI to be implemented later # Frame per second try : fps = 1000 / nd2_file . experiment [ 0 ] . parameters . periods [ 0 ] . periodDiff . avg except : fps = 1000 / nd2_file . experiment [ 0 ] . parameters . periodDiff . avg # Estimate ND2 file scan duration def estimate_nd2_scan_duration ( nd2_scan_obj ): # Calculates scan duration for Nikon images ti = ( nd2_scan_obj . frame_metadata ( 0 ) . channels [ 0 ] . time . absoluteJulianDayNumber ) # Initial frame's JD. tf = ( nd2_scan_obj . frame_metadata ( nd2_scan_obj . shape [ 0 ] - 1 ) . channels [ 0 ] . time . absoluteJulianDayNumber ) # Final frame's JD. return ( tf - ti ) * 86400 + 1 / fps scan_duration = sum ( estimate_nd2_scan_duration ( nd2 . ND2File ( f )) for f in scan_filepaths ) try : scan_datetime = nd2_file . text_info [ \"date\" ] scan_datetime = datetime . strptime ( scan_datetime , \"%m/ %d /%Y %H:%M:%S %p\" if re . search (( \"AM|PM\" ), scan_datetime ) else \"%m/ %d /%Y %H:%M:%S\" , ) scan_datetime = datetime . strftime ( scan_datetime , \"%Y-%m- %d %H:%M:%S\" ) except : scan_datetime = None # Insert in ScanInfo self . insert1 ( dict ( key , nfields = nd2_file . sizes . get ( \"P\" , 1 ), nchannels = nd2_file . attributes . channelCount , nframes = nd2_file . metadata . contents . frameCount , ndepths = nd2_file . sizes . get ( \"Z\" , 1 ), x = None , y = None , z = None , fps = fps , bidirectional = bool ( nd2_file . custom_data [ \"GrabberCameraSettingsV1_0\" ][ \"GrabberCameraSettings\" ][ \"PropertiesQuality\" ][ \"ScanDirection\" ] ), nrois = 0 , scan_datetime = scan_datetime , scan_duration = scan_duration , ) ) # MultiROI to be implemented later # Insert in Field if not is_multiROI : self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = nd2_file . attributes . heightPx , px_width = nd2_file . attributes . widthPx , um_height = nd2_file . attributes . heightPx * nd2_file . voxel_size () . y , um_width = nd2_file . attributes . widthPx * nd2_file . voxel_size () . x , field_x = None , field_y = None , field_z = None , ) for plane_idx in range ( nd2_file . sizes . get ( \"Z\" , 1 )) ] ) elif acq_software == \"PrairieView\" : from element_interface import prairieviewreader scan_filepaths = get_prairieview_files ( key ) pvscan_info = prairieviewreader . get_pv_metadata ( scan_filepaths [ 0 ]) self . insert1 ( dict ( key , nfields = pvscan_info [ \"num_fields\" ], nchannels = pvscan_info [ \"num_channels\" ], ndepths = pvscan_info [ \"num_planes\" ], nframes = pvscan_info [ \"num_frames\" ], nrois = pvscan_info [ \"num_rois\" ], x = pvscan_info [ \"x_pos\" ], y = pvscan_info [ \"y_pos\" ], z = pvscan_info [ \"z_pos\" ], fps = pvscan_info [ \"frame_rate\" ], bidirectional = pvscan_info [ \"bidirectional\" ], bidirectional_z = pvscan_info [ \"bidirectional_z\" ], usecs_per_line = pvscan_info [ \"usecs_per_line\" ], scan_datetime = pvscan_info [ \"scan_datetime\" ], scan_duration = pvscan_info [ \"scan_duration\" ], ) ) self . Field . insert ( dict ( key , field_idx = plane_idx , px_height = pvscan_info [ \"height_in_pixels\" ], px_width = pvscan_info [ \"width_in_pixels\" ], um_height = pvscan_info [ \"height_in_um\" ], um_width = pvscan_info [ \"width_in_um\" ], field_x = pvscan_info [ \"fieldX\" ], field_y = pvscan_info [ \"fieldY\" ], field_z = pvscan_info [ \"fieldZ\" ][ plane_idx ], ) for plane_idx in range ( pvscan_info [ \"num_planes\" ]) ) else : raise NotImplementedError ( f \"Loading routine not implemented for { acq_software } \" \"acquisition software\" ) # Insert file(s) root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_filepaths [ 0 ]) scan_files = [ pathlib . Path ( f ) . relative_to ( root_dir ) . as_posix () for f in scan_filepaths ] self . ScanFile . insert ([{ ** key , \"file_path\" : f } for f in scan_files ])", "title": "ScanInfo"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.Field", "text": "Bases: dj . Part Stores field information of scan, including its coordinates, size, pixel pitch, etc. Attributes: Name Type Description ScanInfo foreign key A primary key from ScanInfo. field_idx int Unique field index. px_height int Image height in pixels. px_width int Image width in pixels. um_height float Image height in microns. um_width float Image width in microns. field_x float X coordinate of the center of field in the motor coordinate system (um). field_y float Y coordinate of the center of field in the motor coordinate system (um). field_z float Relative depth of field (um). delay_image longblob Delay between the start of the scan and pixels in this field (ms). roi int The scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans. Source code in element_calcium_imaging/scan.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 class Field ( dj . Part ): \"\"\"Stores field information of scan, including its coordinates, size, pixel pitch, etc. Attributes: ScanInfo (foreign key): A primary key from ScanInfo. field_idx (int): Unique field index. px_height (int): Image height in pixels. px_width (int): Image width in pixels. um_height (float, optional): Image height in microns. um_width (float, optional): Image width in microns. field_x (float, optional): X coordinate of the center of field in the motor coordinate system (um). field_y (float, optional): Y coordinate of the center of field in the motor coordinate system (um). field_z (float, optional): Relative depth of field (um). delay_image (longblob, optional): Delay between the start of the scan and pixels in this field (ms). roi (int, optional): The scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans. \"\"\" definition = \"\"\" # field-specific scan information -> master field_idx : int --- px_height : smallint # height in pixels px_width : smallint # width in pixels um_height=null : float # height in microns um_width=null : float # width in microns field_x=null : float # (um) center of field in the motor coordinate system field_y=null : float # (um) center of field in the motor coordinate system field_z=null : float # (um) relative depth of field delay_image=null : longblob # (ms) delay between the start of the scan and pixels in this field roi=null : int # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans \"\"\"", "title": "Field"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.ScanFile", "text": "Bases: dj . Part Filepath of the scan relative to root data directory. Attributes: Name Type Description ScanInfo foreign key A primary key from ScanInfo. file_path str Path of the scan file relative to the root data directory. Source code in element_calcium_imaging/scan.py 306 307 308 309 310 311 312 313 314 315 316 317 class ScanFile ( dj . Part ): \"\"\"Filepath of the scan relative to root data directory. Attributes: ScanInfo (foreign key): A primary key from ScanInfo. file_path (str): Path of the scan file relative to the root data directory. \"\"\" definition = \"\"\" -> master file_path: varchar(255) # Filepath relative to root data directory \"\"\"", "title": "ScanFile"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.make", "text": "Populate the ScanInfo with the information parsed from image files. Source code in element_calcium_imaging/scan.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 def make ( self , key ): \"\"\"Populate the ScanInfo with the information parsed from image files.\"\"\" acq_software = ( Scan & key ) . fetch1 ( \"acq_software\" ) if acq_software == \"ScanImage\" : import scanreader # Read the scan scan_filepaths = get_scan_image_files ( key ) scan = scanreader . read_scan ( scan_filepaths ) # Insert in ScanInfo x_zero = ( scan . motor_position_at_zero [ 0 ] if scan . motor_position_at_zero else None ) y_zero = ( scan . motor_position_at_zero [ 1 ] if scan . motor_position_at_zero else None ) z_zero = ( scan . motor_position_at_zero [ 2 ] if scan . motor_position_at_zero else None ) self . insert1 ( dict ( key , nfields = scan . num_fields , nchannels = scan . num_channels , nframes = scan . num_frames , ndepths = scan . num_scanning_depths , x = x_zero , y = y_zero , z = z_zero , fps = scan . fps , bidirectional = scan . is_bidirectional , usecs_per_line = scan . seconds_per_line * 1e6 , fill_fraction = scan . temporal_fill_fraction , nrois = scan . num_rois if scan . is_multiROI else 0 , scan_duration = scan . num_frames / scan . fps , ) ) # Insert Field(s) if scan . is_multiROI : self . Field . insert ( [ dict ( key , field_idx = field_id , px_height = scan . field_heights [ field_id ], px_width = scan . field_widths [ field_id ], um_height = scan . field_heights_in_microns [ field_id ], um_width = scan . field_widths_in_microns [ field_id ], field_x = x_zero + scan . _degrees_to_microns ( scan . fields [ field_id ] . x ) if x_zero else None , field_y = y_zero + scan . _degrees_to_microns ( scan . fields [ field_id ] . y ) if y_zero else None , field_z = z_zero + scan . fields [ field_id ] . depth if z_zero else None , delay_image = scan . field_offsets [ field_id ], roi = scan . field_rois [ field_id ][ 0 ], ) for field_id in range ( scan . num_fields ) ] ) else : self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = scan . image_height , px_width = scan . image_width , um_height = getattr ( scan , \"image_height_in_microns\" , None ), um_width = getattr ( scan , \"image_width_in_microns\" , None ), field_x = x_zero if x_zero else None , field_y = y_zero if y_zero else None , field_z = z_zero + scan . scanning_depths [ plane_idx ] if z_zero else None , delay_image = scan . field_offsets [ plane_idx ], ) for plane_idx in range ( scan . num_scanning_depths ) ] ) elif acq_software == \"Scanbox\" : import sbxreader # Read the scan scan_filepaths = get_scan_box_files ( key ) sbx_meta = sbxreader . sbx_get_metadata ( scan_filepaths [ 0 ]) sbx_matinfo = sbxreader . sbx_get_info ( scan_filepaths [ 0 ]) is_multiROI = bool ( sbx_matinfo . mesoscope . enabled ) # currently not handling \"multiROI\" ingestion if is_multiROI : raise NotImplementedError ( \"Loading routine not implemented for Scanbox multiROI scan mode\" ) # Insert in ScanInfo x_zero , y_zero , z_zero = sbx_meta [ \"stage_pos\" ] self . insert1 ( dict ( key , nfields = sbx_meta [ \"num_fields\" ] if is_multiROI else sbx_meta [ \"num_planes\" ], nchannels = sbx_meta [ \"num_channels\" ], nframes = sbx_meta [ \"num_frames\" ], ndepths = sbx_meta [ \"num_planes\" ], x = x_zero , y = y_zero , z = z_zero , fps = sbx_meta [ \"frame_rate\" ], bidirectional = sbx_meta == \"bidirectional\" , nrois = sbx_meta [ \"num_rois\" ] if is_multiROI else 0 , scan_duration = ( sbx_meta [ \"num_frames\" ] / sbx_meta [ \"frame_rate\" ]), ) ) # Insert Field(s) if not is_multiROI : px_width , px_height = sbx_meta [ \"frame_size\" ] self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = px_height , px_width = px_width , um_height = px_height * sbx_meta [ \"um_per_pixel_y\" ] if sbx_meta [ \"um_per_pixel_y\" ] else None , um_width = px_width * sbx_meta [ \"um_per_pixel_x\" ] if sbx_meta [ \"um_per_pixel_x\" ] else None , field_x = x_zero , field_y = y_zero , field_z = z_zero + sbx_meta [ \"etl_pos\" ][ plane_idx ], ) for plane_idx in range ( sbx_meta [ \"num_planes\" ]) ] ) elif acq_software == \"NIS\" : import nd2 # Read the scan scan_filepaths = get_nd2_files ( key ) nd2_file = nd2 . ND2File ( scan_filepaths [ 0 ]) is_multiROI = False # MultiROI to be implemented later # Frame per second try : fps = 1000 / nd2_file . experiment [ 0 ] . parameters . periods [ 0 ] . periodDiff . avg except : fps = 1000 / nd2_file . experiment [ 0 ] . parameters . periodDiff . avg # Estimate ND2 file scan duration def estimate_nd2_scan_duration ( nd2_scan_obj ): # Calculates scan duration for Nikon images ti = ( nd2_scan_obj . frame_metadata ( 0 ) . channels [ 0 ] . time . absoluteJulianDayNumber ) # Initial frame's JD. tf = ( nd2_scan_obj . frame_metadata ( nd2_scan_obj . shape [ 0 ] - 1 ) . channels [ 0 ] . time . absoluteJulianDayNumber ) # Final frame's JD. return ( tf - ti ) * 86400 + 1 / fps scan_duration = sum ( estimate_nd2_scan_duration ( nd2 . ND2File ( f )) for f in scan_filepaths ) try : scan_datetime = nd2_file . text_info [ \"date\" ] scan_datetime = datetime . strptime ( scan_datetime , \"%m/ %d /%Y %H:%M:%S %p\" if re . search (( \"AM|PM\" ), scan_datetime ) else \"%m/ %d /%Y %H:%M:%S\" , ) scan_datetime = datetime . strftime ( scan_datetime , \"%Y-%m- %d %H:%M:%S\" ) except : scan_datetime = None # Insert in ScanInfo self . insert1 ( dict ( key , nfields = nd2_file . sizes . get ( \"P\" , 1 ), nchannels = nd2_file . attributes . channelCount , nframes = nd2_file . metadata . contents . frameCount , ndepths = nd2_file . sizes . get ( \"Z\" , 1 ), x = None , y = None , z = None , fps = fps , bidirectional = bool ( nd2_file . custom_data [ \"GrabberCameraSettingsV1_0\" ][ \"GrabberCameraSettings\" ][ \"PropertiesQuality\" ][ \"ScanDirection\" ] ), nrois = 0 , scan_datetime = scan_datetime , scan_duration = scan_duration , ) ) # MultiROI to be implemented later # Insert in Field if not is_multiROI : self . Field . insert ( [ dict ( key , field_idx = plane_idx , px_height = nd2_file . attributes . heightPx , px_width = nd2_file . attributes . widthPx , um_height = nd2_file . attributes . heightPx * nd2_file . voxel_size () . y , um_width = nd2_file . attributes . widthPx * nd2_file . voxel_size () . x , field_x = None , field_y = None , field_z = None , ) for plane_idx in range ( nd2_file . sizes . get ( \"Z\" , 1 )) ] ) elif acq_software == \"PrairieView\" : from element_interface import prairieviewreader scan_filepaths = get_prairieview_files ( key ) pvscan_info = prairieviewreader . get_pv_metadata ( scan_filepaths [ 0 ]) self . insert1 ( dict ( key , nfields = pvscan_info [ \"num_fields\" ], nchannels = pvscan_info [ \"num_channels\" ], ndepths = pvscan_info [ \"num_planes\" ], nframes = pvscan_info [ \"num_frames\" ], nrois = pvscan_info [ \"num_rois\" ], x = pvscan_info [ \"x_pos\" ], y = pvscan_info [ \"y_pos\" ], z = pvscan_info [ \"z_pos\" ], fps = pvscan_info [ \"frame_rate\" ], bidirectional = pvscan_info [ \"bidirectional\" ], bidirectional_z = pvscan_info [ \"bidirectional_z\" ], usecs_per_line = pvscan_info [ \"usecs_per_line\" ], scan_datetime = pvscan_info [ \"scan_datetime\" ], scan_duration = pvscan_info [ \"scan_duration\" ], ) ) self . Field . insert ( dict ( key , field_idx = plane_idx , px_height = pvscan_info [ \"height_in_pixels\" ], px_width = pvscan_info [ \"width_in_pixels\" ], um_height = pvscan_info [ \"height_in_um\" ], um_width = pvscan_info [ \"width_in_um\" ], field_x = pvscan_info [ \"fieldX\" ], field_y = pvscan_info [ \"fieldY\" ], field_z = pvscan_info [ \"fieldZ\" ][ plane_idx ], ) for plane_idx in range ( pvscan_info [ \"num_planes\" ]) ) else : raise NotImplementedError ( f \"Loading routine not implemented for { acq_software } \" \"acquisition software\" ) # Insert file(s) root_dir = find_root_directory ( get_imaging_root_data_dir (), scan_filepaths [ 0 ]) scan_files = [ pathlib . Path ( f ) . relative_to ( root_dir ) . as_posix () for f in scan_filepaths ] self . ScanFile . insert ([{ ** key , \"file_path\" : f } for f in scan_files ])", "title": "make()"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanLocation", "text": "Bases: dj . Manual Anatomical location of the scanned region in the brain Attributes: Name Type Description Scan foreign key A primary key from Scan. Locaton foreign key A primary key from Location. Source code in element_calcium_imaging/scan.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 @schema class ScanLocation ( dj . Manual ): \"\"\"Anatomical location of the scanned region in the brain Attributes: Scan (foreign key): A primary key from Scan. Locaton (foreign key): A primary key from Location. \"\"\" definition = \"\"\"Anatomical location -> Scan --- -> Location \"\"\"", "title": "ScanLocation"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.activate", "text": "Activate this schema. Parameters: Name Type Description Default scan_schema_name str Schema name on the database server to activate the scan module required create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the scan module. None Dependencies: Upstream tables Session: Parent table to Scan, typically identifying a recording session Equipment: Reference table for Scan, specifying the equipment used for the acquisition of this scan. Location: Reference table for ScanLocation, specifying the scanned regions's anatomical location in the brain. Source code in element_calcium_imaging/scan.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def activate ( scan_schema_name , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\"Activate this schema. Args: scan_schema_name (str): Schema name on the database server to activate the `scan` module create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `scan` module. Dependencies: Upstream tables: + Session: Parent table to Scan, typically identifying a recording session + Equipment: Reference table for Scan, specifying the equipment used for the acquisition of this scan. + Location: Reference table for ScanLocation, specifying the scanned regions's anatomical location in the brain. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module schema . activate ( scan_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , )", "title": "activate()"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_imaging_root_data_dir", "text": "Return imaging root data director(y/ies) Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations). Returns: Name Type Description dirs list A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies). Source code in element_calcium_imaging/scan.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def get_imaging_root_data_dir () -> list : \"\"\"Return imaging root data director(y/ies) Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations). Returns: dirs (list): A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies). \"\"\" root_directories = _linking_module . get_imaging_root_data_dir () if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ root_directories ] if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): root_directories . append ( _linking_module . get_processed_root_data_dir ()) return root_directories", "title": "get_imaging_root_data_dir()"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_nd2_files", "text": "Retrieve the list of Nikon files (*.nd2) associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key of a Scan entry. required Returns: Type Description list A list of Nikon files' full file-paths. Source code in element_calcium_imaging/scan.py 126 127 128 129 130 131 132 133 134 135 def get_nd2_files ( scan_key : dict ) -> list : \"\"\"Retrieve the list of Nikon files (*.nd2) associated with a given Scan. Args: scan_key: Primary key of a Scan entry. Returns: A list of Nikon files' full file-paths. \"\"\" return _linking_module . get_nd2_files ( scan_key )", "title": "get_nd2_files()"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_prairieview_files", "text": "Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key of a Scan entry. required Returns: Type Description list A list of Bruker PrairieView files' full file-paths. Source code in element_calcium_imaging/scan.py 138 139 140 141 142 143 144 145 146 147 def get_prairieview_files ( scan_key : dict ) -> list : \"\"\"Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan. Args: scan_key: Primary key of a Scan entry. Returns: A list of Bruker PrairieView files' full file-paths. \"\"\" return _linking_module . get_prairieview_files ( scan_key )", "title": "get_prairieview_files()"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_processed_root_data_dir", "text": "Retrieve the root directory for all processed data. All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations). Returns: Name Type Description dir str | pathlib . Path Absolute path of the pocessed imaging root data directory. Source code in element_calcium_imaging/scan.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def get_processed_root_data_dir () -> Union [ str , pathlib . Path ]: \"\"\"Retrieve the root directory for all processed data. All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations). Returns: dir (str| pathlib.Path): Absolute path of the pocessed imaging root data directory. \"\"\" if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): return _linking_module . get_processed_root_data_dir () else : return get_imaging_root_data_dir ()[ 0 ]", "title": "get_processed_root_data_dir()"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_scan_box_files", "text": "Retrieve the list of Scanbox files (*.sbx) associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key of a Scan entry. required Returns: Type Description list A list of Scanbox files' full file-paths. Source code in element_calcium_imaging/scan.py 114 115 116 117 118 119 120 121 122 123 def get_scan_box_files ( scan_key : dict ) -> list : \"\"\"Retrieve the list of Scanbox files (*.sbx) associated with a given Scan. Args: scan_key: Primary key of a Scan entry. Returns: A list of Scanbox files' full file-paths. \"\"\" return _linking_module . get_scan_box_files ( scan_key )", "title": "get_scan_box_files()"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_scan_image_files", "text": "Retrieve the list of ScanImage files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key of a Scan entry. required Returns: Type Description list A list of ScanImage files' full file-paths. Source code in element_calcium_imaging/scan.py 102 103 104 105 106 107 108 109 110 111 def get_scan_image_files ( scan_key : dict ) -> list : \"\"\"Retrieve the list of ScanImage files associated with a given Scan. Args: scan_key: Primary key of a Scan entry. Returns: A list of ScanImage files' full file-paths. \"\"\" return _linking_module . get_scan_image_files ( scan_key )", "title": "get_scan_image_files()"}, {"location": "api/element_calcium_imaging/version/", "text": "Package metadata.", "title": "version.py"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/", "text": "figure_data ( imaging , segmentation_key ) \u00b6 Prepare the images for a given segmentation_key. Parameters: Name Type Description Default imaging dj . Table imaging table. required segmentation_key dict A primary key from Segmentation table. required Returns: Name Type Description background_with_cells np . array Average image with transparently overlayed cells. cells_maskid_image np . array Mask ID image. Source code in element_calcium_imaging/plotting/cell_plot.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def figure_data ( imaging , segmentation_key ): \"\"\"Prepare the images for a given segmentation_key. Args: imaging (dj.Table): imaging table. segmentation_key (dict): A primary key from Segmentation table. Returns: background_with_cells (np.array): Average image with transparently overlayed cells. cells_maskid_image (np.array): Mask ID image. \"\"\" image = ( imaging . MotionCorrection . Summary & segmentation_key ) . fetch1 ( \"average_image\" ) cell_mask_ids , mask_xpix , mask_ypix = ( imaging . Segmentation . Mask * imaging . MaskClassification . MaskType & segmentation_key ) . fetch ( \"mask\" , \"mask_xpix\" , \"mask_ypix\" ) background_with_cells , cells_maskid_image = mask_overlayed_image ( image , mask_xpix , mask_ypix , cell_mask_ids , low_q = 0 , high_q = 0.99 ) return background_with_cells , cells_maskid_image get_tracelayout ( key , width = 600 , height = 600 ) \u00b6 Returns a dictionary of layout settings for the trace figures. Source code in element_calcium_imaging/plotting/cell_plot.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_tracelayout ( key , width = 600 , height = 600 ): \"\"\"Returns a dictionary of layout settings for the trace figures.\"\"\" text = f \"Trace for Cell { key [ 'mask' ] } \" if isinstance ( key , dict ) else \"Trace\" return dict ( margin = dict ( l = 0 , r = 0 , b = 0 , t = 65 , pad = 0 ), width = width , height = height , transition = { \"duration\" : 0 }, title = { \"text\" : text , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" , \"y\" : 0.97 , \"x\" : 0.5 , }, xaxis = { \"title\" : \"Time (sec)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : True , }, yaxis = { \"title\" : \"Fluorescence (a.u.)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : True , \"anchor\" : \"free\" , \"overlaying\" : \"y\" , \"side\" : \"left\" , \"position\" : 0 , }, yaxis2 = { \"title\" : \"Calcium Event (a.u.)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : True , \"anchor\" : \"free\" , \"overlaying\" : \"y\" , \"side\" : \"right\" , \"position\" : 1 , }, shapes = [ go . layout . Shape ( type = \"rect\" , xref = \"paper\" , yref = \"paper\" , x0 = 0 , y0 = 0 , x1 = 1.0 , y1 = 1.0 , line = { \"width\" : 1 , \"color\" : \"black\" }, ) ], legend = { \"traceorder\" : \"normal\" , \"yanchor\" : \"top\" , \"y\" : 0.99 , \"xanchor\" : \"right\" , \"x\" : 0.99 , }, plot_bgcolor = \"rgba(0,0,0,0.05)\" , modebar_remove = [ \"zoom\" , \"resetScale\" , \"pan\" , \"select\" , \"zoomIn\" , \"zoomOut\" , \"autoScale2d\" , ], ) mask_overlayed_image ( image , mask_xpix , mask_ypix , cell_mask_ids , low_q = 0 , high_q = 0.99 ) \u00b6 Overlay transparent cell masks on average image. Source code in element_calcium_imaging/plotting/cell_plot.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def mask_overlayed_image ( image , mask_xpix , mask_ypix , cell_mask_ids , low_q = 0 , high_q = 0.99 ): \"\"\"Overlay transparent cell masks on average image.\"\"\" q_min , q_max = np . quantile ( image , [ low_q , high_q ]) image = np . clip ( image , q_min , q_max ) image = ( image - q_min ) / ( q_max - q_min ) SATURATION = 0.7 image = image [:, :, None ] * np . array ([ 0 , 0 , 1 ]) maskid_image = np . full ( image . shape [: 2 ], - 1 ) for xpix , ypix , roi_id in zip ( mask_xpix , mask_ypix , cell_mask_ids ): image [ ypix , xpix , : 2 ] = [ np . random . rand (), SATURATION ] maskid_image [ ypix , xpix ] = roi_id image = ( colors . hsv_to_rgb ( image ) * 255 ) . astype ( int ) return image , maskid_image plot_cell_overlayed_image ( imaging , segmentation_key ) \u00b6 summary Parameters: Name Type Description Default imaging dj . Table imaging table. required segmentation_key dict A primary key from Segmentation table. required Returns: Name Type Description image_fig plotly . Fig Plotly figure object of the average image with transparently overlayed cells. Source code in element_calcium_imaging/plotting/cell_plot.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def plot_cell_overlayed_image ( imaging , segmentation_key ): \"\"\"_summary_ Args: imaging (dj.Table): imaging table. segmentation_key (dict): A primary key from Segmentation table. Returns: image_fig (plotly.Fig): Plotly figure object of the average image with transparently overlayed cells. \"\"\" background_with_cells , cells_maskid_image = figure_data ( imaging , segmentation_key ) image_fig = go . Figure ( go . Image ( z = background_with_cells , hovertemplate = \"x: % {x} <br>y: % {y} <br>mask_id: % {customdata} \" , customdata = cells_maskid_image , ) ) image_fig . update_layout ( title = \"Average Image with Cells\" , xaxis = { \"title\" : \"X (px)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : False , }, yaxis = { \"title\" : \"Y (px)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : False , }, paper_bgcolor = \"rgba(0,0,0,0)\" , plot_bgcolor = \"rgba(0,0,0,0)\" , ) return image_fig plot_cell_traces ( imaging , cell_key ) \u00b6 Prepare plotly trace figure. Parameters: Name Type Description Default imaging dj . Table imaging table. required cell_key dict A primary key from imaging.Activity.Trace table. required Returns: Name Type Description trace_fig Plotly figure object of the traces. Source code in element_calcium_imaging/plotting/cell_plot.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def plot_cell_traces ( imaging , cell_key ): \"\"\"Prepare plotly trace figure. Args: imaging (dj.Table): imaging table. cell_key (dict): A primary key from imaging.Activity.Trace table. Returns: trace_fig: Plotly figure object of the traces. \"\"\" activity_trace = ( imaging . Activity . Trace & \"extraction_method LIKE ' %d econvolution'\" & cell_key ) . fetch1 ( \"activity_trace\" ) fluorescence , fps = ( scan . ScanInfo * imaging . Fluorescence . Trace & cell_key ) . fetch1 ( \"fluorescence\" , \"fps\" ) trace_fig = go . Figure ( [ go . Scatter ( x = np . arange ( len ( fluorescence )) / fps , y = fluorescence , name = \"Fluorescence\" , yaxis = \"y1\" , ), go . Scatter ( x = np . arange ( len ( activity_trace )) / fps , y = activity_trace , name = \"Calcium Event\" , yaxis = \"y2\" , ), ] ) trace_fig . update_layout ( get_tracelayout ( cell_key )) return trace_fig", "title": "cell_plot.py"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.figure_data", "text": "Prepare the images for a given segmentation_key. Parameters: Name Type Description Default imaging dj . Table imaging table. required segmentation_key dict A primary key from Segmentation table. required Returns: Name Type Description background_with_cells np . array Average image with transparently overlayed cells. cells_maskid_image np . array Mask ID image. Source code in element_calcium_imaging/plotting/cell_plot.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def figure_data ( imaging , segmentation_key ): \"\"\"Prepare the images for a given segmentation_key. Args: imaging (dj.Table): imaging table. segmentation_key (dict): A primary key from Segmentation table. Returns: background_with_cells (np.array): Average image with transparently overlayed cells. cells_maskid_image (np.array): Mask ID image. \"\"\" image = ( imaging . MotionCorrection . Summary & segmentation_key ) . fetch1 ( \"average_image\" ) cell_mask_ids , mask_xpix , mask_ypix = ( imaging . Segmentation . Mask * imaging . MaskClassification . MaskType & segmentation_key ) . fetch ( \"mask\" , \"mask_xpix\" , \"mask_ypix\" ) background_with_cells , cells_maskid_image = mask_overlayed_image ( image , mask_xpix , mask_ypix , cell_mask_ids , low_q = 0 , high_q = 0.99 ) return background_with_cells , cells_maskid_image", "title": "figure_data()"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.get_tracelayout", "text": "Returns a dictionary of layout settings for the trace figures. Source code in element_calcium_imaging/plotting/cell_plot.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_tracelayout ( key , width = 600 , height = 600 ): \"\"\"Returns a dictionary of layout settings for the trace figures.\"\"\" text = f \"Trace for Cell { key [ 'mask' ] } \" if isinstance ( key , dict ) else \"Trace\" return dict ( margin = dict ( l = 0 , r = 0 , b = 0 , t = 65 , pad = 0 ), width = width , height = height , transition = { \"duration\" : 0 }, title = { \"text\" : text , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" , \"y\" : 0.97 , \"x\" : 0.5 , }, xaxis = { \"title\" : \"Time (sec)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : True , }, yaxis = { \"title\" : \"Fluorescence (a.u.)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : True , \"anchor\" : \"free\" , \"overlaying\" : \"y\" , \"side\" : \"left\" , \"position\" : 0 , }, yaxis2 = { \"title\" : \"Calcium Event (a.u.)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : True , \"anchor\" : \"free\" , \"overlaying\" : \"y\" , \"side\" : \"right\" , \"position\" : 1 , }, shapes = [ go . layout . Shape ( type = \"rect\" , xref = \"paper\" , yref = \"paper\" , x0 = 0 , y0 = 0 , x1 = 1.0 , y1 = 1.0 , line = { \"width\" : 1 , \"color\" : \"black\" }, ) ], legend = { \"traceorder\" : \"normal\" , \"yanchor\" : \"top\" , \"y\" : 0.99 , \"xanchor\" : \"right\" , \"x\" : 0.99 , }, plot_bgcolor = \"rgba(0,0,0,0.05)\" , modebar_remove = [ \"zoom\" , \"resetScale\" , \"pan\" , \"select\" , \"zoomIn\" , \"zoomOut\" , \"autoScale2d\" , ], )", "title": "get_tracelayout()"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.mask_overlayed_image", "text": "Overlay transparent cell masks on average image. Source code in element_calcium_imaging/plotting/cell_plot.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def mask_overlayed_image ( image , mask_xpix , mask_ypix , cell_mask_ids , low_q = 0 , high_q = 0.99 ): \"\"\"Overlay transparent cell masks on average image.\"\"\" q_min , q_max = np . quantile ( image , [ low_q , high_q ]) image = np . clip ( image , q_min , q_max ) image = ( image - q_min ) / ( q_max - q_min ) SATURATION = 0.7 image = image [:, :, None ] * np . array ([ 0 , 0 , 1 ]) maskid_image = np . full ( image . shape [: 2 ], - 1 ) for xpix , ypix , roi_id in zip ( mask_xpix , mask_ypix , cell_mask_ids ): image [ ypix , xpix , : 2 ] = [ np . random . rand (), SATURATION ] maskid_image [ ypix , xpix ] = roi_id image = ( colors . hsv_to_rgb ( image ) * 255 ) . astype ( int ) return image , maskid_image", "title": "mask_overlayed_image()"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.plot_cell_overlayed_image", "text": "summary Parameters: Name Type Description Default imaging dj . Table imaging table. required segmentation_key dict A primary key from Segmentation table. required Returns: Name Type Description image_fig plotly . Fig Plotly figure object of the average image with transparently overlayed cells. Source code in element_calcium_imaging/plotting/cell_plot.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def plot_cell_overlayed_image ( imaging , segmentation_key ): \"\"\"_summary_ Args: imaging (dj.Table): imaging table. segmentation_key (dict): A primary key from Segmentation table. Returns: image_fig (plotly.Fig): Plotly figure object of the average image with transparently overlayed cells. \"\"\" background_with_cells , cells_maskid_image = figure_data ( imaging , segmentation_key ) image_fig = go . Figure ( go . Image ( z = background_with_cells , hovertemplate = \"x: % {x} <br>y: % {y} <br>mask_id: % {customdata} \" , customdata = cells_maskid_image , ) ) image_fig . update_layout ( title = \"Average Image with Cells\" , xaxis = { \"title\" : \"X (px)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : False , }, yaxis = { \"title\" : \"Y (px)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : False , }, paper_bgcolor = \"rgba(0,0,0,0)\" , plot_bgcolor = \"rgba(0,0,0,0)\" , ) return image_fig", "title": "plot_cell_overlayed_image()"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.plot_cell_traces", "text": "Prepare plotly trace figure. Parameters: Name Type Description Default imaging dj . Table imaging table. required cell_key dict A primary key from imaging.Activity.Trace table. required Returns: Name Type Description trace_fig Plotly figure object of the traces. Source code in element_calcium_imaging/plotting/cell_plot.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def plot_cell_traces ( imaging , cell_key ): \"\"\"Prepare plotly trace figure. Args: imaging (dj.Table): imaging table. cell_key (dict): A primary key from imaging.Activity.Trace table. Returns: trace_fig: Plotly figure object of the traces. \"\"\" activity_trace = ( imaging . Activity . Trace & \"extraction_method LIKE ' %d econvolution'\" & cell_key ) . fetch1 ( \"activity_trace\" ) fluorescence , fps = ( scan . ScanInfo * imaging . Fluorescence . Trace & cell_key ) . fetch1 ( \"fluorescence\" , \"fps\" ) trace_fig = go . Figure ( [ go . Scatter ( x = np . arange ( len ( fluorescence )) / fps , y = fluorescence , name = \"Fluorescence\" , yaxis = \"y1\" , ), go . Scatter ( x = np . arange ( len ( activity_trace )) / fps , y = activity_trace , name = \"Calcium Event\" , yaxis = \"y2\" , ), ] ) trace_fig . update_layout ( get_tracelayout ( cell_key )) return trace_fig", "title": "plot_cell_traces()"}, {"location": "api/element_calcium_imaging/plotting/widget/", "text": "main ( imaging , usedb = False ) \u00b6 Display the widget. Parameters: Name Type Description Default imaging dj . Table imaging table in the database. required usedb bool Whether to use the figures in the database or compute the figures on the fly. False Returns: Name Type Description widget Widget to display the figures. Source code in element_calcium_imaging/plotting/widget.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def main ( imaging , usedb = False ): \"\"\"Display the widget. Args: imaging (dj.Table): imaging table in the database. usedb (bool, optional): Whether to use the figures in the database or compute the figures on the fly. Returns: widget: Widget to display the figures. \"\"\" motioncorrection_dropdown = wg . Dropdown ( options = imaging . Segmentation . fetch ( \"KEY\" ), description = \"Result:\" , description_tooltip = 'Press \"Load\" to visualize the cells identified.' , disabled = False , layout = wg . Layout ( width = \"95%\" , display = \"flex\" , flex_flow = \"row\" , justify_content = \"space-between\" , grid_area = \"motioncorrection_dropdown\" , ), style = { \"description_width\" : \"80px\" }, ) load_button = wg . Button ( description = \"Load Image\" , tooltip = \"Load the average image.\" , layout = wg . Layout ( width = \"120px\" , grid_area = \"load_button\" ), ) FIG1_WIDTH = 600 FIG1_LAYOUT = go . Layout ( margin = dict ( l = 0 , r = 40 , b = 0 , t = 65 , pad = 0 ), width = FIG1_WIDTH , height = 600 , transition = { \"duration\" : 0 }, title = { \"text\" : \"Average Image with Cells\" , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" , \"y\" : 0.97 , \"x\" : 0.5 , }, xaxis = { \"title\" : \"X (px)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : False , }, yaxis = { \"title\" : \"Y (px)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : False , }, paper_bgcolor = \"rgba(0,0,0,0)\" , plot_bgcolor = \"rgba(0,0,0,0)\" , modebar_remove = [ \"zoom\" , \"resetScale\" , \"pan\" , \"select\" , \"zoomIn\" , \"zoomOut\" , \"autoScale2d\" , ], shapes = [ go . layout . Shape ( type = \"rect\" , xref = \"paper\" , yref = \"paper\" , x0 = 0.035 , y0 = 0 , x1 = 0.965 , y1 = 1.0 , line = { \"width\" : 1 , \"color\" : \"black\" }, ) ], ) fig1 = go . Figure ( go . Image ( z = None , hovertemplate = \"x: % {x} <br>y: % {y} <br>mask_id: % {customdata} <extra></extra>\" , customdata = None , ), layout = FIG1_LAYOUT , ) FIG2_WIDTH = 600 FIG2_HEIGHT = 600 fig2_layout = cell_plot . get_tracelayout ( None , width = FIG2_WIDTH , height = FIG2_HEIGHT ) fig2 = go . Figure ( [ go . Scatter ( x = None , y = None , name = \"Fluorescence\" , yaxis = \"y1\" , ), go . Scatter ( x = None , y = None , name = \"Calcium Event\" , yaxis = \"y2\" ), ], layout = fig2_layout , ) fig1_widget = go . FigureWidget ( fig1 ) fig2_widget = go . FigureWidget ( fig2 ) def tooltip_click ( trace , points , selector ): mask_id = trace . customdata [ points . ys [ 0 ]][ points . xs [ 0 ]] if mask_id > - 1 : cell_traces_figobj = from_json ( ( TraceReport & motioncorrection_dropdown . value & f \"mask=' { mask_id } '\" ) . fetch1 ( \"cell_traces\" ) ) with fig2_widget . batch_update (): fig2_widget . data [ 0 ] . x = cell_traces_figobj . data [ 0 ] . x fig2_widget . data [ 0 ] . y = cell_traces_figobj . data [ 0 ] . y fig2_widget . data [ 0 ] . name = cell_traces_figobj . data [ 0 ] . name fig2_widget . data [ 1 ] . x = cell_traces_figobj . data [ 1 ] . x fig2_widget . data [ 1 ] . y = cell_traces_figobj . data [ 1 ] . y fig2_widget . data [ 1 ] . name = cell_traces_figobj . data [ 1 ] . name fig2_widget . layout [ \"title\" ] = { \"text\" : f \"Trace for Cell { mask_id } \" , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" , \"y\" : 0.97 , \"x\" : 0.5 , } def response ( change , usedb = False ): if usedb : cell_overlayed_image = from_json ( ( ScanLevelReport & motioncorrection_dropdown . value ) . fetch1 ( \"cell_overlayed_image\" ) ) with fig1_widget . batch_update (): fig1_widget . data [ 0 ] . z = cell_overlayed_image . data [ 0 ] . z fig1_widget . data [ 0 ] . customdata = cell_overlayed_image . data [ 0 ] . customdata fig2_widget . data [ 0 ] . x = None fig2_widget . data [ 0 ] . y = None fig2_widget . data [ 1 ] . x = None fig2_widget . data [ 1 ] . y = None else : background_with_cells , cells_maskid_image = cell_plot . figure_data ( imaging , motioncorrection_dropdown . value ) with fig1_widget . batch_update (): fig1_widget . data [ 0 ] . z = background_with_cells fig1_widget . data [ 0 ] . customdata = cells_maskid_image fig2_widget . layout . title = { \"text\" : \"Trace\" , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" , \"y\" : 0.97 , \"x\" : 0.5 , } fig2_widget . data [ 0 ] . x = None fig2_widget . data [ 0 ] . y = None fig2_widget . data [ 1 ] . x = None fig2_widget . data [ 1 ] . y = None fig1_widget . data [ 0 ] . on_click ( tooltip_click ) load_button . on_click ( partial ( response , usedb = usedb )) return wg . VBox ( [ wg . HBox ( [ motioncorrection_dropdown , load_button ], layout = wg . Layout ( width = f \" { FIG1_WIDTH + FIG2_WIDTH } px\" ), ), wg . HBox ([ fig1_widget , fig2_widget ]), ] )", "title": "widget.py"}, {"location": "api/element_calcium_imaging/plotting/widget/#element_calcium_imaging.plotting.widget.main", "text": "Display the widget. Parameters: Name Type Description Default imaging dj . Table imaging table in the database. required usedb bool Whether to use the figures in the database or compute the figures on the fly. False Returns: Name Type Description widget Widget to display the figures. Source code in element_calcium_imaging/plotting/widget.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def main ( imaging , usedb = False ): \"\"\"Display the widget. Args: imaging (dj.Table): imaging table in the database. usedb (bool, optional): Whether to use the figures in the database or compute the figures on the fly. Returns: widget: Widget to display the figures. \"\"\" motioncorrection_dropdown = wg . Dropdown ( options = imaging . Segmentation . fetch ( \"KEY\" ), description = \"Result:\" , description_tooltip = 'Press \"Load\" to visualize the cells identified.' , disabled = False , layout = wg . Layout ( width = \"95%\" , display = \"flex\" , flex_flow = \"row\" , justify_content = \"space-between\" , grid_area = \"motioncorrection_dropdown\" , ), style = { \"description_width\" : \"80px\" }, ) load_button = wg . Button ( description = \"Load Image\" , tooltip = \"Load the average image.\" , layout = wg . Layout ( width = \"120px\" , grid_area = \"load_button\" ), ) FIG1_WIDTH = 600 FIG1_LAYOUT = go . Layout ( margin = dict ( l = 0 , r = 40 , b = 0 , t = 65 , pad = 0 ), width = FIG1_WIDTH , height = 600 , transition = { \"duration\" : 0 }, title = { \"text\" : \"Average Image with Cells\" , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" , \"y\" : 0.97 , \"x\" : 0.5 , }, xaxis = { \"title\" : \"X (px)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : False , }, yaxis = { \"title\" : \"Y (px)\" , \"visible\" : True , \"showticklabels\" : True , \"showgrid\" : False , }, paper_bgcolor = \"rgba(0,0,0,0)\" , plot_bgcolor = \"rgba(0,0,0,0)\" , modebar_remove = [ \"zoom\" , \"resetScale\" , \"pan\" , \"select\" , \"zoomIn\" , \"zoomOut\" , \"autoScale2d\" , ], shapes = [ go . layout . Shape ( type = \"rect\" , xref = \"paper\" , yref = \"paper\" , x0 = 0.035 , y0 = 0 , x1 = 0.965 , y1 = 1.0 , line = { \"width\" : 1 , \"color\" : \"black\" }, ) ], ) fig1 = go . Figure ( go . Image ( z = None , hovertemplate = \"x: % {x} <br>y: % {y} <br>mask_id: % {customdata} <extra></extra>\" , customdata = None , ), layout = FIG1_LAYOUT , ) FIG2_WIDTH = 600 FIG2_HEIGHT = 600 fig2_layout = cell_plot . get_tracelayout ( None , width = FIG2_WIDTH , height = FIG2_HEIGHT ) fig2 = go . Figure ( [ go . Scatter ( x = None , y = None , name = \"Fluorescence\" , yaxis = \"y1\" , ), go . Scatter ( x = None , y = None , name = \"Calcium Event\" , yaxis = \"y2\" ), ], layout = fig2_layout , ) fig1_widget = go . FigureWidget ( fig1 ) fig2_widget = go . FigureWidget ( fig2 ) def tooltip_click ( trace , points , selector ): mask_id = trace . customdata [ points . ys [ 0 ]][ points . xs [ 0 ]] if mask_id > - 1 : cell_traces_figobj = from_json ( ( TraceReport & motioncorrection_dropdown . value & f \"mask=' { mask_id } '\" ) . fetch1 ( \"cell_traces\" ) ) with fig2_widget . batch_update (): fig2_widget . data [ 0 ] . x = cell_traces_figobj . data [ 0 ] . x fig2_widget . data [ 0 ] . y = cell_traces_figobj . data [ 0 ] . y fig2_widget . data [ 0 ] . name = cell_traces_figobj . data [ 0 ] . name fig2_widget . data [ 1 ] . x = cell_traces_figobj . data [ 1 ] . x fig2_widget . data [ 1 ] . y = cell_traces_figobj . data [ 1 ] . y fig2_widget . data [ 1 ] . name = cell_traces_figobj . data [ 1 ] . name fig2_widget . layout [ \"title\" ] = { \"text\" : f \"Trace for Cell { mask_id } \" , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" , \"y\" : 0.97 , \"x\" : 0.5 , } def response ( change , usedb = False ): if usedb : cell_overlayed_image = from_json ( ( ScanLevelReport & motioncorrection_dropdown . value ) . fetch1 ( \"cell_overlayed_image\" ) ) with fig1_widget . batch_update (): fig1_widget . data [ 0 ] . z = cell_overlayed_image . data [ 0 ] . z fig1_widget . data [ 0 ] . customdata = cell_overlayed_image . data [ 0 ] . customdata fig2_widget . data [ 0 ] . x = None fig2_widget . data [ 0 ] . y = None fig2_widget . data [ 1 ] . x = None fig2_widget . data [ 1 ] . y = None else : background_with_cells , cells_maskid_image = cell_plot . figure_data ( imaging , motioncorrection_dropdown . value ) with fig1_widget . batch_update (): fig1_widget . data [ 0 ] . z = background_with_cells fig1_widget . data [ 0 ] . customdata = cells_maskid_image fig2_widget . layout . title = { \"text\" : \"Trace\" , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" , \"y\" : 0.97 , \"x\" : 0.5 , } fig2_widget . data [ 0 ] . x = None fig2_widget . data [ 0 ] . y = None fig2_widget . data [ 1 ] . x = None fig2_widget . data [ 1 ] . y = None fig1_widget . data [ 0 ] . on_click ( tooltip_click ) load_button . on_click ( partial ( response , usedb = usedb )) return wg . VBox ( [ wg . HBox ( [ motioncorrection_dropdown , load_button ], layout = wg . Layout ( width = f \" { FIG1_WIDTH + FIG2_WIDTH } px\" ), ), wg . HBox ([ fig1_widget , fig2_widget ]), ] )", "title": "main()"}, {"location": "api/workflow_calcium_imaging/analysis/", "text": "ActivityAlignment \u00b6 Bases: dj . Computed Attributes: Name Type Description ActivityAlignmentCondition foreign key Primary key from ActivityAlignmentCondition. aligned_timestamps longblob Aligned timestamps. Source code in workflow_calcium_imaging/analysis.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 @schema class ActivityAlignment ( dj . Computed ): \"\"\" Attributes: ActivityAlignmentCondition (foreign key): Primary key from ActivityAlignmentCondition. aligned_timestamps (longblob): Aligned timestamps. \"\"\" definition = \"\"\" -> ActivityAlignmentCondition --- aligned_timestamps: longblob \"\"\" class AlignedTrialActivity ( dj . Part ): \"\"\"Aligned trial activity. Attributes: ActivityAlignment (foriegn key): Primary key from ActivityAlignment. imaging.Activity.Trace (foriegn key): Primary key from imaging.Activity.Trace. ActivityAlignmentCondition.Trial (foreign key): Primary key from ActivityAlignmentCondition.Trial. aligned_trace (longblob): Calcium activity aligned to the event time (s). \"\"\" definition = \"\"\" -> master -> imaging.Activity.Trace -> ActivityAlignmentCondition.Trial --- aligned_trace: longblob # (s) Calcium activity aligned to the event time \"\"\" def make ( self , key ): sess_time , scan_time , nframes , frame_rate = ( _linking_module . scan . ScanInfo * _linking_module . session . Session & key ) . fetch1 ( \"session_datetime\" , \"scan_datetime\" , \"nframes\" , \"fps\" ) trialized_event_times = ( _linking_module . trial . get_trialized_alignment_event_times ( key , _linking_module . trial . Trial & ( ActivityAlignmentCondition . Trial & key ), ) ) min_limit = ( trialized_event_times . event - trialized_event_times . start ) . max () max_limit = ( trialized_event_times . end - trialized_event_times . event ) . max () aligned_timestamps = np . arange ( - min_limit , max_limit , 1 / frame_rate ) nsamples = len ( aligned_timestamps ) trace_keys , activity_traces = ( _linking_module . imaging . Activity . Trace & key ) . fetch ( \"KEY\" , \"activity_trace\" , order_by = \"mask\" ) activity_traces = np . vstack ( activity_traces ) aligned_trial_activities = [] for _ , r in trialized_event_times . iterrows (): if r . event is None or np . isnan ( r . event ): continue alignment_start_idx = int (( r . event - min_limit ) * frame_rate ) roi_aligned_activities = activity_traces [ :, alignment_start_idx : ( alignment_start_idx + nsamples ) ] if roi_aligned_activities . shape [ - 1 ] != nsamples : shape_diff = nsamples - roi_aligned_activities . shape [ - 1 ] roi_aligned_activities = np . pad ( roi_aligned_activities , (( 0 , 0 ), ( 0 , shape_diff )), mode = \"constant\" , constant_values = np . nan , ) aligned_trial_activities . extend ( [ { ** key , ** r . trial_key , ** trace_key , \"aligned_trace\" : aligned_trace } for trace_key , aligned_trace in zip ( trace_keys , roi_aligned_activities ) ] ) self . insert1 ({ ** key , \"aligned_timestamps\" : aligned_timestamps }) self . AlignedTrialActivity . insert ( aligned_trial_activities ) def plot_aligned_activities ( self , key , roi , axs = None , title = None ): \"\"\"Plot event-aligned activities for selected trials, and trial-averaged activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.). Args: key (dict): key of ActivityAlignment master table roi (int): imaging segmentation mask axs (matplotlib.ax): optional definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) title (str): Optional title label Returns: fig (matplotlib.pyplot.figure): Figure of the event aligned activities. \"\"\" import matplotlib.pyplot as plt fig = None if axs is None : fig , ( ax0 , ax1 ) = plt . subplots ( 2 , 1 , figsize = ( 12 , 8 )) else : ax0 , ax1 = axs aligned_timestamps = ( self & key ) . fetch1 ( \"aligned_timestamps\" ) trial_ids , aligned_spikes = ( self . AlignedTrialActivity & key & { \"mask\" : roi } ) . fetch ( \"trial_id\" , \"aligned_trace\" , order_by = \"trial_id\" ) aligned_spikes = np . vstack ( aligned_spikes ) ax0 . imshow ( aligned_spikes , cmap = \"inferno\" , interpolation = \"nearest\" , aspect = \"auto\" , extent = ( aligned_timestamps [ 0 ], aligned_timestamps [ - 1 ], 0 , aligned_spikes . shape [ 0 ], ), ) ax0 . axvline ( x = 0 , linestyle = \"--\" , color = \"white\" ) ax0 . set_axis_off () ax1 . plot ( aligned_timestamps , np . nanmean ( aligned_spikes , axis = 0 )) ax1 . axvline ( x = 0 , linestyle = \"--\" , color = \"black\" ) ax1 . set_xlabel ( \"Time (s)\" ) ax1 . set_xlim ( aligned_timestamps [ 0 ], aligned_timestamps [ - 1 ]) if title : plt . suptitle ( title ) return fig AlignedTrialActivity \u00b6 Bases: dj . Part Aligned trial activity. Attributes: Name Type Description ActivityAlignment foriegn key Primary key from ActivityAlignment. imaging.Activity.Trace foriegn key Primary key from imaging.Activity.Trace. ActivityAlignmentCondition.Trial foreign key Primary key from ActivityAlignmentCondition.Trial. aligned_trace longblob Calcium activity aligned to the event time (s). Source code in workflow_calcium_imaging/analysis.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class AlignedTrialActivity ( dj . Part ): \"\"\"Aligned trial activity. Attributes: ActivityAlignment (foriegn key): Primary key from ActivityAlignment. imaging.Activity.Trace (foriegn key): Primary key from imaging.Activity.Trace. ActivityAlignmentCondition.Trial (foreign key): Primary key from ActivityAlignmentCondition.Trial. aligned_trace (longblob): Calcium activity aligned to the event time (s). \"\"\" definition = \"\"\" -> master -> imaging.Activity.Trace -> ActivityAlignmentCondition.Trial --- aligned_trace: longblob # (s) Calcium activity aligned to the event time \"\"\" plot_aligned_activities ( key , roi , axs = None , title = None ) \u00b6 Plot event-aligned activities for selected trials, and trial-averaged activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.). Parameters: Name Type Description Default key dict key of ActivityAlignment master table required roi int imaging segmentation mask required axs matplotlib . ax optional definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) None title str Optional title label None Returns: Name Type Description fig matplotlib . pyplot . figure Figure of the event aligned activities. Source code in workflow_calcium_imaging/analysis.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def plot_aligned_activities ( self , key , roi , axs = None , title = None ): \"\"\"Plot event-aligned activities for selected trials, and trial-averaged activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.). Args: key (dict): key of ActivityAlignment master table roi (int): imaging segmentation mask axs (matplotlib.ax): optional definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) title (str): Optional title label Returns: fig (matplotlib.pyplot.figure): Figure of the event aligned activities. \"\"\" import matplotlib.pyplot as plt fig = None if axs is None : fig , ( ax0 , ax1 ) = plt . subplots ( 2 , 1 , figsize = ( 12 , 8 )) else : ax0 , ax1 = axs aligned_timestamps = ( self & key ) . fetch1 ( \"aligned_timestamps\" ) trial_ids , aligned_spikes = ( self . AlignedTrialActivity & key & { \"mask\" : roi } ) . fetch ( \"trial_id\" , \"aligned_trace\" , order_by = \"trial_id\" ) aligned_spikes = np . vstack ( aligned_spikes ) ax0 . imshow ( aligned_spikes , cmap = \"inferno\" , interpolation = \"nearest\" , aspect = \"auto\" , extent = ( aligned_timestamps [ 0 ], aligned_timestamps [ - 1 ], 0 , aligned_spikes . shape [ 0 ], ), ) ax0 . axvline ( x = 0 , linestyle = \"--\" , color = \"white\" ) ax0 . set_axis_off () ax1 . plot ( aligned_timestamps , np . nanmean ( aligned_spikes , axis = 0 )) ax1 . axvline ( x = 0 , linestyle = \"--\" , color = \"black\" ) ax1 . set_xlabel ( \"Time (s)\" ) ax1 . set_xlim ( aligned_timestamps [ 0 ], aligned_timestamps [ - 1 ]) if title : plt . suptitle ( title ) return fig ActivityAlignmentCondition \u00b6 Bases: dj . Manual Activity alignment condition. Attributes: Name Type Description imaging.Activity foreign key Primary key from imaging.Activity. event.AlignmentEvent foreign key Primary key from event.AlignmentEvent. trial_condition str User-friendly name of condition. bin_size float bin-size (in second) used to compute the PSTH, Source code in workflow_calcium_imaging/analysis.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @schema class ActivityAlignmentCondition ( dj . Manual ): \"\"\"Activity alignment condition. Attributes: imaging.Activity (foreign key): Primary key from imaging.Activity. event.AlignmentEvent (foreign key): Primary key from event.AlignmentEvent. trial_condition (str): User-friendly name of condition. condition_description (str, optional). Description. Default is ''. bin_size (float): bin-size (in second) used to compute the PSTH, \"\"\" definition = \"\"\" -> imaging.Activity -> event.AlignmentEvent trial_condition: varchar(128) # user-friendly name of condition --- condition_description='': varchar(1000) bin_size=0.04: float # bin-size (in second) used to compute the PSTH \"\"\" class Trial ( dj . Part ): \"\"\"Trial Attributes: ActivityAlignmentCondition (foreign key): Primary key from ActivityAlignmentCondition. trial.Trial: Primary key from trial.Trial. \"\"\" definition = \"\"\" # Trials (or subset) to compute event-aligned activity -> master -> trial.Trial \"\"\" Trial \u00b6 Bases: dj . Part Trial Attributes: Name Type Description ActivityAlignmentCondition foreign key Primary key from ActivityAlignmentCondition. trial.Trial foreign key Primary key from trial.Trial. Source code in workflow_calcium_imaging/analysis.py 66 67 68 69 70 71 72 73 74 75 76 77 78 class Trial ( dj . Part ): \"\"\"Trial Attributes: ActivityAlignmentCondition (foreign key): Primary key from ActivityAlignmentCondition. trial.Trial: Primary key from trial.Trial. \"\"\" definition = \"\"\" # Trials (or subset) to compute event-aligned activity -> master -> trial.Trial \"\"\" activate ( schema_name , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activate this schema. Parameters: Name Type Description Default schema_name str Schema name on the database server to activate the subject element. required create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the subject element: Upstream schema: scan, session, trial. None Source code in workflow_calcium_imaging/analysis.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def activate ( schema_name , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\"Activate this schema. Args: schema_name (str): Schema name on the database server to activate the `subject` element. create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `subject` element: Upstream schema: scan, session, trial. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), ( \"The argument 'dependency' must \" + \"be a module's name or a module\" ) global _linking_module _linking_module = linking_module schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = linking_module . __dict__ , )", "title": "analysis.py"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment", "text": "Bases: dj . Computed Attributes: Name Type Description ActivityAlignmentCondition foreign key Primary key from ActivityAlignmentCondition. aligned_timestamps longblob Aligned timestamps. Source code in workflow_calcium_imaging/analysis.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 @schema class ActivityAlignment ( dj . Computed ): \"\"\" Attributes: ActivityAlignmentCondition (foreign key): Primary key from ActivityAlignmentCondition. aligned_timestamps (longblob): Aligned timestamps. \"\"\" definition = \"\"\" -> ActivityAlignmentCondition --- aligned_timestamps: longblob \"\"\" class AlignedTrialActivity ( dj . Part ): \"\"\"Aligned trial activity. Attributes: ActivityAlignment (foriegn key): Primary key from ActivityAlignment. imaging.Activity.Trace (foriegn key): Primary key from imaging.Activity.Trace. ActivityAlignmentCondition.Trial (foreign key): Primary key from ActivityAlignmentCondition.Trial. aligned_trace (longblob): Calcium activity aligned to the event time (s). \"\"\" definition = \"\"\" -> master -> imaging.Activity.Trace -> ActivityAlignmentCondition.Trial --- aligned_trace: longblob # (s) Calcium activity aligned to the event time \"\"\" def make ( self , key ): sess_time , scan_time , nframes , frame_rate = ( _linking_module . scan . ScanInfo * _linking_module . session . Session & key ) . fetch1 ( \"session_datetime\" , \"scan_datetime\" , \"nframes\" , \"fps\" ) trialized_event_times = ( _linking_module . trial . get_trialized_alignment_event_times ( key , _linking_module . trial . Trial & ( ActivityAlignmentCondition . Trial & key ), ) ) min_limit = ( trialized_event_times . event - trialized_event_times . start ) . max () max_limit = ( trialized_event_times . end - trialized_event_times . event ) . max () aligned_timestamps = np . arange ( - min_limit , max_limit , 1 / frame_rate ) nsamples = len ( aligned_timestamps ) trace_keys , activity_traces = ( _linking_module . imaging . Activity . Trace & key ) . fetch ( \"KEY\" , \"activity_trace\" , order_by = \"mask\" ) activity_traces = np . vstack ( activity_traces ) aligned_trial_activities = [] for _ , r in trialized_event_times . iterrows (): if r . event is None or np . isnan ( r . event ): continue alignment_start_idx = int (( r . event - min_limit ) * frame_rate ) roi_aligned_activities = activity_traces [ :, alignment_start_idx : ( alignment_start_idx + nsamples ) ] if roi_aligned_activities . shape [ - 1 ] != nsamples : shape_diff = nsamples - roi_aligned_activities . shape [ - 1 ] roi_aligned_activities = np . pad ( roi_aligned_activities , (( 0 , 0 ), ( 0 , shape_diff )), mode = \"constant\" , constant_values = np . nan , ) aligned_trial_activities . extend ( [ { ** key , ** r . trial_key , ** trace_key , \"aligned_trace\" : aligned_trace } for trace_key , aligned_trace in zip ( trace_keys , roi_aligned_activities ) ] ) self . insert1 ({ ** key , \"aligned_timestamps\" : aligned_timestamps }) self . AlignedTrialActivity . insert ( aligned_trial_activities ) def plot_aligned_activities ( self , key , roi , axs = None , title = None ): \"\"\"Plot event-aligned activities for selected trials, and trial-averaged activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.). Args: key (dict): key of ActivityAlignment master table roi (int): imaging segmentation mask axs (matplotlib.ax): optional definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) title (str): Optional title label Returns: fig (matplotlib.pyplot.figure): Figure of the event aligned activities. \"\"\" import matplotlib.pyplot as plt fig = None if axs is None : fig , ( ax0 , ax1 ) = plt . subplots ( 2 , 1 , figsize = ( 12 , 8 )) else : ax0 , ax1 = axs aligned_timestamps = ( self & key ) . fetch1 ( \"aligned_timestamps\" ) trial_ids , aligned_spikes = ( self . AlignedTrialActivity & key & { \"mask\" : roi } ) . fetch ( \"trial_id\" , \"aligned_trace\" , order_by = \"trial_id\" ) aligned_spikes = np . vstack ( aligned_spikes ) ax0 . imshow ( aligned_spikes , cmap = \"inferno\" , interpolation = \"nearest\" , aspect = \"auto\" , extent = ( aligned_timestamps [ 0 ], aligned_timestamps [ - 1 ], 0 , aligned_spikes . shape [ 0 ], ), ) ax0 . axvline ( x = 0 , linestyle = \"--\" , color = \"white\" ) ax0 . set_axis_off () ax1 . plot ( aligned_timestamps , np . nanmean ( aligned_spikes , axis = 0 )) ax1 . axvline ( x = 0 , linestyle = \"--\" , color = \"black\" ) ax1 . set_xlabel ( \"Time (s)\" ) ax1 . set_xlim ( aligned_timestamps [ 0 ], aligned_timestamps [ - 1 ]) if title : plt . suptitle ( title ) return fig", "title": "ActivityAlignment"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment.AlignedTrialActivity", "text": "Bases: dj . Part Aligned trial activity. Attributes: Name Type Description ActivityAlignment foriegn key Primary key from ActivityAlignment. imaging.Activity.Trace foriegn key Primary key from imaging.Activity.Trace. ActivityAlignmentCondition.Trial foreign key Primary key from ActivityAlignmentCondition.Trial. aligned_trace longblob Calcium activity aligned to the event time (s). Source code in workflow_calcium_imaging/analysis.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class AlignedTrialActivity ( dj . Part ): \"\"\"Aligned trial activity. Attributes: ActivityAlignment (foriegn key): Primary key from ActivityAlignment. imaging.Activity.Trace (foriegn key): Primary key from imaging.Activity.Trace. ActivityAlignmentCondition.Trial (foreign key): Primary key from ActivityAlignmentCondition.Trial. aligned_trace (longblob): Calcium activity aligned to the event time (s). \"\"\" definition = \"\"\" -> master -> imaging.Activity.Trace -> ActivityAlignmentCondition.Trial --- aligned_trace: longblob # (s) Calcium activity aligned to the event time \"\"\"", "title": "AlignedTrialActivity"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment.plot_aligned_activities", "text": "Plot event-aligned activities for selected trials, and trial-averaged activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.). Parameters: Name Type Description Default key dict key of ActivityAlignment master table required roi int imaging segmentation mask required axs matplotlib . ax optional definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) None title str Optional title label None Returns: Name Type Description fig matplotlib . pyplot . figure Figure of the event aligned activities. Source code in workflow_calcium_imaging/analysis.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def plot_aligned_activities ( self , key , roi , axs = None , title = None ): \"\"\"Plot event-aligned activities for selected trials, and trial-averaged activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.). Args: key (dict): key of ActivityAlignment master table roi (int): imaging segmentation mask axs (matplotlib.ax): optional definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) title (str): Optional title label Returns: fig (matplotlib.pyplot.figure): Figure of the event aligned activities. \"\"\" import matplotlib.pyplot as plt fig = None if axs is None : fig , ( ax0 , ax1 ) = plt . subplots ( 2 , 1 , figsize = ( 12 , 8 )) else : ax0 , ax1 = axs aligned_timestamps = ( self & key ) . fetch1 ( \"aligned_timestamps\" ) trial_ids , aligned_spikes = ( self . AlignedTrialActivity & key & { \"mask\" : roi } ) . fetch ( \"trial_id\" , \"aligned_trace\" , order_by = \"trial_id\" ) aligned_spikes = np . vstack ( aligned_spikes ) ax0 . imshow ( aligned_spikes , cmap = \"inferno\" , interpolation = \"nearest\" , aspect = \"auto\" , extent = ( aligned_timestamps [ 0 ], aligned_timestamps [ - 1 ], 0 , aligned_spikes . shape [ 0 ], ), ) ax0 . axvline ( x = 0 , linestyle = \"--\" , color = \"white\" ) ax0 . set_axis_off () ax1 . plot ( aligned_timestamps , np . nanmean ( aligned_spikes , axis = 0 )) ax1 . axvline ( x = 0 , linestyle = \"--\" , color = \"black\" ) ax1 . set_xlabel ( \"Time (s)\" ) ax1 . set_xlim ( aligned_timestamps [ 0 ], aligned_timestamps [ - 1 ]) if title : plt . suptitle ( title ) return fig", "title": "plot_aligned_activities()"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignmentCondition", "text": "Bases: dj . Manual Activity alignment condition. Attributes: Name Type Description imaging.Activity foreign key Primary key from imaging.Activity. event.AlignmentEvent foreign key Primary key from event.AlignmentEvent. trial_condition str User-friendly name of condition. bin_size float bin-size (in second) used to compute the PSTH, Source code in workflow_calcium_imaging/analysis.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @schema class ActivityAlignmentCondition ( dj . Manual ): \"\"\"Activity alignment condition. Attributes: imaging.Activity (foreign key): Primary key from imaging.Activity. event.AlignmentEvent (foreign key): Primary key from event.AlignmentEvent. trial_condition (str): User-friendly name of condition. condition_description (str, optional). Description. Default is ''. bin_size (float): bin-size (in second) used to compute the PSTH, \"\"\" definition = \"\"\" -> imaging.Activity -> event.AlignmentEvent trial_condition: varchar(128) # user-friendly name of condition --- condition_description='': varchar(1000) bin_size=0.04: float # bin-size (in second) used to compute the PSTH \"\"\" class Trial ( dj . Part ): \"\"\"Trial Attributes: ActivityAlignmentCondition (foreign key): Primary key from ActivityAlignmentCondition. trial.Trial: Primary key from trial.Trial. \"\"\" definition = \"\"\" # Trials (or subset) to compute event-aligned activity -> master -> trial.Trial \"\"\"", "title": "ActivityAlignmentCondition"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignmentCondition.Trial", "text": "Bases: dj . Part Trial Attributes: Name Type Description ActivityAlignmentCondition foreign key Primary key from ActivityAlignmentCondition. trial.Trial foreign key Primary key from trial.Trial. Source code in workflow_calcium_imaging/analysis.py 66 67 68 69 70 71 72 73 74 75 76 77 78 class Trial ( dj . Part ): \"\"\"Trial Attributes: ActivityAlignmentCondition (foreign key): Primary key from ActivityAlignmentCondition. trial.Trial: Primary key from trial.Trial. \"\"\" definition = \"\"\" # Trials (or subset) to compute event-aligned activity -> master -> trial.Trial \"\"\"", "title": "Trial"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.activate", "text": "Activate this schema. Parameters: Name Type Description Default schema_name str Schema name on the database server to activate the subject element. required create_schema bool When True (default), create schema in the database if it does not yet exist. True create_tables bool When True (default), create tables in the database if they do not yet exist. True linking_module str A module name or a module containing the required dependencies to activate the subject element: Upstream schema: scan, session, trial. None Source code in workflow_calcium_imaging/analysis.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def activate ( schema_name , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\"Activate this schema. Args: schema_name (str): Schema name on the database server to activate the `subject` element. create_schema (bool): When True (default), create schema in the database if it does not yet exist. create_tables (bool): When True (default), create tables in the database if they do not yet exist. linking_module (str): A module name or a module containing the required dependencies to activate the `subject` element: Upstream schema: scan, session, trial. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), ( \"The argument 'dependency' must \" + \"be a module's name or a module\" ) global _linking_module _linking_module = linking_module schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = linking_module . __dict__ , )", "title": "activate()"}, {"location": "api/workflow_calcium_imaging/ingest/", "text": "ingest_alignment ( alignment_csv_path = './user_data/alignments.csv' , skip_duplicates = True , verbose = True ) \u00b6 Ingest event alignment information This is duplicated across wf-array-ephys and wf-calcium-imaging. Parameters: Name Type Description Default alignment_csv_path str relative path of alignments.csv './user_data/alignments.csv' skip_duplicates bool Default True. Passed to DataJoint insert. True verbose bool Display number of entries inserted when ingesting. Default True. True Source code in workflow_calcium_imaging/ingest.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def ingest_alignment ( alignment_csv_path = \"./user_data/alignments.csv\" , skip_duplicates = True , verbose = True ): \"\"\"Ingest event alignment information This is duplicated across wf-array-ephys and wf-calcium-imaging. Args: alignment_csv_path (str): relative path of alignments.csv skip_duplicates (bool, optional): Default True. Passed to DataJoint insert. verbose (bool, optional): Display number of entries inserted when ingesting. Default True. \"\"\" csvs = [ alignment_csv_path ] tables = [ event . AlignmentEvent ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose ) ingest_events ( recording_csv_path = './user_data/behavior_recordings.csv' , block_csv_path = './user_data/blocks.csv' , trial_csv_path = './user_data/trials.csv' , event_csv_path = './user_data/events.csv' , skip_duplicates = True , verbose = True ) \u00b6 Ingest session, block, trial, and event data. Ingest each level of experiment hierarchy for element-trial: recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurances within trial). This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging. Parameters: Name Type Description Default recording_csv_path str relative path of behavior_recordings.csv. './user_data/behavior_recordings.csv' block_csv_path str relative path of blocks.csv. './user_data/blocks.csv' trial_csv_path str relative path of trials.csv. './user_data/trials.csv' event_csv_path str relative path of events.csv. './user_data/events.csv' skip_duplicates bool Default True. Passed to DataJoint insert. True verbose bool Display number of entries inserted when ingesting. Default True. True Source code in workflow_calcium_imaging/ingest.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def ingest_events ( recording_csv_path = \"./user_data/behavior_recordings.csv\" , block_csv_path = \"./user_data/blocks.csv\" , trial_csv_path = \"./user_data/trials.csv\" , event_csv_path = \"./user_data/events.csv\" , skip_duplicates = True , verbose = True , ): \"\"\" Ingest session, block, trial, and event data. Ingest each level of experiment hierarchy for element-trial: recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurances within trial). This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging. Args: recording_csv_path (str, optional): relative path of behavior_recordings.csv. block_csv_path (str, optional): relative path of blocks.csv. trial_csv_path (str, optional): relative path of trials.csv. event_csv_path (str, optional): relative path of events.csv. skip_duplicates (bool, optional): Default True. Passed to DataJoint insert. verbose (bool, optional): Display number of entries inserted when ingesting. Default True. \"\"\" csvs = [ recording_csv_path , recording_csv_path , block_csv_path , block_csv_path , trial_csv_path , trial_csv_path , trial_csv_path , trial_csv_path , event_csv_path , event_csv_path , event_csv_path , ] tables = [ event . BehaviorRecording (), event . BehaviorRecording . File (), trial . Block (), trial . Block . Attribute (), trial . TrialType (), trial . Trial (), trial . Trial . Attribute (), trial . BlockTrial (), event . EventType (), event . Event (), trial . TrialEvent (), ] # Allow direct insert required bc element-trial has Imported that should be Manual ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose , allow_direct_insert = True , ) ingest_sessions ( session_csv_path = './user_data/sessions.csv' , skip_duplicates = True , verbose = True ) \u00b6 Ingests all the manual table starting from session schema from ./user_data/sessions.csv. Parameters: Name Type Description Default session_csv_path str relative path of session csv. './user_data/sessions.csv' skip_duplicates bool Default True. Passed to DataJoint insert. True verbose bool Default True. Display number of entries inserted when ingesting. True Source code in workflow_calcium_imaging/ingest.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def ingest_sessions ( session_csv_path = \"./user_data/sessions.csv\" , skip_duplicates = True , verbose = True ): \"\"\"Ingests all the manual table starting from session schema from ./user_data/sessions.csv. Args: session_csv_path (str): relative path of session csv. skip_duplicates (bool): Default True. Passed to DataJoint insert. verbose (bool): Default True. Display number of entries inserted when ingesting. \"\"\" root_data_dir = get_imaging_root_data_dir () # ---------- Insert new \"Session\" and \"Scan\" --------- with open ( session_csv_path , newline = \"\" ) as f : input_sessions = list ( csv . DictReader ( f , delimiter = \",\" )) # Folder structure: root / subject / session / .tif (raw) session_list , session_dir_list , scan_list , scanner_list = [], [], [], [] for sess in input_sessions : sess_dir = find_full_path ( root_data_dir , Path ( sess [ \"session_dir\" ])) # search for either ScanImage or Scanbox files (in that order) for scan_pattern , scan_type , glob_func in zip ( [ \"*.tif\" , \"*.sbx\" ], [ \"ScanImage\" , \"Scanbox\" ], [ sess_dir . glob , sess_dir . rglob ], ): scan_filepaths = [ fp . as_posix () for fp in glob_func ( scan_pattern )] if len ( scan_filepaths ): acq_software = scan_type break else : raise FileNotFoundError ( \"Unable to identify scan files from the supported \" + \"acquisition softwares (ScanImage, Scanbox) at: \" + f \" { sess_dir } \" ) if acq_software == \"ScanImage\" : import scanreader from element_interface import scanimage_utils try : # attempt to read .tif as a scanimage file loaded_scan = scanreader . read_scan ( scan_filepaths ) recording_time = scanimage_utils . get_scanimage_acq_time ( loaded_scan ) header = scanimage_utils . parse_scanimage_header ( loaded_scan ) scanner = header [ \"SI_imagingSystem\" ] . strip ( \"'\" ) except Exception as e : print ( f \"ScanImage loading error: { scan_filepaths } \\n { str ( e ) } \" ) continue elif acq_software == \"Scanbox\" : import sbxreader try : # attempt to load Scanbox sbx_fp = pathlib . Path ( scan_filepaths [ 0 ]) sbx_meta = sbxreader . sbx_get_metadata ( sbx_fp ) # read from file when Scanbox support this recording_time = datetime . fromtimestamp ( sbx_fp . stat () . st_ctime ) scanner = sbx_meta . get ( \"imaging_system\" , \"Scanbox\" ) except Exception as e : print ( f \"Scanbox loading error: { scan_filepaths } \\n { str ( e ) } \" ) continue else : raise NotImplementedError ( \"Processing scan from acquisition software of \" + f \"type { acq_software } is not yet implemented\" ) session_key = { \"subject\" : sess [ \"subject\" ], \"session_datetime\" : recording_time } if session_key not in session . Session (): scanner_list . append ({ \"scanner\" : scanner }) session_list . append ( session_key ) scan_list . append ( { ** session_key , \"scan_id\" : 0 , \"scanner\" : scanner , \"acq_software\" : acq_software , } ) session_dir_list . append ( { ** session_key , \"session_dir\" : sess_dir . relative_to ( root_data_dir ) . as_posix (), } ) new_equipment = set ( val for dic in scanner_list for val in dic . values ()) if verbose : print ( f \" \\n ---- Insert { len ( new_equipment ) } entry(s) into \" + \"experiment.Equipment ----\" ) Equipment . insert ( scanner_list , skip_duplicates = skip_duplicates ) if verbose : print ( f \" \\n ---- Insert { len ( session_list ) } entry(s) into session.Session ----\" ) session . Session . insert ( session_list , skip_duplicates = skip_duplicates ) session . SessionDirectory . insert ( session_dir_list , skip_duplicates = skip_duplicates ) if verbose : print ( f \" \\n ---- Insert { len ( scan_list ) } entry(s) into scan.Scan ----\" ) scan . Scan . insert ( scan_list , skip_duplicates = skip_duplicates ) if verbose : print ( \" \\n ---- Successfully completed ingest_sessions ----\" ) ingest_subjects ( subject_csv_path = './user_data/subjects.csv' , skip_duplicates = True , verbose = True ) \u00b6 Inserts ./user_data/subject.csv data into corresponding subject schema tables. Parameters: Name Type Description Default subject_csv_path str relative path of subject csv. './user_data/subjects.csv' skip_duplicates bool Default True. Passed to DataJoint insert. True verbose bool Display number of entries inserted when ingesting. True Source code in workflow_calcium_imaging/ingest.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def ingest_subjects ( subject_csv_path : str = \"./user_data/subjects.csv\" , skip_duplicates : bool = True , verbose : bool = True ): \"\"\"Inserts ./user_data/subject.csv data into corresponding subject schema tables. Args: subject_csv_path (str): relative path of subject csv. skip_duplicates (bool): Default True. Passed to DataJoint insert. verbose (bool): Display number of entries inserted when ingesting. \"\"\" csvs = [ subject_csv_path ] tables = [ subject . Subject ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose )", "title": "ingest.py"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_alignment", "text": "Ingest event alignment information This is duplicated across wf-array-ephys and wf-calcium-imaging. Parameters: Name Type Description Default alignment_csv_path str relative path of alignments.csv './user_data/alignments.csv' skip_duplicates bool Default True. Passed to DataJoint insert. True verbose bool Display number of entries inserted when ingesting. Default True. True Source code in workflow_calcium_imaging/ingest.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def ingest_alignment ( alignment_csv_path = \"./user_data/alignments.csv\" , skip_duplicates = True , verbose = True ): \"\"\"Ingest event alignment information This is duplicated across wf-array-ephys and wf-calcium-imaging. Args: alignment_csv_path (str): relative path of alignments.csv skip_duplicates (bool, optional): Default True. Passed to DataJoint insert. verbose (bool, optional): Display number of entries inserted when ingesting. Default True. \"\"\" csvs = [ alignment_csv_path ] tables = [ event . AlignmentEvent ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose )", "title": "ingest_alignment()"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_events", "text": "Ingest session, block, trial, and event data. Ingest each level of experiment hierarchy for element-trial: recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurances within trial). This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging. Parameters: Name Type Description Default recording_csv_path str relative path of behavior_recordings.csv. './user_data/behavior_recordings.csv' block_csv_path str relative path of blocks.csv. './user_data/blocks.csv' trial_csv_path str relative path of trials.csv. './user_data/trials.csv' event_csv_path str relative path of events.csv. './user_data/events.csv' skip_duplicates bool Default True. Passed to DataJoint insert. True verbose bool Display number of entries inserted when ingesting. Default True. True Source code in workflow_calcium_imaging/ingest.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def ingest_events ( recording_csv_path = \"./user_data/behavior_recordings.csv\" , block_csv_path = \"./user_data/blocks.csv\" , trial_csv_path = \"./user_data/trials.csv\" , event_csv_path = \"./user_data/events.csv\" , skip_duplicates = True , verbose = True , ): \"\"\" Ingest session, block, trial, and event data. Ingest each level of experiment hierarchy for element-trial: recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurances within trial). This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging. Args: recording_csv_path (str, optional): relative path of behavior_recordings.csv. block_csv_path (str, optional): relative path of blocks.csv. trial_csv_path (str, optional): relative path of trials.csv. event_csv_path (str, optional): relative path of events.csv. skip_duplicates (bool, optional): Default True. Passed to DataJoint insert. verbose (bool, optional): Display number of entries inserted when ingesting. Default True. \"\"\" csvs = [ recording_csv_path , recording_csv_path , block_csv_path , block_csv_path , trial_csv_path , trial_csv_path , trial_csv_path , trial_csv_path , event_csv_path , event_csv_path , event_csv_path , ] tables = [ event . BehaviorRecording (), event . BehaviorRecording . File (), trial . Block (), trial . Block . Attribute (), trial . TrialType (), trial . Trial (), trial . Trial . Attribute (), trial . BlockTrial (), event . EventType (), event . Event (), trial . TrialEvent (), ] # Allow direct insert required bc element-trial has Imported that should be Manual ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose , allow_direct_insert = True , )", "title": "ingest_events()"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_sessions", "text": "Ingests all the manual table starting from session schema from ./user_data/sessions.csv. Parameters: Name Type Description Default session_csv_path str relative path of session csv. './user_data/sessions.csv' skip_duplicates bool Default True. Passed to DataJoint insert. True verbose bool Default True. Display number of entries inserted when ingesting. True Source code in workflow_calcium_imaging/ingest.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def ingest_sessions ( session_csv_path = \"./user_data/sessions.csv\" , skip_duplicates = True , verbose = True ): \"\"\"Ingests all the manual table starting from session schema from ./user_data/sessions.csv. Args: session_csv_path (str): relative path of session csv. skip_duplicates (bool): Default True. Passed to DataJoint insert. verbose (bool): Default True. Display number of entries inserted when ingesting. \"\"\" root_data_dir = get_imaging_root_data_dir () # ---------- Insert new \"Session\" and \"Scan\" --------- with open ( session_csv_path , newline = \"\" ) as f : input_sessions = list ( csv . DictReader ( f , delimiter = \",\" )) # Folder structure: root / subject / session / .tif (raw) session_list , session_dir_list , scan_list , scanner_list = [], [], [], [] for sess in input_sessions : sess_dir = find_full_path ( root_data_dir , Path ( sess [ \"session_dir\" ])) # search for either ScanImage or Scanbox files (in that order) for scan_pattern , scan_type , glob_func in zip ( [ \"*.tif\" , \"*.sbx\" ], [ \"ScanImage\" , \"Scanbox\" ], [ sess_dir . glob , sess_dir . rglob ], ): scan_filepaths = [ fp . as_posix () for fp in glob_func ( scan_pattern )] if len ( scan_filepaths ): acq_software = scan_type break else : raise FileNotFoundError ( \"Unable to identify scan files from the supported \" + \"acquisition softwares (ScanImage, Scanbox) at: \" + f \" { sess_dir } \" ) if acq_software == \"ScanImage\" : import scanreader from element_interface import scanimage_utils try : # attempt to read .tif as a scanimage file loaded_scan = scanreader . read_scan ( scan_filepaths ) recording_time = scanimage_utils . get_scanimage_acq_time ( loaded_scan ) header = scanimage_utils . parse_scanimage_header ( loaded_scan ) scanner = header [ \"SI_imagingSystem\" ] . strip ( \"'\" ) except Exception as e : print ( f \"ScanImage loading error: { scan_filepaths } \\n { str ( e ) } \" ) continue elif acq_software == \"Scanbox\" : import sbxreader try : # attempt to load Scanbox sbx_fp = pathlib . Path ( scan_filepaths [ 0 ]) sbx_meta = sbxreader . sbx_get_metadata ( sbx_fp ) # read from file when Scanbox support this recording_time = datetime . fromtimestamp ( sbx_fp . stat () . st_ctime ) scanner = sbx_meta . get ( \"imaging_system\" , \"Scanbox\" ) except Exception as e : print ( f \"Scanbox loading error: { scan_filepaths } \\n { str ( e ) } \" ) continue else : raise NotImplementedError ( \"Processing scan from acquisition software of \" + f \"type { acq_software } is not yet implemented\" ) session_key = { \"subject\" : sess [ \"subject\" ], \"session_datetime\" : recording_time } if session_key not in session . Session (): scanner_list . append ({ \"scanner\" : scanner }) session_list . append ( session_key ) scan_list . append ( { ** session_key , \"scan_id\" : 0 , \"scanner\" : scanner , \"acq_software\" : acq_software , } ) session_dir_list . append ( { ** session_key , \"session_dir\" : sess_dir . relative_to ( root_data_dir ) . as_posix (), } ) new_equipment = set ( val for dic in scanner_list for val in dic . values ()) if verbose : print ( f \" \\n ---- Insert { len ( new_equipment ) } entry(s) into \" + \"experiment.Equipment ----\" ) Equipment . insert ( scanner_list , skip_duplicates = skip_duplicates ) if verbose : print ( f \" \\n ---- Insert { len ( session_list ) } entry(s) into session.Session ----\" ) session . Session . insert ( session_list , skip_duplicates = skip_duplicates ) session . SessionDirectory . insert ( session_dir_list , skip_duplicates = skip_duplicates ) if verbose : print ( f \" \\n ---- Insert { len ( scan_list ) } entry(s) into scan.Scan ----\" ) scan . Scan . insert ( scan_list , skip_duplicates = skip_duplicates ) if verbose : print ( \" \\n ---- Successfully completed ingest_sessions ----\" )", "title": "ingest_sessions()"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_subjects", "text": "Inserts ./user_data/subject.csv data into corresponding subject schema tables. Parameters: Name Type Description Default subject_csv_path str relative path of subject csv. './user_data/subjects.csv' skip_duplicates bool Default True. Passed to DataJoint insert. True verbose bool Display number of entries inserted when ingesting. True Source code in workflow_calcium_imaging/ingest.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def ingest_subjects ( subject_csv_path : str = \"./user_data/subjects.csv\" , skip_duplicates : bool = True , verbose : bool = True ): \"\"\"Inserts ./user_data/subject.csv data into corresponding subject schema tables. Args: subject_csv_path (str): relative path of subject csv. skip_duplicates (bool): Default True. Passed to DataJoint insert. verbose (bool): Display number of entries inserted when ingesting. \"\"\" csvs = [ subject_csv_path ] tables = [ subject . Subject ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose )", "title": "ingest_subjects()"}, {"location": "api/workflow_calcium_imaging/paths/", "text": "get_imaging_root_data_dir () \u00b6 Retrieve imaging root data directory. Source code in workflow_calcium_imaging/paths.py 5 6 7 8 9 def get_imaging_root_data_dir () -> pathlib . Path : \"\"\"Retrieve imaging root data directory.\"\"\" data_dir = dj . config . get ( \"custom\" , {}) . get ( \"imaging_root_data_dir\" , None ) return pathlib . Path ( data_dir ) if data_dir else None get_nd2_files ( scan_key ) \u00b6 Retrieve the list of Nikon files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or nd2 files are not found. Source code in workflow_calcium_imaging/paths.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_nd2_files ( scan_key ): \"\"\"Retrieve the list of Nikon files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or nd2 files are not found. \"\"\" # Folder structure: root / subject / session / .nd2 data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) nd2_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.nd2\" )] if nd2_filepaths : return nd2_filepaths else : raise FileNotFoundError ( f \"No .nd2 file found in { sess_dir } \" ) get_prairieview_files ( scan_key ) \u00b6 Retrieve the list of PrairieView files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or tiff files are not found. Source code in workflow_calcium_imaging/paths.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def get_prairieview_files ( scan_key ): \"\"\"Retrieve the list of PrairieView files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or tiff files are not found. \"\"\" # Folder structure: root / subject / session / .tif data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) pv_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.tif\" )] if pv_filepaths : return pv_filepaths else : raise FileNotFoundError ( f \"No .tif file found in { sess_dir } \" ) get_scan_box_files ( scan_key ) \u00b6 Retrieve the list of Scanbox files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or scanbox files are not found. Source code in workflow_calcium_imaging/paths.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def get_scan_box_files ( scan_key ): \"\"\"Retrieve the list of Scanbox files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or scanbox files are not found. \"\"\" # Folder structure: root / subject / session / .sbx data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) sbx_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.sbx\" )] if sbx_filepaths : return sbx_filepaths else : raise FileNotFoundError ( f \"No .sbx file found in { sess_dir } \" ) get_scan_image_files ( scan_key ) \u00b6 Retrieve the list of ScanImage files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or tiff files are not found. Source code in workflow_calcium_imaging/paths.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def get_scan_image_files ( scan_key ): \"\"\"Retrieve the list of ScanImage files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or tiff files are not found. \"\"\" # Folder structure: root / subject / session / .tif (raw) data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) tiff_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.tif\" )] if tiff_filepaths : return tiff_filepaths else : raise FileNotFoundError ( f \"No tiff file found in { sess_dir } \" )", "title": "paths.py"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_imaging_root_data_dir", "text": "Retrieve imaging root data directory. Source code in workflow_calcium_imaging/paths.py 5 6 7 8 9 def get_imaging_root_data_dir () -> pathlib . Path : \"\"\"Retrieve imaging root data directory.\"\"\" data_dir = dj . config . get ( \"custom\" , {}) . get ( \"imaging_root_data_dir\" , None ) return pathlib . Path ( data_dir ) if data_dir else None", "title": "get_imaging_root_data_dir()"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_nd2_files", "text": "Retrieve the list of Nikon files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or nd2 files are not found. Source code in workflow_calcium_imaging/paths.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_nd2_files ( scan_key ): \"\"\"Retrieve the list of Nikon files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or nd2 files are not found. \"\"\" # Folder structure: root / subject / session / .nd2 data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) nd2_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.nd2\" )] if nd2_filepaths : return nd2_filepaths else : raise FileNotFoundError ( f \"No .nd2 file found in { sess_dir } \" )", "title": "get_nd2_files()"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_prairieview_files", "text": "Retrieve the list of PrairieView files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or tiff files are not found. Source code in workflow_calcium_imaging/paths.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def get_prairieview_files ( scan_key ): \"\"\"Retrieve the list of PrairieView files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or tiff files are not found. \"\"\" # Folder structure: root / subject / session / .tif data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) pv_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.tif\" )] if pv_filepaths : return pv_filepaths else : raise FileNotFoundError ( f \"No .tif file found in { sess_dir } \" )", "title": "get_prairieview_files()"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_scan_box_files", "text": "Retrieve the list of Scanbox files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or scanbox files are not found. Source code in workflow_calcium_imaging/paths.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def get_scan_box_files ( scan_key ): \"\"\"Retrieve the list of Scanbox files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or scanbox files are not found. \"\"\" # Folder structure: root / subject / session / .sbx data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) sbx_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.sbx\" )] if sbx_filepaths : return sbx_filepaths else : raise FileNotFoundError ( f \"No .sbx file found in { sess_dir } \" )", "title": "get_scan_box_files()"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_scan_image_files", "text": "Retrieve the list of ScanImage files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or tiff files are not found. Source code in workflow_calcium_imaging/paths.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def get_scan_image_files ( scan_key ): \"\"\"Retrieve the list of ScanImage files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or tiff files are not found. \"\"\" # Folder structure: root / subject / session / .tif (raw) data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) tiff_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.tif\" )] if tiff_filepaths : return tiff_filepaths else : raise FileNotFoundError ( f \"No tiff file found in { sess_dir } \" )", "title": "get_scan_image_files()"}, {"location": "api/workflow_calcium_imaging/pipeline/", "text": "Equipment \u00b6 Bases: dj . Manual Equipment Attributes: Name Type Description scanner str Scanner used in imaging. Source code in workflow_calcium_imaging/pipeline.py 67 68 69 70 71 72 73 74 75 76 77 @lab . schema class Equipment ( dj . Manual ): \"\"\"Equipment Attributes: scanner (str): Scanner used in imaging. \"\"\" definition = \"\"\" scanner: varchar(32) \"\"\" get_imaging_root_data_dir () \u00b6 Retrieve imaging root data directory. Source code in workflow_calcium_imaging/paths.py 5 6 7 8 9 def get_imaging_root_data_dir () -> pathlib . Path : \"\"\"Retrieve imaging root data directory.\"\"\" data_dir = dj . config . get ( \"custom\" , {}) . get ( \"imaging_root_data_dir\" , None ) return pathlib . Path ( data_dir ) if data_dir else None get_nd2_files ( scan_key ) \u00b6 Retrieve the list of Nikon files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or nd2 files are not found. Source code in workflow_calcium_imaging/paths.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_nd2_files ( scan_key ): \"\"\"Retrieve the list of Nikon files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or nd2 files are not found. \"\"\" # Folder structure: root / subject / session / .nd2 data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) nd2_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.nd2\" )] if nd2_filepaths : return nd2_filepaths else : raise FileNotFoundError ( f \"No .nd2 file found in { sess_dir } \" ) get_prairieview_files ( scan_key ) \u00b6 Retrieve the list of PrairieView files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or tiff files are not found. Source code in workflow_calcium_imaging/paths.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def get_prairieview_files ( scan_key ): \"\"\"Retrieve the list of PrairieView files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or tiff files are not found. \"\"\" # Folder structure: root / subject / session / .tif data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) pv_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.tif\" )] if pv_filepaths : return pv_filepaths else : raise FileNotFoundError ( f \"No .tif file found in { sess_dir } \" ) get_scan_box_files ( scan_key ) \u00b6 Retrieve the list of Scanbox files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or scanbox files are not found. Source code in workflow_calcium_imaging/paths.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def get_scan_box_files ( scan_key ): \"\"\"Retrieve the list of Scanbox files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or scanbox files are not found. \"\"\" # Folder structure: root / subject / session / .sbx data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) sbx_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.sbx\" )] if sbx_filepaths : return sbx_filepaths else : raise FileNotFoundError ( f \"No .sbx file found in { sess_dir } \" ) get_scan_image_files ( scan_key ) \u00b6 Retrieve the list of ScanImage files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or tiff files are not found. Source code in workflow_calcium_imaging/paths.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def get_scan_image_files ( scan_key ): \"\"\"Retrieve the list of ScanImage files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or tiff files are not found. \"\"\" # Folder structure: root / subject / session / .tif (raw) data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) tiff_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.tif\" )] if tiff_filepaths : return tiff_filepaths else : raise FileNotFoundError ( f \"No tiff file found in { sess_dir } \" )", "title": "pipeline.py"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.Equipment", "text": "Bases: dj . Manual Equipment Attributes: Name Type Description scanner str Scanner used in imaging. Source code in workflow_calcium_imaging/pipeline.py 67 68 69 70 71 72 73 74 75 76 77 @lab . schema class Equipment ( dj . Manual ): \"\"\"Equipment Attributes: scanner (str): Scanner used in imaging. \"\"\" definition = \"\"\" scanner: varchar(32) \"\"\"", "title": "Equipment"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_imaging_root_data_dir", "text": "Retrieve imaging root data directory. Source code in workflow_calcium_imaging/paths.py 5 6 7 8 9 def get_imaging_root_data_dir () -> pathlib . Path : \"\"\"Retrieve imaging root data directory.\"\"\" data_dir = dj . config . get ( \"custom\" , {}) . get ( \"imaging_root_data_dir\" , None ) return pathlib . Path ( data_dir ) if data_dir else None", "title": "get_imaging_root_data_dir()"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_nd2_files", "text": "Retrieve the list of Nikon files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or nd2 files are not found. Source code in workflow_calcium_imaging/paths.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_nd2_files ( scan_key ): \"\"\"Retrieve the list of Nikon files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or nd2 files are not found. \"\"\" # Folder structure: root / subject / session / .nd2 data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) nd2_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.nd2\" )] if nd2_filepaths : return nd2_filepaths else : raise FileNotFoundError ( f \"No .nd2 file found in { sess_dir } \" )", "title": "get_nd2_files()"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_prairieview_files", "text": "Retrieve the list of PrairieView files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or tiff files are not found. Source code in workflow_calcium_imaging/paths.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def get_prairieview_files ( scan_key ): \"\"\"Retrieve the list of PrairieView files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or tiff files are not found. \"\"\" # Folder structure: root / subject / session / .tif data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) pv_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.tif\" )] if pv_filepaths : return pv_filepaths else : raise FileNotFoundError ( f \"No .tif file found in { sess_dir } \" )", "title": "get_prairieview_files()"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_scan_box_files", "text": "Retrieve the list of Scanbox files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or scanbox files are not found. Source code in workflow_calcium_imaging/paths.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def get_scan_box_files ( scan_key ): \"\"\"Retrieve the list of Scanbox files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or scanbox files are not found. \"\"\" # Folder structure: root / subject / session / .sbx data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) sbx_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.sbx\" )] if sbx_filepaths : return sbx_filepaths else : raise FileNotFoundError ( f \"No .sbx file found in { sess_dir } \" )", "title": "get_scan_box_files()"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_scan_image_files", "text": "Retrieve the list of ScanImage files associated with a given Scan. Parameters: Name Type Description Default scan_key dict Primary key from Scan. required Returns: Name Type Description path list Absolute path(s) of the scan files. Raises: Type Description FileNotFoundError If the session directory or tiff files are not found. Source code in workflow_calcium_imaging/paths.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def get_scan_image_files ( scan_key ): \"\"\"Retrieve the list of ScanImage files associated with a given Scan. Args: scan_key (dict): Primary key from Scan. Returns: path (list): Absolute path(s) of the scan files. Raises: FileNotFoundError: If the session directory or tiff files are not found. \"\"\" # Folder structure: root / subject / session / .tif (raw) data_dir = get_imaging_root_data_dir () from .pipeline import session sess_dir = data_dir / ( session . SessionDirectory & scan_key ) . fetch1 ( \"session_dir\" ) if not sess_dir . exists (): raise FileNotFoundError ( f \"Session directory not found ( { sess_dir } )\" ) tiff_filepaths = [ fp . as_posix () for fp in sess_dir . glob ( \"*.tif\" )] if tiff_filepaths : return tiff_filepaths else : raise FileNotFoundError ( f \"No tiff file found in { sess_dir } \" )", "title": "get_scan_image_files()"}, {"location": "api/workflow_calcium_imaging/process/", "text": "run ( display_progress = True ) \u00b6 Run all make methods from element-calcium imaging Parameters: Name Type Description Default display_progress bool Whether to display the progress. Default is True. True Source code in workflow_calcium_imaging/process.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def run ( display_progress = True ): \"\"\"Run all `make` methods from element-calcium imaging Args: display_progress (bool, optional): Whether to display the progress. Default is True. \"\"\" populate_settings = { \"display_progress\" : display_progress , \"reserve_jobs\" : False , \"suppress_errors\" : False , } print ( \" \\n ---- Populate imported and computed tables ----\" ) scan . ScanInfo . populate ( ** populate_settings ) imaging . Processing . populate ( ** populate_settings ) imaging . MotionCorrection . populate ( ** populate_settings ) imaging . Segmentation . populate ( ** populate_settings ) imaging . MaskClassification . populate ( ** populate_settings ) imaging . Fluorescence . populate ( ** populate_settings ) imaging . Activity . populate ( ** populate_settings ) print ( \" \\n ---- Successfully completed workflow_calcium_imaging/process.py ----\" )", "title": "process.py"}, {"location": "api/workflow_calcium_imaging/process/#workflow_calcium_imaging.process.run", "text": "Run all make methods from element-calcium imaging Parameters: Name Type Description Default display_progress bool Whether to display the progress. Default is True. True Source code in workflow_calcium_imaging/process.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def run ( display_progress = True ): \"\"\"Run all `make` methods from element-calcium imaging Args: display_progress (bool, optional): Whether to display the progress. Default is True. \"\"\" populate_settings = { \"display_progress\" : display_progress , \"reserve_jobs\" : False , \"suppress_errors\" : False , } print ( \" \\n ---- Populate imported and computed tables ----\" ) scan . ScanInfo . populate ( ** populate_settings ) imaging . Processing . populate ( ** populate_settings ) imaging . MotionCorrection . populate ( ** populate_settings ) imaging . Segmentation . populate ( ** populate_settings ) imaging . MaskClassification . populate ( ** populate_settings ) imaging . Fluorescence . populate ( ** populate_settings ) imaging . Activity . populate ( ** populate_settings ) print ( \" \\n ---- Successfully completed workflow_calcium_imaging/process.py ----\" )", "title": "run()"}, {"location": "api/workflow_calcium_imaging/version/", "text": "Package metadata Update the Docker image tag in docker-compose-test.yaml and docker-compose-dev.yaml to match", "title": "version.py"}]}