{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Element Calcium Imaging", "text": "<p>DataJoint Element for functional calcium imaging with <code>ScanImage</code>, <code>Scanbox</code>, <code>Nikon NIS</code>, or <code>PrairieView</code> acquisition software and <code>Suite2p</code>, <code>CaImAn</code>, and <code>EXTRACT</code> analysis software. DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline.</p> <p></p> <p>Visit the Concepts page for more information on Element Calcium Imaging. To get started with building your data pipeline navigate to the Tutorials page.</p>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>Observes Semantic Versioning standard and Keep a Changelog convention.</p>"}, {"location": "changelog/#052-2023-01-11", "title": "0.5.2 - 2023-01-11", "text": "<ul> <li>Bugfix - fix errors in ingesting single-plane PrairieView scans into <code>ScanInfo</code></li> <li>Add - Optional installation of caiman and suite2p through pip</li> </ul>"}, {"location": "changelog/#051-2022-12-15", "title": "0.5.1 - 2022-12-15", "text": "<ul> <li>Add - Imports for prairieview loader</li> </ul>"}, {"location": "changelog/#050-2022-12-14", "title": "0.5.0 - 2022-12-14", "text": "<ul> <li>Add - Cell extraction with EXTRACT package</li> </ul>"}, {"location": "changelog/#042-2022-11-02", "title": "0.4.2 - 2022-11-02", "text": "<ul> <li>Bugfix - Add plotting package to the requirements to generate the figures</li> <li>Add - Scan date parser from nd2 files</li> </ul>"}, {"location": "changelog/#041-2022-10-28", "title": "0.4.1 - 2022-10-28", "text": "<ul> <li>Update - Bump version to trigger PyPI release to revert updates from incorrect tag</li> </ul>"}, {"location": "changelog/#040-2022-10-28", "title": "0.4.0 - 2022-10-28", "text": "<ul> <li>Add - New schema <code>imaging_report</code> to compute and store figures from results</li> <li>Add - Widget to display figures</li> </ul>"}, {"location": "changelog/#030-2022-10-07", "title": "0.3.0 - 2022-10-07", "text": "<ul> <li>Add - Reader for <code>Bruker PrairieView</code> acquisition system</li> </ul>"}, {"location": "changelog/#022-2022-09-28", "title": "0.2.2 - 2022-09-28", "text": "<ul> <li>Update - Minor table explanation edits</li> <li>Update - Query simplifications</li> <li>Update - Minor code refactoring</li> </ul>"}, {"location": "changelog/#021-2022-09-12", "title": "0.2.1 - 2022-09-12", "text": "<ul> <li>Bugfix - fix errors in auto generating new ProcessingTask</li> </ul>"}, {"location": "changelog/#020-2022-07-01", "title": "0.2.0 - 2022-07-01", "text": "<ul> <li>Add - Imaging module (imaging_preprocess.py) for pre-processing steps</li> </ul>"}, {"location": "changelog/#010-2022-06-29", "title": "0.1.0 - 2022-06-29", "text": "<ul> <li>Add - Support for element-interface</li> <li>Add - Trigger Suite2p and CaImAn</li> <li>Add - Imaging module for no curation</li> <li>Add - Support for Nikon acquisition system</li> <li>Add - <code>scan_datetime</code> and <code>scan_duration</code> attributes</li> <li>Add - Estimate for scan duration</li> <li>Add - Citation section to README</li> <li>Update - Move background file to elements.datajoint.org</li> <li>Add - Adopt black formatting into code base</li> </ul>"}, {"location": "changelog/#010b0-2021-05-07", "title": "0.1.0b0 - 2021-05-07", "text": "<ul> <li>Update - First beta release</li> </ul>"}, {"location": "changelog/#010a4-2021-05-07", "title": "0.1.0a4 - 2021-05-07", "text": "<ul> <li>Update - Add workaround to handle DataJoint 0.13.* issue #914</li> </ul>"}, {"location": "changelog/#010a3-2021-05-03", "title": "0.1.0a3 - 2021-05-03", "text": "<ul> <li>Add - GitHub Action release process</li> <li>Add - <code>scan</code> and <code>imaging</code> modules</li> <li>Add - Readers for <code>ScanImage</code>, <code>ScanBox</code>, <code>Suite2p</code>, <code>CaImAn</code></li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID).</p> <ul> <li>Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D,   Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for   Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358</li> </ul> <ul> <li>DataJoint Elements (RRID:SCR_021894) -   Element Calcium Imaging (version 0.5.2)</li> </ul>"}, {"location": "concepts/", "title": "Concepts", "text": ""}, {"location": "concepts/#multiphoton-calcium-imaging", "title": "Multiphoton Calcium Imaging", "text": "<p>Over the past two decades, in vivo two-photon laser-scanning imaging of calcium signals has evolved into a mainstream modality for neurophysiology experiments to record population activity in intact neural circuits. The tools for signal acquisition and analysis continue to evolve but common patterns and elements of standardization have emerged.</p> <p>The preprocessing workflow for two-photon laser-scanning microscopy includes motion correction (rigid or non-rigid), cell segmentation, and calcium event extraction (sometimes described as \"deconvolution\" or \"spike inference\"). Some include raster artifact correction, cropping and stitching operations.</p> <p> </p> Left to right: Raw scans, Motion corrected scans, Cell segmentation, Calcium events <p>For a long time, most labs developed custom processing pipelines, sharing them with others as academic open-source projects. This has changed recently with the emerging of a few leaders as the standardization candidates for the initial preprocessing.</p> <ul> <li>CaImAn (Originally developed by Andrea Giovannucci, current support by FlatIron Institute: Eftychios A. Pnevmatikakis, Johannes Friedrich)</li> <li>Suite2p (Carsen Stringer and Marius Pachitariu at Janelia), 200+ users, active support</li> <li>EXTRACT (Hakan Inan et al. 2017, 2021).</li> </ul> <p>Element Calcium Imaging encapsulates these packages to ease the management of data and its analysis.</p>"}, {"location": "concepts/#key-partnerships", "title": "Key partnerships", "text": "<p>Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for two-photon Calcium imaging. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, pipeline design, associated tools, and interfaces.</p> <p>These teams include:</p> <ul> <li>MICrONS (Andreas Tolias Lab, BCM) - https://github.com/cajal</li> <li>BrainCoGs (Princeton) - https://github.com/BrainCOGS</li> <li>Moser Group (Kavli Institute/NTNU) - private repository</li> <li>Anne Churchland Lab (UCLA)</li> </ul>"}, {"location": "concepts/#acquisition-tools", "title": "Acquisition tools", "text": ""}, {"location": "concepts/#hardware", "title": "Hardware", "text": "<p>The primary acquisition systems are:</p> <ul> <li>Sutter</li> <li>Thorlabs</li> <li>Bruker</li> <li>Neurolabware</li> </ul> <p>We do not include Miniscopes in these estimates. In, all there are perhaps on the order of 3000 two-photon setups globally but their processing needs may need to be further segmented.</p>"}, {"location": "concepts/#software", "title": "Software", "text": "<ul> <li>ScanImage</li> <li>ThorImageLS</li> <li>Scanbox</li> <li>Nikon</li> </ul> <p>Vidrio\u2019s ScanImage is the data acquisition software for two types of home-built scanning two-photon systems, either based on Thorlabs and Sutter hardware. ScanImage has a free version and a licensed version. Thorlabs also provides their own acquisition software - ThorImageLS (probably half of the systems).</p>"}, {"location": "concepts/#element-features", "title": "Element Features", "text": "<p>Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Calcium Imaging Element with the repository hosted at https://github.com/datajoint/element-calcium-imaging.</p> <p>Major features of the Calcium Imaging Element include:</p> <ul> <li>Calcium-imaging scanning metadata, also compatible with mesoscale imaging and multi-ROI scanning mode</li> <li>Tables for all processing steps: motion correction, segmentation, cell spatial footprint, fluorescence trace extraction, spike inference and cell classification</li> <li>Store/track/manage different curations of the segmentation results</li> <li>Ingestion support for data acquired with ScanImage, Scanbox, Nikon NIS, and PrairieView acquisition systems</li> <li>Ingestion support for processing outputs from both Suite2p and CaImAn analysis suites</li> <li>Sample data and complete test suite for quality assurance</li> <li>Cell extraction with the EXTRACT analysis package.</li> </ul> <p>The processing workflow is typically performed on a per-scan basis, however, depending on the nature of the research questions, different labs may opt to perform processing/segmentation on a concatenated set of data from multiple scans. To this end, we have extended the Calcium Imaging Element and provided a design version capable of supporting a multi-scan processing scheme.</p>"}, {"location": "concepts/#element-architecture", "title": "Element Architecture", "text": "<p>Each node in the following diagram represents the analysis code in the workflow and the corresponding table in the database.  Within the workflow, Element Calcium Imaging connects to upstream Elements including Lab, Animal, and Session. For more detailed documentation on each table, see the API docs for the respective schemas.</p> <p>The Element is composed of two main schemas, <code>scan</code> and <code>imaging</code>. To handle several use cases of this pipeline, we have designed two alternatives to <code>imaging</code> schemas, including <code>imaging_no_curation</code> and <code>imaging_preprocess</code>.</p> <ul> <li><code>imaging</code> module - Multiple scans are acquired during each session and each scan is processed independently.</li> </ul> <p></p> <ul> <li><code>imaging_no_curation</code> module - Same as <code>imaging</code> module, without the Curation table.</li> </ul> <p></p> <ul> <li><code>imaging_preprocess</code> module - Same as <code>imaging</code> module. Additionally, pre-processing steps can be performed on each scan prior to processing with Suite2p or CaImAn. </li> </ul>"}, {"location": "concepts/#lab-schema-api-docs", "title": "<code>lab</code> schema (API docs)", "text": "Table Description Equipment Scanner metadata"}, {"location": "concepts/#subject-schema-api-docs", "title": "<code>subject</code> schema (API docs)", "text": "<ul> <li>Although not required, most choose to connect the <code>Session</code> table to a <code>Subject</code> table.</li> </ul> Table Description Subject Basic information of the research subject"}, {"location": "concepts/#session-schema-api-docs", "title": "<code>session</code> schema (API docs)", "text": "Table Description Session Unique experimental session identifier"}, {"location": "concepts/#scan-schema-api-docs", "title": "<code>scan</code> schema (API docs)", "text": "Table Description AcquisitionSoftware Software used in the acquisiton of the imaging scans Channel Recording Channel Scan A set of imaging scans perfomed in a single session ScanLocation Anatomical location of the region scanned ScanInfo Metadata of the imaging scan ScanInfo.Field Metadata of the fields imaged ScanInfo.ScanFile Path of the scan file"}, {"location": "concepts/#imaging-schema-api-docs", "title": "<code>imaging</code> schema (API docs)", "text": "Table Description ProcessingMethod Available analysis suites that can be used in processing of the imaging scans ProcessingParamSet All parameters required to process a calcium imaging scan CellCompartment Cell compartments that can be imaged MaskType Available labels for segmented masks ProcessingTask Task defined by a combination of Scan and ProcessingParamSet Processing The core table that executes a ProcessingTask Curation Curated results MotionCorrection Results of the motion correction procedure MotionCorrection.RigidMotionCorrection Details of the rigid motion correction performed on the imaging data MotionCorrection.NonRigidMotionCorrection Details of nonrigid motion correction performed on the imaging data MotionCorrection.NonRigidMotionCorrection.Block Results of non-rigid motion correction for each block MotionCorrection.Summary Summary images for each field and channel after motion corrections Segmentation Results of the segmentation Segmentation.Mask Masks identified in the segmentation procedure MaskClassificationMethod Method used in the mask classification procedure MaskClassification Result of the mask classification procedure MaskClassification.MaskType Type assigned to each mask Fluorescence Fluorescence measurements Fluorescence.Trace Fluorescence traces for each region of interest ActivityExtractionMethod Method used in activity extraction Activity Inferred neural activity Activity.Trace Inferred neural activity from fluorescence traces"}, {"location": "concepts/#roadmap", "title": "Roadmap", "text": "<p>Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will add the following features to this Element:</p> <ul> <li>Data quality metrics</li> <li>Data compression</li> <li>Deepinterpolation</li> <li>Data export to NWB</li> <li>Data publishing to DANDI</li> </ul>"}, {"location": "tutorials/", "title": "Tutorials", "text": ""}, {"location": "tutorials/#installation", "title": "Installation", "text": "<p>Installation of the Element requires an integrated development environment and database. Instructions to setup each of the components can be found on the  User Instructions page. These  instructions use the example  workflow for Element Calcium Imaging,  which can be modified for a user's specific experimental requirements. This example workflow uses four Elements (Lab, Animal, Session, and Calcium Imaging) to construct a complete pipeline, and is able to ingest experimental metadata and process calcium imaging scans.</p>"}, {"location": "tutorials/#videos", "title": "Videos", "text": "<p>Our YouTube tutorial gives an overview  of the workflow files, notebooks, as well as core concepts related to calcium imaging analysis. To try out Elements notebooks in an online Jupyter environment with access to example data, visit  CodeBook. (Calcium Imaging notebooks coming soon!)</p>"}, {"location": "tutorials/#notebooks", "title": "Notebooks", "text": "<p>Each of the  notebooks in  the workflow steps through ways to interact with the Element itself.</p> <ul> <li>00-DataDownload highlights how to use DataJoint tools to download a sample model for trying out the Element.</li> </ul> <ul> <li>01-Configure helps configure your local DataJoint installation to point to the correct database.</li> </ul> <ul> <li>02-WorkflowStructure demonstrates the table architecture of the Element and key DataJoint basics for interacting with these tables.</li> </ul> <ul> <li>03-Process steps through adding data to the tables and analyzing a calcium imaging scan.</li> </ul> <ul> <li>04-Automate highlights the same steps as above, but utilizing all built-in automation tools.</li> </ul> <ul> <li>05-Explore demonstrates the steps to fetch the results stored in the tables and plot them.</li> </ul> <ul> <li>06-Drop provides the steps for dropping all the tables to start fresh.</li> </ul> <ul> <li>07-DownStreamAnalysis demonstrates event- and trial-based analysis.</li> </ul>"}, {"location": "tutorials/#extract", "title": "EXTRACT", "text": "<p>Analysis with the EXTRACT package is currently supported for single channel, single plane scans with using Suite2p for motion correction. For processing with EXTRACT, please refer to the notebook 03-Process, set <code>processing_method=\"extract\"</code> in the ProcessingParamSet table, and provide the <code>params</code> attribute of the ProcessingParamSet table in the <code>{'suite2p': {...}, 'extract': {...}}</code> dictionary format. Please also install the MATLAB engine API for Python.</p>"}, {"location": "api/element_calcium_imaging/imaging/", "title": "imaging.py", "text": ""}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity", "title": "<code>Activity</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>ActivityExtractionMethod</code> <code>foreign key</code> <p>Primary key from ActivityExtractionMethod.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Activity(dj.Computed):\n\"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        ActivityExtractionMethod (foreign key): Primary key from\n            ActivityExtractionMethod.\n    \"\"\"\n\n    definition = \"\"\"# Neural Activity\n    -&gt; Fluorescence\n    -&gt; ActivityExtractionMethod\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n        Attributes:\n            Activity (foreign key): Primary key from Activity.\n            Fluorescence.Trace (foreign key): Fluorescence.Trace.\n            activity_trace (longblob): Neural activity from fluoresence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        activity_trace: longblob\n        \"\"\"\n\n    @property\n    def key_source(self):\n        suite2p_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"suite2p\"'\n            &amp; 'extraction_method LIKE \"suite2p%\"'\n        )\n        caiman_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"caiman\"'\n            &amp; 'extraction_method LIKE \"caiman%\"'\n        )\n        return suite2p_key_source.proj() + caiman_key_source.proj()\n\n    def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis\n        outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n                suite2p_dataset = imaging_dataset\n                # ---- iterate through all s2p plane outputs ----\n                spikes = [\n                    dict(\n                        key,\n                        mask=mask_idx,\n                        fluo_channel=0,\n                        activity_trace=spks,\n                    )\n                    for mask_idx, spks in enumerate(\n                        s\n                        for plane in suite2p_dataset.planes.values()\n                        for s in plane.spks\n                    )\n                ]\n\n                self.insert1(key)\n                self.Trace.insert(spikes)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            if key[\"extraction_method\"] in (\n                \"caiman_deconvolution\",\n                \"caiman_dff\",\n            ):\n                attr_mapper = {\n                    \"caiman_deconvolution\": \"spikes\",\n                    \"caiman_dff\": \"dff\",\n                }\n\n                # infer \"segmentation_channel\" - from params if available, else from caiman loader\n                params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n                segmentation_channel = params.get(\n                    \"segmentation_channel\", caiman_dataset.segmentation_channel\n                )\n\n                self.insert1(key)\n                self.Trace.insert(\n                    dict(\n                        key,\n                        mask=mask[\"mask_id\"],\n                        fluo_channel=segmentation_channel,\n                        activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                    )\n                    for mask in caiman_dataset.masks\n                )\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trace(s) for each mask.</p> <p>Attributes:</p> Name Type Description <code>Activity</code> <code>foreign key</code> <p>Primary key from Activity.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Fluorescence.Trace.</p> <code>activity_trace</code> <code>longblob</code> <p>Neural activity from fluoresence trace.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n    Attributes:\n        Activity (foreign key): Primary key from Activity.\n        Fluorescence.Trace (foreign key): Fluorescence.Trace.\n        activity_trace (longblob): Neural activity from fluoresence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    activity_trace: longblob\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Activity with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis\n    outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n            suite2p_dataset = imaging_dataset\n            # ---- iterate through all s2p plane outputs ----\n            spikes = [\n                dict(\n                    key,\n                    mask=mask_idx,\n                    fluo_channel=0,\n                    activity_trace=spks,\n                )\n                for mask_idx, spks in enumerate(\n                    s\n                    for plane in suite2p_dataset.planes.values()\n                    for s in plane.spks\n                )\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(spikes)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        if key[\"extraction_method\"] in (\n            \"caiman_deconvolution\",\n            \"caiman_dff\",\n        ):\n            attr_mapper = {\n                \"caiman_deconvolution\": \"spikes\",\n                \"caiman_dff\": \"dff\",\n            }\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            self.insert1(key)\n            self.Trace.insert(\n                dict(\n                    key,\n                    mask=mask[\"mask_id\"],\n                    fluo_channel=segmentation_channel,\n                    activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                )\n                for mask in caiman_dataset.masks\n            )\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ActivityExtractionMethod", "title": "<code>ActivityExtractionMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available activity extraction methods.</p> <p>Attributes:</p> Name Type Description <code>extraction_method</code> <code>str</code> <p>Extraction method.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ActivityExtractionMethod(dj.Lookup):\n\"\"\"Available activity extraction methods.\n\n    Attributes:\n        extraction_method (str): Extraction method.\n    \"\"\"\n\n    definition = \"\"\"# Activity extraction method \n    extraction_method: varchar(32)\n    \"\"\"\n\n    contents = zip([\"suite2p_deconvolution\", \"caiman_deconvolution\", \"caiman_dff\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.CellCompartment", "title": "<code>CellCompartment</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)</p> <p>Attributes:</p> Name Type Description <code>cell_compartment</code> <code>str</code> <p>Cell compartment.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass CellCompartment(dj.Lookup):\n\"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)\n\n    Attributes:\n        cell_compartment (str): Cell compartment.\n    \"\"\"\n\n    definition = \"\"\"# Cell compartments\n    cell_compartment: char(16)\n    \"\"\"\n\n    contents = zip([\"axon\", \"soma\", \"bouton\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Curated results</p> <p>If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir.</p> <p>Attributes:</p> Name Type Description <code>Processing</code> <code>foreign key</code> <p>Primary key from Processing.</p> <code>curation_id</code> <code>int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time of generation of this set of curated results.</p> <code>curation_output_dir</code> <code>str</code> <p>Output directory of the curated results, relative to root data directory.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this result.</p> <code>curation_note</code> <code>str</code> <p>Notes about the curation task.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curated results\n\n    If no curation is applied, the curation_output_dir can be set to\n    the value of processing_output_dir.\n\n    Attributes:\n        Processing (foreign key): Primary key from Processing.\n        curation_id (int): Unique curation ID.\n        curation_time (datetime): Time of generation of this set of curated results.\n        curation_output_dir (str): Output directory of the curated results, relative to\n            root data directory.\n        manual_curation (bool): If True, manual curation has been performed on this\n            result.\n        curation_note (str, optional): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"# Curation(s) results\n    -&gt; Processing\n    curation_id: int\n    ---\n    curation_time: datetime  # Time of generation of this set of curated results \n    curation_output_dir: varchar(255)  # Output directory of the curated results, relative to root data directory\n    manual_curation: bool  # Has manual curation been performed on this result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_processing_task(self, key, is_curated=False, curation_note=\"\"):\n\"\"\"Create a Curation entry for a given ProcessingTask key.\n\n        Args:\n            key (dict): Primary key set of an entry in the ProcessingTask table.\n            is_curated (bool): When True, indicates a manual curation.\n            curation_note (str): User's note on the specifics of the curation.\n        \"\"\"\n        if key not in Processing():\n            raise ValueError(\n                f\"No corresponding entry in Processing available for: {key};\"\n                f\"Please run `Processing.populate(key)`\"\n            )\n\n        output_dir = (ProcessingTask &amp; key).fetch1(\"processing_output_dir\")\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n            curation_time = suite2p_dataset.creation_time\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            curation_time = caiman_dataset.creation_time\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            curation_time = extract_dataset.creation_time\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": curation_time,\n                \"curation_output_dir\": output_dir,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Curation.create1_from_processing_task", "title": "<code>create1_from_processing_task(key, is_curated=False, curation_note='')</code>", "text": "<p>Create a Curation entry for a given ProcessingTask key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key set of an entry in the ProcessingTask table.</p> required <code>is_curated</code> <code>bool</code> <p>When True, indicates a manual curation.</p> <code>False</code> <code>curation_note</code> <code>str</code> <p>User's note on the specifics of the curation.</p> <code>''</code> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def create1_from_processing_task(self, key, is_curated=False, curation_note=\"\"):\n\"\"\"Create a Curation entry for a given ProcessingTask key.\n\n    Args:\n        key (dict): Primary key set of an entry in the ProcessingTask table.\n        is_curated (bool): When True, indicates a manual curation.\n        curation_note (str): User's note on the specifics of the curation.\n    \"\"\"\n    if key not in Processing():\n        raise ValueError(\n            f\"No corresponding entry in Processing available for: {key};\"\n            f\"Please run `Processing.populate(key)`\"\n        )\n\n    output_dir = (ProcessingTask &amp; key).fetch1(\"processing_output_dir\")\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n        curation_time = suite2p_dataset.creation_time\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n        curation_time = caiman_dataset.creation_time\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        curation_time = extract_dataset.creation_time\n    else:\n        raise NotImplementedError(\"Unknown method: {}\".format(method))\n\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": curation_time,\n            \"curation_output_dir\": output_dir,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence", "title": "<code>Fluorescence</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Fluorescence(dj.Computed):\n\"\"\"Fluorescence traces.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n    \"\"\"\n\n    definition = \"\"\"# Fluorescence traces before spike extraction or filtering\n    -&gt; Segmentation\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n                comes from.\n            fluorescence (longblob): Fluorescence trace associated with this mask.\n            neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from         \n        ---\n        fluorescence                : longblob  # Fluorescence trace associated with this mask\n        neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            fluo_traces, fluo_chn2_traces = [], []\n            for s2p in suite2p_dataset.planes.values():\n                mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n                for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                    fluo_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"fluo_channel\": 0,\n                            \"fluorescence\": f,\n                            \"neuropil_fluorescence\": fneu,\n                        }\n                    )\n                if len(s2p.F_chan2):\n                    mask_chn2_count = len(\n                        fluo_chn2_traces\n                    )  # increment mask id from all planes\n                    for mask_idx, (f2, fneu2) in enumerate(\n                        zip(s2p.F_chan2, s2p.Fneu_chan2)\n                    ):\n                        fluo_chn2_traces.append(\n                            {\n                                **key,\n                                \"mask\": mask_idx + mask_chn2_count,\n                                \"fluo_channel\": 1,\n                                \"fluorescence\": f2,\n                                \"neuropil_fluorescence\": fneu2,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces + fluo_chn2_traces)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            fluo_traces = []\n            for mask in caiman_dataset.masks:\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask[\"mask_id\"],\n                        \"fluo_channel\": segmentation_channel,\n                        \"fluorescence\": mask[\"inferred_trace\"],\n                    }\n                )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n\n            fluo_traces = [\n                {\n                    **key,\n                    \"mask\": mask_id,\n                    \"fluo_channel\": 0,\n                    \"fluorescence\": fluorescence,\n                }\n                for mask_id, fluorescence in enumerate(extract_dataset.T)\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Traces obtained from segmented region of interests.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>scan.Channel.proj(fluo_channel='channel')</code> <code>int</code> <p>The channel that this trace comes from.</p> <code>fluorescence</code> <code>longblob</code> <p>Fluorescence trace associated with this mask.</p> <code>neuropil_fluorescence</code> <code>longblob</code> <p>Neuropil fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n            comes from.\n        fluorescence (longblob): Fluorescence trace associated with this mask.\n        neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from         \n    ---\n    fluorescence                : longblob  # Fluorescence trace associated with this mask\n    neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Fluorescence with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        fluo_traces, fluo_chn2_traces = [], []\n        for s2p in suite2p_dataset.planes.values():\n            mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n            for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"fluo_channel\": 0,\n                        \"fluorescence\": f,\n                        \"neuropil_fluorescence\": fneu,\n                    }\n                )\n            if len(s2p.F_chan2):\n                mask_chn2_count = len(\n                    fluo_chn2_traces\n                )  # increment mask id from all planes\n                for mask_idx, (f2, fneu2) in enumerate(\n                    zip(s2p.F_chan2, s2p.Fneu_chan2)\n                ):\n                    fluo_chn2_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_chn2_count,\n                            \"fluo_channel\": 1,\n                            \"fluorescence\": f2,\n                            \"neuropil_fluorescence\": fneu2,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces + fluo_chn2_traces)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        fluo_traces = []\n        for mask in caiman_dataset.masks:\n            fluo_traces.append(\n                {\n                    **key,\n                    \"mask\": mask[\"mask_id\"],\n                    \"fluo_channel\": segmentation_channel,\n                    \"fluorescence\": mask[\"inferred_trace\"],\n                }\n            )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n\n        fluo_traces = [\n            {\n                **key,\n                \"mask\": mask_id,\n                \"fluo_channel\": 0,\n                \"fluorescence\": fluorescence,\n            }\n            for mask_id, fluorescence in enumerate(extract_dataset.T)\n        ]\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassification", "title": "<code>MaskClassification</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Classes assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>MaskClassificationMethod</code> <code>foreign key</code> <p>Primary key from MaskClassificationMethod.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass MaskClassification(dj.Computed):\n\"\"\"Classes assigned to each mask.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        MaskClassificationMethod (foreign key): Primary key from\n            MaskClassificationMethod.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Segmentation\n    -&gt; MaskClassificationMethod\n    \"\"\"\n\n    class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n        Attributes:\n            MaskClassification (foreign key): Primary key from MaskClassification.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            MaskType: Primary key from MaskType.\n            confidence (float, optional): Confidence level of the mask classification.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        -&gt; MaskType\n        confidence=null: float\n        \"\"\"\n\n    def make(self, key):\n        pass\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassification.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Type assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>MaskClassification</code> <code>foreign key</code> <p>Primary key from MaskClassification.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>MaskType</code> <code>foreign key</code> <p>Primary key from MaskType.</p> <code>confidence</code> <code>float</code> <p>Confidence level of the mask classification.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n    Attributes:\n        MaskClassification (foreign key): Primary key from MaskClassification.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        MaskType: Primary key from MaskType.\n        confidence (float, optional): Confidence level of the mask classification.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    -&gt; MaskType\n    confidence=null: float\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassificationMethod", "title": "<code>MaskClassificationMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available mask classification methods.</p> <p>Attributes:</p> Name Type Description <code>mask_classification_method</code> <code>str</code> <p>Mask classification method.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass MaskClassificationMethod(dj.Lookup):\n\"\"\"Available mask classification methods.\n\n    Attributes:\n        mask_classification_method (str): Mask classification method.\n    \"\"\"\n\n    definition = \"\"\"\n    mask_classification_method: varchar(48)\n    \"\"\"\n\n    contents = zip([\"suite2p_default_classifier\", \"caiman_default_classifier\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').</p> <p>Attributes:</p> Name Type Description <code>masky_type</code> <code>str</code> <p>Mask type.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass MaskType(dj.Lookup):\n\"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').\n\n    Attributes:\n        masky_type (str): Mask type.\n    \"\"\"\n\n    definition = \"\"\"# Possible types of a segmented mask\n    mask_type: varchar(16)\n    \"\"\"\n\n    contents = zip([\"soma\", \"axon\", \"dendrite\", \"neuropil\", \"artefact\", \"unknown\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection", "title": "<code>MotionCorrection</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Results of motion correction shifts performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Primary key from Curation.</p> <code>scan.Channel.proj(motion_correct_channel='channel')</code> <code>int</code> <p>Channel used for motion correction in this processing task.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass MotionCorrection(dj.Imported):\n\"\"\"Results of motion correction shifts performed on the imaging data.\n\n    Attributes:\n        Curation (foreign key): Primary key from Curation.\n        scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for\n            motion correction in this processing task.\n    \"\"\"\n\n    definition = \"\"\"# Results of motion correction\n    -&gt; Curation\n    ---\n    -&gt; scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task\n    \"\"\"\n\n    class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob): Mask with true for frames with outlier shifts\n                (already corrected).\n            y_shifts (longblob): y motion correction shifts (pixels).\n            x_shifts (longblob): x motion correction shifts (pixels).\n            z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n            y_std (float): standard deviation of y shifts across all frames (pixels).\n            x_std (float): standard deviation of x shifts across all frames (pixels).\n            z_std (float, optional): standard deviation of z shifts across all frames\n                (pixels).\n        \"\"\"\n\n        definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n        y_shifts            : longblob  # (pixels) y motion correction shifts\n        x_shifts            : longblob  # (pixels) x motion correction shifts\n        z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift) \n        y_std               : float     # (pixels) standard deviation of y shifts across all frames\n        x_std               : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n        blocks/patches.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob, null): Mask with true for frames with outlier\n                shifts (already corrected).\n            block_height (int): Block height in pixels.\n            block_width (int): Block width in pixels.\n            block_depth (int): Block depth in pixels.\n            block_count_y (int): Number of blocks tiled in the y direction.\n            block_count_x (int): Number of blocks tiled in the x direction.\n            block_count_z (int): Number of blocks tiled in the z direction.\n        \"\"\"\n\n        definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n        block_height        : int      # (pixels)\n        block_width         : int      # (pixels)\n        block_depth         : int      # (pixels)\n        block_count_y       : int      # number of blocks tiled in the y direction\n        block_count_x       : int      # number of blocks tiled in the x direction\n        block_count_z       : int      # number of blocks tiled in the z direction\n        \"\"\"\n\n    class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n        Attributes:\n            NonRigidMotionCorrection (foreign key): Primary key from\n                NonRigidMotionCorrection.\n            block_id (int): Unique block ID.\n            block_y (longblob): y_start and y_end in pixels for this block\n            block_x (longblob): x_start and x_end in pixels for this block\n            block_z (longblob): z_start and z_end in pixels for this block\n            y_shifts (longblob): y motion correction shifts for every frame in pixels\n            x_shifts (longblob): x motion correction shifts for every frame in pixels\n            z_shift=null (longblob, optional): x motion correction shifts for every frame\n                in pixels\n            y_std (float): standard deviation of y shifts across all frames in pixels\n            x_std (float): standard deviation of x shifts across all frames in pixels\n            z_std=null (float, optional): standard deviation of z shifts across all frames\n                in pixels\n        \"\"\"\n\n        definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n        -&gt; master.NonRigidMotionCorrection\n        block_id        : int\n        ---\n        block_y         : longblob  # (y_start, y_end) in pixel of this block\n        block_x         : longblob  # (x_start, x_end) in pixel of this block\n        block_z         : longblob  # (z_start, z_end) in pixel of this block\n        y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n        x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n        z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n        y_std           : float     # (pixels) standard deviation of y shifts across all frames\n        x_std           : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n            ref_image (longblob): Image used as alignment template.\n            average_image (longblob): Mean of registered frames.\n            correlation_image (longblob, optional): Correlation map (computed during\n                cell detection).\n            max_proj_image (longblob, optional): Max of registered frames.\n        \"\"\"\n\n        definition = \"\"\"# Summary images for each field and channel after corrections\n        -&gt; master\n        -&gt; scan.ScanInfo.Field\n        ---\n        ref_image               : longblob  # image used as alignment template\n        average_image           : longblob  # mean of registered frames\n        correlation_image=null  : longblob  # correlation map (computed during cell detection)\n        max_proj_image=null     : longblob  # max of registered frames\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n            \"KEY\", \"field_z\", order_by=\"field_z\"\n        )\n\n        if method in [\"suite2p\", \"extract\"]:\n            suite2p_dataset = imaging_dataset\n\n            motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n            # ---- iterate through all s2p plane outputs ----\n            rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n            summary_images = []\n            for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n                # -- rigid motion correction --\n                if idx == 0:\n                    rigid_correction = {\n                        **key,\n                        \"y_shifts\": s2p.ops[\"yoff\"],\n                        \"x_shifts\": s2p.ops[\"xoff\"],\n                        \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                        \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                        \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                        \"z_std\": np.nan,\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    rigid_correction[\"y_shifts\"] = np.vstack(\n                        [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                    )\n                    rigid_correction[\"y_std\"] = np.nanstd(\n                        rigid_correction[\"y_shifts\"].flatten()\n                    )\n                    rigid_correction[\"x_shifts\"] = np.vstack(\n                        [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                    )\n                    rigid_correction[\"x_std\"] = np.nanstd(\n                        rigid_correction[\"x_shifts\"].flatten()\n                    )\n                    rigid_correction[\"outlier_frames\"] = np.logical_or(\n                        rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                    )\n                # -- non-rigid motion correction --\n                if s2p.ops[\"nonrigid\"]:\n                    if idx == 0:\n                        nonrigid_correction = {\n                            **key,\n                            \"block_height\": s2p.ops[\"block_size\"][0],\n                            \"block_width\": s2p.ops[\"block_size\"][1],\n                            \"block_depth\": 1,\n                            \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                            \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                            \"block_count_z\": len(suite2p_dataset.planes),\n                            \"outlier_frames\": s2p.ops[\"badframes\"],\n                        }\n                    else:\n                        nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                            nonrigid_correction[\"outlier_frames\"],\n                            s2p.ops[\"badframes\"],\n                        )\n                    for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                        zip(\n                            s2p.ops[\"xblock\"],\n                            s2p.ops[\"yblock\"],\n                            s2p.ops[\"yoff1\"].T,\n                            s2p.ops[\"xoff1\"].T,\n                        )\n                    ):\n                        if b_id in nonrigid_blocks:\n                            nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                            )\n                            nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                            )\n                            nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                            )\n                            nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                            )\n                        else:\n                            nonrigid_blocks[b_id] = {\n                                **key,\n                                \"block_id\": b_id,\n                                \"block_y\": b_y,\n                                \"block_x\": b_x,\n                                \"block_z\": np.full_like(b_x, plane),\n                                \"y_shifts\": bshift_y,\n                                \"x_shifts\": bshift_x,\n                                \"z_shifts\": np.full(\n                                    (\n                                        len(suite2p_dataset.planes),\n                                        len(bshift_x),\n                                    ),\n                                    0,\n                                ),\n                                \"y_std\": np.nanstd(bshift_y),\n                                \"x_std\": np.nanstd(bshift_x),\n                                \"z_std\": np.nan,\n                            }\n\n                # -- summary images --\n                motion_correction_key = (\n                    scan.ScanInfo.Field * Curation &amp; key &amp; field_keys[plane]\n                ).fetch1(\"KEY\")\n                summary_images.append(\n                    {\n                        **motion_correction_key,\n                        \"ref_image\": s2p.ref_image,\n                        \"average_image\": s2p.mean_image,\n                        \"correlation_image\": s2p.correlation_map,\n                        \"max_proj_image\": s2p.max_proj_image,\n                    }\n                )\n\n            self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n            if rigid_correction:\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            if nonrigid_correction:\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks.values())\n            self.Summary.insert(summary_images)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            self.insert1(\n                {\n                    **key,\n                    \"motion_correct_channel\": caiman_dataset.alignment_channel,\n                }\n            )\n\n            is3D = caiman_dataset.params.motion[\"is3D\"]\n            if not caiman_dataset.params.motion[\"pw_rigid\"]:\n                # -- rigid motion correction --\n                rigid_correction = {\n                    **key,\n                    \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                    \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                    \"z_shifts\": (\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                        if is3D\n                        else np.full_like(\n                            caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                            0,\n                        )\n                    ),\n                    \"x_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                    ),\n                    \"y_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                    ),\n                    \"z_std\": (\n                        np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                        if is3D\n                        else np.nan\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            else:\n                # -- non-rigid motion correction --\n                nonrigid_correction = {\n                    **key,\n                    \"block_height\": (\n                        caiman_dataset.params.motion[\"strides\"][0]\n                        + caiman_dataset.params.motion[\"overlaps\"][0]\n                    ),\n                    \"block_width\": (\n                        caiman_dataset.params.motion[\"strides\"][1]\n                        + caiman_dataset.params.motion[\"overlaps\"][1]\n                    ),\n                    \"block_depth\": (\n                        caiman_dataset.params.motion[\"strides\"][2]\n                        + caiman_dataset.params.motion[\"overlaps\"][2]\n                        if is3D\n                        else 1\n                    ),\n                    \"block_count_x\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                    ),\n                    \"block_count_y\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                    ),\n                    \"block_count_z\": (\n                        len(\n                            set(\n                                caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    :, 4\n                                ]\n                            )\n                        )\n                        if is3D\n                        else 1\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                nonrigid_blocks = []\n                for b_id in range(\n                    len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n                ):\n                    nonrigid_blocks.append(\n                        {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_x\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 0:2\n                                ]\n                            ),\n                            \"block_y\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 2:4\n                                ]\n                            ),\n                            \"block_z\": (\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 4:6]\n                                )\n                                if is3D\n                                else np.full_like(\n                                    np.arange(\n                                        *caiman_dataset.motion_correction[\n                                            \"coord_shifts_els\"\n                                        ][b_id, 0:2]\n                                    ),\n                                    0,\n                                )\n                            ),\n                            \"x_shifts\": caiman_dataset.motion_correction[\n                                \"x_shifts_els\"\n                            ][:, b_id],\n                            \"y_shifts\": caiman_dataset.motion_correction[\n                                \"y_shifts_els\"\n                            ][:, b_id],\n                            \"z_shifts\": (\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                                if is3D\n                                else np.full_like(\n                                    caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                        :, b_id\n                                    ],\n                                    0,\n                                )\n                            ),\n                            \"x_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"y_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"z_std\": (\n                                np.nanstd(\n                                    caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                        :, b_id\n                                    ]\n                                )\n                                if is3D\n                                else np.nan\n                            ),\n                        }\n                    )\n\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks)\n\n            # -- summary images --\n            summary_images = [\n                {\n                    **key,\n                    **fkey,\n                    \"ref_image\": ref_image,\n                    \"average_image\": ave_img,\n                    \"correlation_image\": corr_img,\n                    \"max_proj_image\": max_img,\n                }\n                for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                    field_keys,\n                    caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"reference_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"average_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"max_image\"][...][\n                        np.newaxis, ...\n                    ],\n                )\n            ]\n            self.Summary.insert(summary_images)\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.Block", "title": "<code>Block</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>FOV-tiled blocks used for non-rigid motion correction.</p> <p>Attributes:</p> Name Type Description <code>NonRigidMotionCorrection</code> <code>foreign key</code> <p>Primary key from NonRigidMotionCorrection.</p> <code>block_id</code> <code>int</code> <p>Unique block ID.</p> <code>block_y</code> <code>longblob</code> <p>y_start and y_end in pixels for this block</p> <code>block_x</code> <code>longblob</code> <p>x_start and x_end in pixels for this block</p> <code>block_z</code> <code>longblob</code> <p>z_start and z_end in pixels for this block</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts for every frame in pixels</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>z_shift=null</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames in pixels</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames in pixels</p> <code>z_std=null</code> <code>float</code> <p>standard deviation of z shifts across all frames in pixels</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n    Attributes:\n        NonRigidMotionCorrection (foreign key): Primary key from\n            NonRigidMotionCorrection.\n        block_id (int): Unique block ID.\n        block_y (longblob): y_start and y_end in pixels for this block\n        block_x (longblob): x_start and x_end in pixels for this block\n        block_z (longblob): z_start and z_end in pixels for this block\n        y_shifts (longblob): y motion correction shifts for every frame in pixels\n        x_shifts (longblob): x motion correction shifts for every frame in pixels\n        z_shift=null (longblob, optional): x motion correction shifts for every frame\n            in pixels\n        y_std (float): standard deviation of y shifts across all frames in pixels\n        x_std (float): standard deviation of x shifts across all frames in pixels\n        z_std=null (float, optional): standard deviation of z shifts across all frames\n            in pixels\n    \"\"\"\n\n    definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n    -&gt; master.NonRigidMotionCorrection\n    block_id        : int\n    ---\n    block_y         : longblob  # (y_start, y_end) in pixel of this block\n    block_x         : longblob  # (x_start, x_end) in pixel of this block\n    block_z         : longblob  # (z_start, z_end) in pixel of this block\n    y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n    x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n    z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n    y_std           : float     # (pixels) standard deviation of y shifts across all frames\n    x_std           : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.NonRigidMotionCorrection", "title": "<code>NonRigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob, null</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>block_height</code> <code>int</code> <p>Block height in pixels.</p> <code>block_width</code> <code>int</code> <p>Block width in pixels.</p> <code>block_depth</code> <code>int</code> <p>Block depth in pixels.</p> <code>block_count_y</code> <code>int</code> <p>Number of blocks tiled in the y direction.</p> <code>block_count_x</code> <code>int</code> <p>Number of blocks tiled in the x direction.</p> <code>block_count_z</code> <code>int</code> <p>Number of blocks tiled in the z direction.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n    blocks/patches.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob, null): Mask with true for frames with outlier\n            shifts (already corrected).\n        block_height (int): Block height in pixels.\n        block_width (int): Block width in pixels.\n        block_depth (int): Block depth in pixels.\n        block_count_y (int): Number of blocks tiled in the y direction.\n        block_count_x (int): Number of blocks tiled in the x direction.\n        block_count_z (int): Number of blocks tiled in the z direction.\n    \"\"\"\n\n    definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n    block_height        : int      # (pixels)\n    block_width         : int      # (pixels)\n    block_depth         : int      # (pixels)\n    block_count_y       : int      # number of blocks tiled in the y direction\n    block_count_x       : int      # number of blocks tiled in the x direction\n    block_count_z       : int      # number of blocks tiled in the z direction\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.RigidMotionCorrection", "title": "<code>RigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of rigid motion correction performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts (pixels).</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts (pixels).</p> <code>z_shifts</code> <code>longblob</code> <p>z motion correction shifts (z-drift, pixels).</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames (pixels).</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames (pixels).</p> <code>z_std</code> <code>float</code> <p>standard deviation of z shifts across all frames (pixels).</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob): Mask with true for frames with outlier shifts\n            (already corrected).\n        y_shifts (longblob): y motion correction shifts (pixels).\n        x_shifts (longblob): x motion correction shifts (pixels).\n        z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n        y_std (float): standard deviation of y shifts across all frames (pixels).\n        x_std (float): standard deviation of x shifts across all frames (pixels).\n        z_std (float, optional): standard deviation of z shifts across all frames\n            (pixels).\n    \"\"\"\n\n    definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n    y_shifts            : longblob  # (pixels) y motion correction shifts\n    x_shifts            : longblob  # (pixels) x motion correction shifts\n    z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift) \n    y_std               : float     # (pixels) standard deviation of y shifts across all frames\n    x_std               : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.Summary", "title": "<code>Summary</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Summary images for each field and channel after corrections.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>scan.ScanInfo.Field</code> <code>foreign key</code> <p>Primary key from scan.ScanInfo.Field.</p> <code>ref_image</code> <code>longblob</code> <p>Image used as alignment template.</p> <code>average_image</code> <code>longblob</code> <p>Mean of registered frames.</p> <code>correlation_image</code> <code>longblob</code> <p>Correlation map (computed during cell detection).</p> <code>max_proj_image</code> <code>longblob</code> <p>Max of registered frames.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n        ref_image (longblob): Image used as alignment template.\n        average_image (longblob): Mean of registered frames.\n        correlation_image (longblob, optional): Correlation map (computed during\n            cell detection).\n        max_proj_image (longblob, optional): Max of registered frames.\n    \"\"\"\n\n    definition = \"\"\"# Summary images for each field and channel after corrections\n    -&gt; master\n    -&gt; scan.ScanInfo.Field\n    ---\n    ref_image               : longblob  # image used as alignment template\n    average_image           : longblob  # mean of registered frames\n    correlation_image=null  : longblob  # correlation map (computed during cell detection)\n    max_proj_image=null     : longblob  # max of registered frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.make", "title": "<code>make(key)</code>", "text": "<p>Populate MotionCorrection with results parsed from analysis outputs</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n        \"KEY\", \"field_z\", order_by=\"field_z\"\n    )\n\n    if method in [\"suite2p\", \"extract\"]:\n        suite2p_dataset = imaging_dataset\n\n        motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n        # ---- iterate through all s2p plane outputs ----\n        rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n        summary_images = []\n        for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n            # -- rigid motion correction --\n            if idx == 0:\n                rigid_correction = {\n                    **key,\n                    \"y_shifts\": s2p.ops[\"yoff\"],\n                    \"x_shifts\": s2p.ops[\"xoff\"],\n                    \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                    \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                    \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                    \"z_std\": np.nan,\n                    \"outlier_frames\": s2p.ops[\"badframes\"],\n                }\n            else:\n                rigid_correction[\"y_shifts\"] = np.vstack(\n                    [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                )\n                rigid_correction[\"y_std\"] = np.nanstd(\n                    rigid_correction[\"y_shifts\"].flatten()\n                )\n                rigid_correction[\"x_shifts\"] = np.vstack(\n                    [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                )\n                rigid_correction[\"x_std\"] = np.nanstd(\n                    rigid_correction[\"x_shifts\"].flatten()\n                )\n                rigid_correction[\"outlier_frames\"] = np.logical_or(\n                    rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                )\n            # -- non-rigid motion correction --\n            if s2p.ops[\"nonrigid\"]:\n                if idx == 0:\n                    nonrigid_correction = {\n                        **key,\n                        \"block_height\": s2p.ops[\"block_size\"][0],\n                        \"block_width\": s2p.ops[\"block_size\"][1],\n                        \"block_depth\": 1,\n                        \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                        \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                        \"block_count_z\": len(suite2p_dataset.planes),\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                        nonrigid_correction[\"outlier_frames\"],\n                        s2p.ops[\"badframes\"],\n                    )\n                for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                    zip(\n                        s2p.ops[\"xblock\"],\n                        s2p.ops[\"yblock\"],\n                        s2p.ops[\"yoff1\"].T,\n                        s2p.ops[\"xoff1\"].T,\n                    )\n                ):\n                    if b_id in nonrigid_blocks:\n                        nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                        )\n                        nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                        )\n                        nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                        )\n                        nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                        )\n                    else:\n                        nonrigid_blocks[b_id] = {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_y\": b_y,\n                            \"block_x\": b_x,\n                            \"block_z\": np.full_like(b_x, plane),\n                            \"y_shifts\": bshift_y,\n                            \"x_shifts\": bshift_x,\n                            \"z_shifts\": np.full(\n                                (\n                                    len(suite2p_dataset.planes),\n                                    len(bshift_x),\n                                ),\n                                0,\n                            ),\n                            \"y_std\": np.nanstd(bshift_y),\n                            \"x_std\": np.nanstd(bshift_x),\n                            \"z_std\": np.nan,\n                        }\n\n            # -- summary images --\n            motion_correction_key = (\n                scan.ScanInfo.Field * Curation &amp; key &amp; field_keys[plane]\n            ).fetch1(\"KEY\")\n            summary_images.append(\n                {\n                    **motion_correction_key,\n                    \"ref_image\": s2p.ref_image,\n                    \"average_image\": s2p.mean_image,\n                    \"correlation_image\": s2p.correlation_map,\n                    \"max_proj_image\": s2p.max_proj_image,\n                }\n            )\n\n        self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n        if rigid_correction:\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        if nonrigid_correction:\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks.values())\n        self.Summary.insert(summary_images)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        self.insert1(\n            {\n                **key,\n                \"motion_correct_channel\": caiman_dataset.alignment_channel,\n            }\n        )\n\n        is3D = caiman_dataset.params.motion[\"is3D\"]\n        if not caiman_dataset.params.motion[\"pw_rigid\"]:\n            # -- rigid motion correction --\n            rigid_correction = {\n                **key,\n                \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                \"z_shifts\": (\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                    if is3D\n                    else np.full_like(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                        0,\n                    )\n                ),\n                \"x_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                ),\n                \"y_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                ),\n                \"z_std\": (\n                    np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                    if is3D\n                    else np.nan\n                ),\n                \"outlier_frames\": None,\n            }\n\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        else:\n            # -- non-rigid motion correction --\n            nonrigid_correction = {\n                **key,\n                \"block_height\": (\n                    caiman_dataset.params.motion[\"strides\"][0]\n                    + caiman_dataset.params.motion[\"overlaps\"][0]\n                ),\n                \"block_width\": (\n                    caiman_dataset.params.motion[\"strides\"][1]\n                    + caiman_dataset.params.motion[\"overlaps\"][1]\n                ),\n                \"block_depth\": (\n                    caiman_dataset.params.motion[\"strides\"][2]\n                    + caiman_dataset.params.motion[\"overlaps\"][2]\n                    if is3D\n                    else 1\n                ),\n                \"block_count_x\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                ),\n                \"block_count_y\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                ),\n                \"block_count_z\": (\n                    len(\n                        set(\n                            caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                :, 4\n                            ]\n                        )\n                    )\n                    if is3D\n                    else 1\n                ),\n                \"outlier_frames\": None,\n            }\n\n            nonrigid_blocks = []\n            for b_id in range(\n                len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n            ):\n                nonrigid_blocks.append(\n                    {\n                        **key,\n                        \"block_id\": b_id,\n                        \"block_x\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 0:2\n                            ]\n                        ),\n                        \"block_y\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 2:4\n                            ]\n                        ),\n                        \"block_z\": (\n                            np.arange(\n                                *caiman_dataset.motion_correction[\n                                    \"coord_shifts_els\"\n                                ][b_id, 4:6]\n                            )\n                            if is3D\n                            else np.full_like(\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 0:2]\n                                ),\n                                0,\n                            )\n                        ),\n                        \"x_shifts\": caiman_dataset.motion_correction[\n                            \"x_shifts_els\"\n                        ][:, b_id],\n                        \"y_shifts\": caiman_dataset.motion_correction[\n                            \"y_shifts_els\"\n                        ][:, b_id],\n                        \"z_shifts\": (\n                            caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                :, b_id\n                            ]\n                            if is3D\n                            else np.full_like(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ],\n                                0,\n                            )\n                        ),\n                        \"x_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"y_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"z_std\": (\n                            np.nanstd(\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                            )\n                            if is3D\n                            else np.nan\n                        ),\n                    }\n                )\n\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks)\n\n        # -- summary images --\n        summary_images = [\n            {\n                **key,\n                **fkey,\n                \"ref_image\": ref_image,\n                \"average_image\": ave_img,\n                \"correlation_image\": corr_img,\n                \"max_proj_image\": max_img,\n            }\n            for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                field_keys,\n                caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"reference_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"average_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"max_image\"][...][\n                    np.newaxis, ...\n                ],\n            )\n        ]\n        self.Summary.insert(summary_images)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing", "title": "<code>Processing</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted.</p> <p>Attributes:</p> Name Type Description <code>ProcessingTask</code> <code>foreign key</code> <p>Primary key from ProcessingTask.</p> <code>processing_time</code> <code>datetime</code> <p>Process completion datetime.</p> <code>package_version</code> <code>str</code> <p>Version of the analysis package used in processing the data.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Processing(dj.Computed):\n\"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table.\n    The computation is performed only on the scans with ScanInfo inserted.\n\n    Attributes:\n        ProcessingTask (foreign key): Primary key from ProcessingTask.\n        processing_time (datetime): Process completion datetime.\n        package_version (str, optional): Version of the analysis package used in\n            processing the data.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ProcessingTask\n    ---\n    processing_time     : datetime  # Time of generation of this set of processed, segmented results\n    package_version=''  : varchar(16)\n    \"\"\"\n\n    # Run processing only on Scan with ScanInfo inserted\n    @property\n    def key_source(self):\n\"\"\"Limit the Processing to Scans that have their metadata ingested to the\n        database.\"\"\"\n\n        return ProcessingTask &amp; scan.ScanInfo\n\n    def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n        task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n            \"task_mode\", \"processing_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update processing_output_dir\n            ProcessingTask.update1(\n                {**key, \"processing_output_dir\": output_dir.as_posix()}\n            )\n        output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n        if task_mode == \"load\":\n            method, imaging_dataset = get_loader_result(key, ProcessingTask)\n            if method == \"suite2p\":\n                if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                    raise NotImplementedError(\n                        f\"Suite2p ingestion error - Unable to handle\"\n                        f\" ScanImage multi-ROI scanning mode yet\"\n                    )\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n            elif method == \"caiman\":\n                caiman_dataset = imaging_dataset\n                key = {**key, \"processing_time\": caiman_dataset.creation_time}\n            elif method == \"extract\":\n                raise NotImplementedError(\n                    \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n                )\n            else:\n                raise NotImplementedError(\"Unknown method: {}\".format(method))\n        elif task_mode == \"trigger\":\n\n            method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n                \"processing_method\"\n            )\n\n            image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n            image_files = [\n                find_full_path(get_imaging_root_data_dir(), image_file)\n                for image_file in image_files\n            ]\n\n            if method == \"suite2p\":\n                import suite2p\n\n                suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                suite2p_params[\"save_path0\"] = output_dir\n                (\n                    suite2p_params[\"fs\"],\n                    suite2p_params[\"nplanes\"],\n                    suite2p_params[\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                suite2p_params[\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n            elif method == \"caiman\":\n                from element_interface.run_caiman import run_caiman\n                from element_interface.caiman_loader import (\n                    _process_scanimage_tiff,\n                )\n\n                caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                    \"fps\", \"ndepths\", \"nchannels\"\n                )\n\n                is3D = bool(ndepths &gt; 1)\n                if is3D:\n                    raise NotImplementedError(\n                        \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                    )\n\n                # handle multi-channel tiff image before running CaImAn\n                if nchannels &gt; 1:\n                    channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                    tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                    tmp_dir.mkdir(exist_ok=True)\n                    _process_scanimage_tiff(\n                        [f.as_posix() for f in image_files], output_dir=tmp_dir\n                    )\n                    image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n                run_caiman(\n                    file_paths=[f.as_posix() for f in image_files],\n                    parameters=caiman_params,\n                    sampling_rate=sampling_rate,\n                    output_dir=output_dir,\n                    is3D=is3D,\n                )\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                caiman_dataset = imaging_dataset\n                key[\"processing_time\"] = caiman_dataset.creation_time\n\n            elif method == \"extract\":\n                import suite2p\n                from scipy.io import savemat\n                from element_interface.extract_trigger import EXTRACT_trigger\n\n                # Motion Correction with Suite2p\n                params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n                params[\"suite2p\"][\"save_path0\"] = output_dir\n                (\n                    params[\"suite2p\"][\"fs\"],\n                    params[\"suite2p\"][\"nplanes\"],\n                    params[\"suite2p\"][\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n                # Convert data.bin to registered_scans.mat\n                scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n                data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                    \"nframes\", \"px_height\", \"px_width\"\n                )\n                data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n                scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n                # Save the motion corrected movie (data.bin) in a .mat file\n                savemat(\n                    scan_matlab_fullpath,\n                    {\"M\": np.transpose(data, axes=[1, 2, 0])},\n                )\n\n                # Execute EXTRACT\n\n                ex = EXTRACT_trigger(\n                    scan_matlab_fullpath, params[\"extract\"], output_dir\n                )\n                ex.run()\n\n                _, extract_dataset = get_loader_result(key, ProcessingTask)\n                key[\"processing_time\"] = extract_dataset.creation_time\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the Processing to Scans that have their metadata ingested to the database.</p>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing.make", "title": "<code>make(key)</code>", "text": "<p>Execute the calcium imaging analysis defined by the ProcessingTask.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n    task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n        \"task_mode\", \"processing_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update processing_output_dir\n        ProcessingTask.update1(\n            {**key, \"processing_output_dir\": output_dir.as_posix()}\n        )\n    output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n    if task_mode == \"load\":\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n        if method == \"suite2p\":\n            if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                raise NotImplementedError(\n                    f\"Suite2p ingestion error - Unable to handle\"\n                    f\" ScanImage multi-ROI scanning mode yet\"\n                )\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            key = {**key, \"processing_time\": caiman_dataset.creation_time}\n        elif method == \"extract\":\n            raise NotImplementedError(\n                \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n            )\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n    elif task_mode == \"trigger\":\n\n        method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n            \"processing_method\"\n        )\n\n        image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n        image_files = [\n            find_full_path(get_imaging_root_data_dir(), image_file)\n            for image_file in image_files\n        ]\n\n        if method == \"suite2p\":\n            import suite2p\n\n            suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            suite2p_params[\"save_path0\"] = output_dir\n            (\n                suite2p_params[\"fs\"],\n                suite2p_params[\"nplanes\"],\n                suite2p_params[\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            suite2p_params[\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n        elif method == \"caiman\":\n            from element_interface.run_caiman import run_caiman\n            from element_interface.caiman_loader import (\n                _process_scanimage_tiff,\n            )\n\n            caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                \"fps\", \"ndepths\", \"nchannels\"\n            )\n\n            is3D = bool(ndepths &gt; 1)\n            if is3D:\n                raise NotImplementedError(\n                    \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                )\n\n            # handle multi-channel tiff image before running CaImAn\n            if nchannels &gt; 1:\n                channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                tmp_dir.mkdir(exist_ok=True)\n                _process_scanimage_tiff(\n                    [f.as_posix() for f in image_files], output_dir=tmp_dir\n                )\n                image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n            run_caiman(\n                file_paths=[f.as_posix() for f in image_files],\n                parameters=caiman_params,\n                sampling_rate=sampling_rate,\n                output_dir=output_dir,\n                is3D=is3D,\n            )\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            caiman_dataset = imaging_dataset\n            key[\"processing_time\"] = caiman_dataset.creation_time\n\n        elif method == \"extract\":\n            import suite2p\n            from scipy.io import savemat\n            from element_interface.extract_trigger import EXTRACT_trigger\n\n            # Motion Correction with Suite2p\n            params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n            params[\"suite2p\"][\"save_path0\"] = output_dir\n            (\n                params[\"suite2p\"][\"fs\"],\n                params[\"suite2p\"][\"nplanes\"],\n                params[\"suite2p\"][\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n            # Convert data.bin to registered_scans.mat\n            scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n            data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                \"nframes\", \"px_height\", \"px_width\"\n            )\n            data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n            scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n            # Save the motion corrected movie (data.bin) in a .mat file\n            savemat(\n                scan_matlab_fullpath,\n                {\"M\": np.transpose(data, axes=[1, 2, 0])},\n            )\n\n            # Execute EXTRACT\n\n            ex = EXTRACT_trigger(\n                scan_matlab_fullpath, params[\"extract\"], output_dir\n            )\n            ex.run()\n\n            _, extract_dataset = get_loader_result(key, ProcessingTask)\n            key[\"processing_time\"] = extract_dataset.creation_time\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingMethod", "title": "<code>ProcessingMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Method, package, or analysis suite used for processing of calcium imaging data     (e.g. Suite2p, CaImAn, etc.).</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method.</p> <code>processing_method_desc</code> <code>str</code> <p>Processing method description.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ProcessingMethod(dj.Lookup):\n\"\"\"Method, package, or analysis suite used for processing of calcium imaging data\n        (e.g. Suite2p, CaImAn, etc.).\n\n    Attributes:\n        processing_method (str): Processing method.\n        processing_method_desc (str): Processing method description.\n    \"\"\"\n\n    definition = \"\"\"# Method for calcium imaging processing\n    processing_method: char(8)\n    ---\n    processing_method_desc: varchar(1000)  # Processing method description\n    \"\"\"\n\n    contents = [\n        (\"suite2p\", \"suite2p analysis suite\"),\n        (\"caiman\", \"caiman analysis suite\"),\n        (\"extract\", \"extract analysis suite\"),\n    ]\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingParamSet", "title": "<code>ProcessingParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters.</p> <p>A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Uniqiue parameter set ID.</p> <code>ProcessingMethod</code> <code>foreign key</code> <p>A primary key from ProcessingMethod.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>param_set_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameter Set, a dictionary of all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ProcessingParamSet(dj.Lookup):\n\"\"\"Parameter set used for the processing of the calcium imaging scans,\n    including both the analysis suite and its respective input parameters.\n\n    A hash of the parameters of the analysis suite is also stored in order\n    to avoid duplicated entries.\n\n    Attributes:\n        paramset_idx (int): Uniqiue parameter set ID.\n        ProcessingMethod (foreign key): A primary key from ProcessingMethod.\n        paramset_desc (str): Parameter set description.\n        param_set_hash (uuid): A universally unique identifier for the parameter set.\n        params (longblob): Parameter Set, a dictionary of all applicable parameters to\n            the analysis suite.\n    \"\"\"\n\n    definition = \"\"\"# Processing Parameter Set\n    paramset_idx: smallint  # Uniqiue parameter set ID.\n    ---\n    -&gt; ProcessingMethod\n    paramset_desc: varchar(1280)  # Parameter-set description\n    param_set_hash: uuid  # A universally unique identifier for the parameter set\n    unique index (param_set_hash)\n    params: longblob  # Parameter Set, a dictionary of all applicable parameters to the analysis suite.\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        processing_method: str,\n        paramset_idx: int,\n        paramset_desc: str,\n        params: dict,\n    ):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n        This function automizes the parameter set hashing and avoids insertion of an\n            existing parameter set.\n\n        Attributes:\n            processing_method (str): Processing method/package used for processing of\n                calcium imaging.\n            paramset_idx (int): Uniqiue parameter set ID.\n            paramset_desc (str): Parameter set description.\n            params (dict): Parameter Set, all applicable parameters to the analysis\n                suite.\n        \"\"\"\n        if processing_method == \"extract\":\n            assert (\n                params.get(\"extract\") != None and params.get(\"suite2p\") != None\n            ), ValueError(\n                \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n            )\n\n            # Force Suite2p to only run motion correction.\n            params[\"suite2p\"][\"do_registration\"] = True\n            params[\"suite2p\"][\"roidetect\"] = False\n\n        param_dict = {\n            \"processing_method\": processing_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if q_param:  # If the specified param-set already exists\n            pname = q_param.fetch1(\"paramset_idx\")\n            if pname == paramset_idx:  # If the existed set has the same name: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set already exists - name: {}\".format(pname)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingParamSet.insert_new_params", "title": "<code>insert_new_params(processing_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Insert a parameter set into ProcessingParamSet table.</p> <p>This function automizes the parameter set hashing and avoids insertion of an     existing parameter set.</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method/package used for processing of calcium imaging.</p> <code>paramset_idx</code> <code>int</code> <p>Uniqiue parameter set ID.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>params</code> <code>dict</code> <p>Parameter Set, all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    processing_method: str,\n    paramset_idx: int,\n    paramset_desc: str,\n    params: dict,\n):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n    This function automizes the parameter set hashing and avoids insertion of an\n        existing parameter set.\n\n    Attributes:\n        processing_method (str): Processing method/package used for processing of\n            calcium imaging.\n        paramset_idx (int): Uniqiue parameter set ID.\n        paramset_desc (str): Parameter set description.\n        params (dict): Parameter Set, all applicable parameters to the analysis\n            suite.\n    \"\"\"\n    if processing_method == \"extract\":\n        assert (\n            params.get(\"extract\") != None and params.get(\"suite2p\") != None\n        ), ValueError(\n            \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n        )\n\n        # Force Suite2p to only run motion correction.\n        params[\"suite2p\"][\"do_registration\"] = True\n        params[\"suite2p\"][\"roidetect\"] = False\n\n    param_dict = {\n        \"processing_method\": processing_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if q_param:  # If the specified param-set already exists\n        pname = q_param.fetch1(\"paramset_idx\")\n        if pname == paramset_idx:  # If the existed set has the same name: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set already exists - name: {}\".format(pname)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask", "title": "<code>ProcessingTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A pairing of processing params and scans to be loaded or triggered</p> <p>This table defines a calcium imaging processing task for a combination of a <code>Scan</code> and a <code>ProcessingParamSet</code> entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods</p> <p>Attributes:</p> Name Type Description <code>scan.Scan</code> <code>foreign key</code> <code>ProcessingParamSet</code> <code>foreign key</code> <code>processing_output_dir</code> <code>str</code> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results) or 'trigger' (trigger computation).</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ProcessingTask(dj.Manual):\n\"\"\"A pairing of processing params and scans to be loaded or triggered\n\n    This table defines a calcium imaging processing task for a combination of a\n    `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method,\n    method's parameters). The task defined here is then run in the downstream table\n    Processing. This table supports definitions of both loading of pre-generated results\n    and the triggering of new analysis for all supported analysis methods\n\n    Attributes:\n        scan.Scan (foreign key):\n        ProcessingParamSet (foreign key):\n        processing_output_dir (str):\n        task_mode (str): One of 'load' (load computed analysis results) or 'trigger'\n            (trigger computation).\n    \"\"\"\n\n    definition = \"\"\"# Manual table for defining a processing task ready to be run\n    -&gt; scan.Scan\n    -&gt; ProcessingParamSet\n    ---\n    processing_output_dir: varchar(255)  #  Output directory of the processed scan relative to root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n        Args:\n            key (dict): Primary key from the ProcessingTask table.\n            relative (bool): If True, processing_output_dir is returned relative to\n                imaging_root_dir. Default False.\n            mkdir (bool): If True, create the processing_output_dir directory.\n                Default True.\n\n        Returns:\n            dir (str): A default output directory for the processed results (processed_output_dir\n                in ProcessingTask) based on the following convention:\n                processed_dir / scan_dir / {processing_method}_{paramset_idx}\n                e.g.: sub4/sess1/scan0/suite2p_0\n        \"\"\"\n        image_locators = {\n            \"NIS\": get_nd2_files,\n            \"ScanImage\": get_scan_image_files,\n            \"Scanbox\": get_scan_box_files,\n            \"PrairieView\": get_prairieview_files,\n        }\n        image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n        scan_dir = find_full_path(\n            get_imaging_root_data_dir(), image_locator(key)[0]\n        ).parent\n        root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n        method = (\n            (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n        )\n\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        output_dir = (\n            processed_dir\n            / scan_dir.relative_to(root_dir)\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n        Generate an entry in the ProcessingTask table for a particular scan using an\n        existing parameter set from the ProcessingParamSet table.\n\n        Args:\n            scan_key (dict): Primary key from Scan table.\n            paramset_idx (int): Unique parameter set ID.\n        \"\"\"\n        key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n        method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n            \"processing_method\"\n        )\n\n        try:\n            if method == \"suite2p\":\n                from element_interface import suite2p_loader\n\n                suite2p_loader.Suite2p(output_dir)\n            elif method == \"caiman\":\n                from element_interface import caiman_loader\n\n                caiman_loader.CaImAn(output_dir)\n            elif method == \"extract\":\n                from element_interface import extract_loader\n\n                extract_loader.EXTRACT(output_dir)\n\n            else:\n                raise NotImplementedError(\n                    \"Unknown/unimplemented method: {}\".format(method)\n                )\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"processing_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n\n    auto_generate_entries = generate\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask.generate", "title": "<code>generate(scan_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet</p> <p>Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan table.</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>0</code> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@classmethod\ndef generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n    Generate an entry in the ProcessingTask table for a particular scan using an\n    existing parameter set from the ProcessingParamSet table.\n\n    Args:\n        scan_key (dict): Primary key from Scan table.\n        paramset_idx (int): Unique parameter set ID.\n    \"\"\"\n    key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n    method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n        \"processing_method\"\n    )\n\n    try:\n        if method == \"suite2p\":\n            from element_interface import suite2p_loader\n\n            suite2p_loader.Suite2p(output_dir)\n        elif method == \"caiman\":\n            from element_interface import caiman_loader\n\n            caiman_loader.CaImAn(output_dir)\n        elif method == \"extract\":\n            from element_interface import extract_loader\n\n            extract_loader.EXTRACT(output_dir)\n\n        else:\n            raise NotImplementedError(\n                \"Unknown/unimplemented method: {}\".format(method)\n            )\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"processing_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer an output directory for an entry in ProcessingTask table.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from the ProcessingTask table.</p> required <code>relative</code> <code>bool</code> <p>If True, processing_output_dir is returned relative to imaging_root_dir. Default False.</p> <code>False</code> <code>mkdir</code> <code>bool</code> <p>If True, create the processing_output_dir directory. Default True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dir</code> <code>str</code> <p>A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n    Args:\n        key (dict): Primary key from the ProcessingTask table.\n        relative (bool): If True, processing_output_dir is returned relative to\n            imaging_root_dir. Default False.\n        mkdir (bool): If True, create the processing_output_dir directory.\n            Default True.\n\n    Returns:\n        dir (str): A default output directory for the processed results (processed_output_dir\n            in ProcessingTask) based on the following convention:\n            processed_dir / scan_dir / {processing_method}_{paramset_idx}\n            e.g.: sub4/sess1/scan0/suite2p_0\n    \"\"\"\n    image_locators = {\n        \"NIS\": get_nd2_files,\n        \"ScanImage\": get_scan_image_files,\n        \"Scanbox\": get_scan_box_files,\n        \"PrairieView\": get_prairieview_files,\n    }\n    image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n    scan_dir = find_full_path(\n        get_imaging_root_data_dir(), image_locator(key)[0]\n    ).parent\n    root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n    method = (\n        (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n    )\n\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    output_dir = (\n        processed_dir\n        / scan_dir.relative_to(root_dir)\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation", "title": "<code>Segmentation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Result of the Segmentation process.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Primary key from Curation.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Segmentation(dj.Computed):\n\"\"\"Result of the Segmentation process.\n\n    Attributes:\n        Curation (foreign key): Primary key from Curation.\n    \"\"\"\n\n    definition = \"\"\"# Different mask segmentations.\n    -&gt; Curation\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n        Attributes:\n            Segmentation (foreign key): Primary key from Segmentation.\n            mask (int): Unique mask ID.\n            scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n                used for segmentation.\n            mask_npix (int): Number of pixels in ROIs.\n            mask_center_x (int): Center x coordinate in pixel.\n            mask_center_y (int): Center y coordinate in pixel.\n            mask_center_z (int): Center z coordinate in pixel.\n            mask_xpix (longblob): X coordinates in pixels.\n            mask_ypix (longblob): Y coordinates in pixels.\n            mask_zpix (longblob): Z coordinates in pixels.\n            mask_weights (longblob): Weights of the mask at the indices above.\n        \"\"\"\n\n        definition = \"\"\" # A mask produced by segmentation.\n        -&gt; master\n        mask               : smallint\n        ---\n        -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n        mask_npix          : int       # number of pixels in ROIs\n        mask_center_x      : int       # center x coordinate in pixel\n        mask_center_y      : int       # center y coordinate in pixel\n        mask_center_z=null : int       # center z coordinate in pixel\n        mask_xpix          : longblob  # x coordinates in pixels\n        mask_ypix          : longblob  # y coordinates in pixels      \n        mask_zpix=null     : longblob  # z coordinates in pixels        \n        mask_weights       : longblob  # weights of the mask at the indices above\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            masks, cells = [], []\n            for plane, s2p in suite2p_dataset.planes.items():\n                mask_count = len(masks)  # increment mask id from all \"plane\"\n                for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                    zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n                ):\n                    masks.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"segmentation_channel\": s2p.segmentation_channel,\n                            \"mask_npix\": mask_stat[\"npix\"],\n                            \"mask_center_x\": mask_stat[\"med\"][1],\n                            \"mask_center_y\": mask_stat[\"med\"][0],\n                            \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                            \"mask_xpix\": mask_stat[\"xpix\"],\n                            \"mask_ypix\": mask_stat[\"ypix\"],\n                            \"mask_zpix\": np.full(\n                                mask_stat[\"npix\"],\n                                mask_stat.get(\"iplane\", plane),\n                            ),\n                            \"mask_weights\": mask_stat[\"lam\"],\n                        }\n                    )\n                    if is_cell:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"suite2p_default_classifier\",\n                                \"mask\": mask_idx + mask_count,\n                                \"mask_type\": \"soma\",\n                                \"confidence\": cell_prob,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"suite2p_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            masks, cells = [], []\n            for mask in caiman_dataset.masks:\n                masks.append(\n                    {\n                        **key,\n                        \"segmentation_channel\": segmentation_channel,\n                        \"mask\": mask[\"mask_id\"],\n                        \"mask_npix\": mask[\"mask_npix\"],\n                        \"mask_center_x\": mask[\"mask_center_x\"],\n                        \"mask_center_y\": mask[\"mask_center_y\"],\n                        \"mask_center_z\": mask[\"mask_center_z\"],\n                        \"mask_xpix\": mask[\"mask_xpix\"],\n                        \"mask_ypix\": mask[\"mask_ypix\"],\n                        \"mask_zpix\": mask[\"mask_zpix\"],\n                        \"mask_weights\": mask[\"mask_weights\"],\n                    }\n                )\n                if caiman_dataset.cnmf.estimates.idx_components is not None:\n                    if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"caiman_default_classifier\",\n                                \"mask\": mask[\"mask_id\"],\n                                \"mask_type\": \"soma\",\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"caiman_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            masks = [\n                dict(\n                    **key,\n                    segmentation_channel=0,\n                    mask=mask[\"mask_id\"],\n                    mask_npix=mask[\"mask_npix\"],\n                    mask_center_x=mask[\"mask_center_x\"],\n                    mask_center_y=mask[\"mask_center_y\"],\n                    mask_center_z=mask[\"mask_center_z\"],\n                    mask_xpix=mask[\"mask_xpix\"],\n                    mask_ypix=mask[\"mask_ypix\"],\n                    mask_zpix=mask[\"mask_zpix\"],\n                    mask_weights=mask[\"mask_weights\"],\n                )\n                for mask in extract_dataset.load_results()\n            ]\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n        else:\n            raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of the masks identified from the Segmentation procedure.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>mask</code> <code>int</code> <p>Unique mask ID.</p> <code>scan.Channel.proj(segmentation_channel='channel')</code> <code>foreign key</code> <p>Channel used for segmentation.</p> <code>mask_npix</code> <code>int</code> <p>Number of pixels in ROIs.</p> <code>mask_center_x</code> <code>int</code> <p>Center x coordinate in pixel.</p> <code>mask_center_y</code> <code>int</code> <p>Center y coordinate in pixel.</p> <code>mask_center_z</code> <code>int</code> <p>Center z coordinate in pixel.</p> <code>mask_xpix</code> <code>longblob</code> <p>X coordinates in pixels.</p> <code>mask_ypix</code> <code>longblob</code> <p>Y coordinates in pixels.</p> <code>mask_zpix</code> <code>longblob</code> <p>Z coordinates in pixels.</p> <code>mask_weights</code> <code>longblob</code> <p>Weights of the mask at the indices above.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        mask (int): Unique mask ID.\n        scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n            used for segmentation.\n        mask_npix (int): Number of pixels in ROIs.\n        mask_center_x (int): Center x coordinate in pixel.\n        mask_center_y (int): Center y coordinate in pixel.\n        mask_center_z (int): Center z coordinate in pixel.\n        mask_xpix (longblob): X coordinates in pixels.\n        mask_ypix (longblob): Y coordinates in pixels.\n        mask_zpix (longblob): Z coordinates in pixels.\n        mask_weights (longblob): Weights of the mask at the indices above.\n    \"\"\"\n\n    definition = \"\"\" # A mask produced by segmentation.\n    -&gt; master\n    mask               : smallint\n    ---\n    -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n    mask_npix          : int       # number of pixels in ROIs\n    mask_center_x      : int       # center x coordinate in pixel\n    mask_center_y      : int       # center y coordinate in pixel\n    mask_center_z=null : int       # center z coordinate in pixel\n    mask_xpix          : longblob  # x coordinates in pixels\n    mask_ypix          : longblob  # y coordinates in pixels      \n    mask_zpix=null     : longblob  # z coordinates in pixels        \n    mask_weights       : longblob  # weights of the mask at the indices above\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Segmentation with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        masks, cells = [], []\n        for plane, s2p in suite2p_dataset.planes.items():\n            mask_count = len(masks)  # increment mask id from all \"plane\"\n            for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n            ):\n                masks.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"segmentation_channel\": s2p.segmentation_channel,\n                        \"mask_npix\": mask_stat[\"npix\"],\n                        \"mask_center_x\": mask_stat[\"med\"][1],\n                        \"mask_center_y\": mask_stat[\"med\"][0],\n                        \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                        \"mask_xpix\": mask_stat[\"xpix\"],\n                        \"mask_ypix\": mask_stat[\"ypix\"],\n                        \"mask_zpix\": np.full(\n                            mask_stat[\"npix\"],\n                            mask_stat.get(\"iplane\", plane),\n                        ),\n                        \"mask_weights\": mask_stat[\"lam\"],\n                    }\n                )\n                if is_cell:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"suite2p_default_classifier\",\n                            \"mask\": mask_idx + mask_count,\n                            \"mask_type\": \"soma\",\n                            \"confidence\": cell_prob,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"suite2p_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        masks, cells = [], []\n        for mask in caiman_dataset.masks:\n            masks.append(\n                {\n                    **key,\n                    \"segmentation_channel\": segmentation_channel,\n                    \"mask\": mask[\"mask_id\"],\n                    \"mask_npix\": mask[\"mask_npix\"],\n                    \"mask_center_x\": mask[\"mask_center_x\"],\n                    \"mask_center_y\": mask[\"mask_center_y\"],\n                    \"mask_center_z\": mask[\"mask_center_z\"],\n                    \"mask_xpix\": mask[\"mask_xpix\"],\n                    \"mask_ypix\": mask[\"mask_ypix\"],\n                    \"mask_zpix\": mask[\"mask_zpix\"],\n                    \"mask_weights\": mask[\"mask_weights\"],\n                }\n            )\n            if caiman_dataset.cnmf.estimates.idx_components is not None:\n                if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"caiman_default_classifier\",\n                            \"mask\": mask[\"mask_id\"],\n                            \"mask_type\": \"soma\",\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"caiman_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        masks = [\n            dict(\n                **key,\n                segmentation_channel=0,\n                mask=mask[\"mask_id\"],\n                mask_npix=mask[\"mask_npix\"],\n                mask_center_x=mask[\"mask_center_x\"],\n                mask_center_y=mask[\"mask_center_y\"],\n                mask_center_z=mask[\"mask_center_z\"],\n                mask_xpix=mask[\"mask_xpix\"],\n                mask_ypix=mask[\"mask_ypix\"],\n                mask_zpix=mask[\"mask_zpix\"],\n                mask_weights=mask[\"mask_weights\"],\n            )\n            for mask in extract_dataset.load_results()\n        ]\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n    else:\n        raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.activate", "title": "<code>activate(imaging_schema_name, scan_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>imaging_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>imaging</code> module.</p> required <code>scan_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>scan</code> module. Omitted, if the <code>scan</code> module is already activated.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>imaging</code> module: + all that are required by the <code>scan</code> module.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <ul> <li>Session: A parent table to Scan, identifying a scanning session.</li> <li>Equipment: A parent table to Scan, identifying a scanning device.</li> </ul> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def activate(\n    imaging_schema_name,\n    scan_schema_name=None,\n    *,\n    create_schema=True,\n    create_tables=True,\n    linking_module=None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        imaging_schema_name (str): Schema name on the database server to activate the\n            `imaging` module.\n        scan_schema_name (str): Schema name on the database server to activate the\n            `scan` module. Omitted, if the `scan` module is already activated.\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `imaging` module: + all that are required by\n            the `scan` module.\n\n    Dependencies:\n    Upstream tables:\n        + Session: A parent table to Scan, identifying a scanning session.\n        + Equipment: A parent table to Scan, identifying a scanning device.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    scan.activate(\n        scan_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        linking_module=linking_module,\n    )\n    schema.activate(\n        imaging_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    imaging_report.activate(f\"{imaging_schema_name}_report\", imaging_schema_name)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_loader_result", "title": "<code>get_loader_result(key, table)</code>", "text": "<p>Retrieve the processed imaging results from a suite2p or caiman loader.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>The <code>key</code> to one entry of ProcessingTask or Curation</p> required <code>table</code> <code>dj.Table</code> <p>A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation)</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the processing_method is different than 'suite2p' or 'caiman'.</p> <p>Returns:</p> Type Description <p>A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,</p> <p>see element-interface for more information on the loaders.)</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def get_loader_result(key: dict, table: dj.Table):\n\"\"\"Retrieve the processed imaging results from a suite2p or caiman loader.\n\n    Args:\n        key (dict): The `key` to one entry of ProcessingTask or Curation\n        table (dj.Table): A datajoint table to retrieve the loaded results from (e.g.\n            ProcessingTask, Curation)\n\n    Raises:\n        NotImplementedError: If the processing_method is different than 'suite2p' or\n            'caiman'.\n\n    Returns:\n        A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,\n        see element-interface for more information on the loaders.)\n    \"\"\"\n    method, output_dir = (ProcessingParamSet * table &amp; key).fetch1(\n        \"processing_method\", _table_attribute_mapper[table.__name__]\n    )\n\n    output_path = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n    if method == \"suite2p\" or (\n        method == \"extract\" and table.__name__ == \"MotionCorrection\"\n    ):\n        from element_interface import suite2p_loader\n\n        loaded_dataset = suite2p_loader.Suite2p(output_path)\n    elif method == \"caiman\":\n        from element_interface import caiman_loader\n\n        loaded_dataset = caiman_loader.CaImAn(output_path)\n    elif method == \"extract\":\n        from element_interface import extract_loader\n\n        loaded_dataset = extract_loader.EXTRACT(output_path)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n\n    return method, loaded_dataset\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/", "title": "imaging_no_curation.py", "text": ""}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity", "title": "<code>Activity</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>ActivityExtractionMethod</code> <code>foreign key</code> <p>Primary key from ActivityExtractionMethod.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass Activity(dj.Computed):\n\"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        ActivityExtractionMethod (foreign key): Primary key from\n            ActivityExtractionMethod.\n    \"\"\"\n\n    definition = \"\"\"# Neural Activity\n    -&gt; Fluorescence\n    -&gt; ActivityExtractionMethod\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n        Attributes:\n            Activity (foreign key): Primary key from Activity.\n            Fluorescence.Trace (foreign key): Fluorescence.Trace.\n            activity_trace (longblob): Neural activity from fluoresence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        activity_trace: longblob\n        \"\"\"\n\n    @property\n    def key_source(self):\n        suite2p_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"suite2p\"'\n            &amp; 'extraction_method LIKE \"suite2p%\"'\n        )\n        caiman_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"caiman\"'\n            &amp; 'extraction_method LIKE \"caiman%\"'\n        )\n        return suite2p_key_source.proj() + caiman_key_source.proj()\n\n    def make(self, key):\n\"\"\"\n        Populate the Activity with the results parsed from analysis outputs.\n        \"\"\"\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n                suite2p_dataset = imaging_dataset\n                # ---- iterate through all s2p plane outputs ----\n                spikes = [\n                    dict(\n                        key,\n                        mask=mask_idx,\n                        fluo_channel=0,\n                        activity_trace=spks,\n                    )\n                    for mask_idx, spks in enumerate(\n                        s\n                        for plane in suite2p_dataset.planes.values()\n                        for s in plane.spks\n                    )\n                ]\n\n                self.insert1(key)\n                self.Trace.insert(spikes)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            if key[\"extraction_method\"] in (\n                \"caiman_deconvolution\",\n                \"caiman_dff\",\n            ):\n                attr_mapper = {\n                    \"caiman_deconvolution\": \"spikes\",\n                    \"caiman_dff\": \"dff\",\n                }\n\n                # infer \"segmentation_channel\" - from params if available, else from caiman loader\n                params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n                segmentation_channel = params.get(\n                    \"segmentation_channel\", caiman_dataset.segmentation_channel\n                )\n\n                self.insert1(key)\n                self.Trace.insert(\n                    dict(\n                        key,\n                        mask=mask[\"mask_id\"],\n                        fluo_channel=segmentation_channel,\n                        activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                    )\n                    for mask in caiman_dataset.masks\n                )\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trace(s) for each mask.</p> <p>Attributes:</p> Name Type Description <code>Activity</code> <code>foreign key</code> <p>Primary key from Activity.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Fluorescence.Trace.</p> <code>activity_trace</code> <code>longblob</code> <p>Neural activity from fluoresence trace.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n    Attributes:\n        Activity (foreign key): Primary key from Activity.\n        Fluorescence.Trace (foreign key): Fluorescence.Trace.\n        activity_trace (longblob): Neural activity from fluoresence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    activity_trace: longblob\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Activity with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"\n    Populate the Activity with the results parsed from analysis outputs.\n    \"\"\"\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n            suite2p_dataset = imaging_dataset\n            # ---- iterate through all s2p plane outputs ----\n            spikes = [\n                dict(\n                    key,\n                    mask=mask_idx,\n                    fluo_channel=0,\n                    activity_trace=spks,\n                )\n                for mask_idx, spks in enumerate(\n                    s\n                    for plane in suite2p_dataset.planes.values()\n                    for s in plane.spks\n                )\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(spikes)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        if key[\"extraction_method\"] in (\n            \"caiman_deconvolution\",\n            \"caiman_dff\",\n        ):\n            attr_mapper = {\n                \"caiman_deconvolution\": \"spikes\",\n                \"caiman_dff\": \"dff\",\n            }\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            self.insert1(key)\n            self.Trace.insert(\n                dict(\n                    key,\n                    mask=mask[\"mask_id\"],\n                    fluo_channel=segmentation_channel,\n                    activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                )\n                for mask in caiman_dataset.masks\n            )\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ActivityExtractionMethod", "title": "<code>ActivityExtractionMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available activity extraction methods.</p> <p>Attributes:</p> Name Type Description <code>extraction_method</code> <code>str</code> <p>Extraction method.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ActivityExtractionMethod(dj.Lookup):\n\"\"\"Available activity extraction methods.\n\n    Attributes:\n        extraction_method (str): Extraction method.\n    \"\"\"\n\n    definition = \"\"\"# Activity extraction method \n    extraction_method: varchar(32)\n    \"\"\"\n\n    contents = zip([\"suite2p_deconvolution\", \"caiman_deconvolution\", \"caiman_dff\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.CellCompartment", "title": "<code>CellCompartment</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)</p> <p>Attributes:</p> Name Type Description <code>cell_compartment</code> <code>str</code> <p>Cell compartment.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass CellCompartment(dj.Lookup):\n\"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)\n\n    Attributes:\n        cell_compartment (str): Cell compartment.\n    \"\"\"\n\n    definition = \"\"\"# Cell compartments\n    cell_compartment: char(16)\n    \"\"\"\n\n    contents = zip([\"axon\", \"soma\", \"bouton\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence", "title": "<code>Fluorescence</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass Fluorescence(dj.Computed):\n\"\"\"Fluorescence traces.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n    \"\"\"\n\n    definition = \"\"\"# Fluorescence traces before spike extraction or filtering\n    -&gt; Segmentation\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n                comes from.\n            fluorescence (longblob): Fluorescence trace associated with this mask.\n            neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from\n        ---\n        fluorescence                : longblob  # Fluorescence trace associated with this mask\n        neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            fluo_traces, fluo_chn2_traces = [], []\n            for s2p in suite2p_dataset.planes.values():\n                mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n                for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                    fluo_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"fluo_channel\": 0,\n                            \"fluorescence\": f,\n                            \"neuropil_fluorescence\": fneu,\n                        }\n                    )\n                if len(s2p.F_chan2):\n                    mask_chn2_count = len(\n                        fluo_chn2_traces\n                    )  # increment mask id from all planes\n                    for mask_idx, (f2, fneu2) in enumerate(\n                        zip(s2p.F_chan2, s2p.Fneu_chan2)\n                    ):\n                        fluo_chn2_traces.append(\n                            {\n                                **key,\n                                \"mask\": mask_idx + mask_chn2_count,\n                                \"fluo_channel\": 1,\n                                \"fluorescence\": f2,\n                                \"neuropil_fluorescence\": fneu2,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces + fluo_chn2_traces)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            fluo_traces = []\n            for mask in caiman_dataset.masks:\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask[\"mask_id\"],\n                        \"fluo_channel\": segmentation_channel,\n                        \"fluorescence\": mask[\"inferred_trace\"],\n                    }\n                )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n\n            fluo_traces = [\n                {\n                    **key,\n                    \"mask\": mask_id,\n                    \"fluo_channel\": 0,\n                    \"fluorescence\": fluorescence,\n                }\n                for mask_id, fluorescence in enumerate(extract_dataset.T)\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Traces obtained from segmented region of interests.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>scan.Channel.proj(fluo_channel='channel')</code> <code>int</code> <p>The channel that this trace comes from.</p> <code>fluorescence</code> <code>longblob</code> <p>Fluorescence trace associated with this mask.</p> <code>neuropil_fluorescence</code> <code>longblob</code> <p>Neuropil fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n            comes from.\n        fluorescence (longblob): Fluorescence trace associated with this mask.\n        neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from\n    ---\n    fluorescence                : longblob  # Fluorescence trace associated with this mask\n    neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Fluorescence with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        fluo_traces, fluo_chn2_traces = [], []\n        for s2p in suite2p_dataset.planes.values():\n            mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n            for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"fluo_channel\": 0,\n                        \"fluorescence\": f,\n                        \"neuropil_fluorescence\": fneu,\n                    }\n                )\n            if len(s2p.F_chan2):\n                mask_chn2_count = len(\n                    fluo_chn2_traces\n                )  # increment mask id from all planes\n                for mask_idx, (f2, fneu2) in enumerate(\n                    zip(s2p.F_chan2, s2p.Fneu_chan2)\n                ):\n                    fluo_chn2_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_chn2_count,\n                            \"fluo_channel\": 1,\n                            \"fluorescence\": f2,\n                            \"neuropil_fluorescence\": fneu2,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces + fluo_chn2_traces)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        fluo_traces = []\n        for mask in caiman_dataset.masks:\n            fluo_traces.append(\n                {\n                    **key,\n                    \"mask\": mask[\"mask_id\"],\n                    \"fluo_channel\": segmentation_channel,\n                    \"fluorescence\": mask[\"inferred_trace\"],\n                }\n            )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n\n        fluo_traces = [\n            {\n                **key,\n                \"mask\": mask_id,\n                \"fluo_channel\": 0,\n                \"fluorescence\": fluorescence,\n            }\n            for mask_id, fluorescence in enumerate(extract_dataset.T)\n        ]\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassification", "title": "<code>MaskClassification</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Classes assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>MaskClassificationMethod</code> <code>foreign key</code> <p>Primary key from MaskClassificationMethod.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass MaskClassification(dj.Computed):\n\"\"\"Classes assigned to each mask.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        MaskClassificationMethod (foreign key): Primary key from\n            MaskClassificationMethod.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Segmentation\n    -&gt; MaskClassificationMethod\n    \"\"\"\n\n    class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n        Attributes:\n            MaskClassification (foreign key): Primary key from MaskClassification.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            MaskType: Primary key from MaskType.\n            confidence (float, optional): Confidence level of the mask classification.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        -&gt; MaskType\n        confidence=null: float\n        \"\"\"\n\n    def make(self, key):\n        pass\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassification.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Type assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>MaskClassification</code> <code>foreign key</code> <p>Primary key from MaskClassification.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>MaskType</code> <code>foreign key</code> <p>Primary key from MaskType.</p> <code>confidence</code> <code>float</code> <p>Confidence level of the mask classification.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n    Attributes:\n        MaskClassification (foreign key): Primary key from MaskClassification.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        MaskType: Primary key from MaskType.\n        confidence (float, optional): Confidence level of the mask classification.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    -&gt; MaskType\n    confidence=null: float\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassificationMethod", "title": "<code>MaskClassificationMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available mask classification methods.</p> <p>Attributes:</p> Name Type Description <code>mask_classification_method</code> <code>str</code> <p>Mask classification method.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass MaskClassificationMethod(dj.Lookup):\n\"\"\"Available mask classification methods.\n\n    Attributes:\n        mask_classification_method (str): Mask classification method.\n    \"\"\"\n\n    definition = \"\"\"\n    mask_classification_method: varchar(48)\n    \"\"\"\n\n    contents = zip([\"suite2p_default_classifier\", \"caiman_default_classifier\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').</p> <p>Attributes:</p> Name Type Description <code>masky_type</code> <code>str</code> <p>Mask type.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass MaskType(dj.Lookup):\n\"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').\n\n    Attributes:\n        masky_type (str): Mask type.\n    \"\"\"\n\n    definition = \"\"\"# Possible types of a segmented mask\n    mask_type: varchar(16)\n    \"\"\"\n\n    contents = zip([\"soma\", \"axon\", \"dendrite\", \"neuropil\", \"artefact\", \"unknown\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection", "title": "<code>MotionCorrection</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Results of motion correction shifts performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>Processing</code> <code>foreign key</code> <p>Primary key from Processing.</p> <code>scan.Channel.proj(motion_correct_channel='channel')</code> <code>int</code> <p>Channel used for motion correction in this processing task.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass MotionCorrection(dj.Imported):\n\"\"\"Results of motion correction shifts performed on the imaging data.\n\n    Attributes:\n        Processing (foreign key): Primary key from Processing.\n        scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for\n            motion correction in this processing task.\n    \"\"\"\n\n    definition = \"\"\"# Results of motion correction\n    -&gt; Processing\n    ---\n    -&gt; scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task\n    \"\"\"\n\n    class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob): Mask with true for frames with outlier shifts\n                (already corrected).\n            y_shifts (longblob): y motion correction shifts (pixels).\n            x_shifts (longblob): x motion correction shifts (pixels).\n            z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n            y_std (float): standard deviation of y shifts across all frames (pixels).\n            x_std (float): standard deviation of x shifts across all frames (pixels).\n            z_std (float, optional): standard deviation of z shifts across all frames\n                (pixels).\n        \"\"\"\n\n        definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n        y_shifts            : longblob  # (pixels) y motion correction shifts\n        x_shifts            : longblob  # (pixels) x motion correction shifts\n        z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift) \n        y_std               : float     # (pixels) standard deviation of y shifts across all frames\n        x_std               : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n        blocks/patches.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob, null): Mask with true for frames with outlier\n                shifts (already corrected).\n            block_height (int): Block height in pixels.\n            block_width (int): Block width in pixels.\n            block_depth (int): Block depth in pixels.\n            block_count_y (int): Number of blocks tiled in the y direction.\n            block_count_x (int): Number of blocks tiled in the x direction.\n            block_count_z (int): Number of blocks tiled in the z direction.\n        \"\"\"\n\n        definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n        block_height        : int      # (pixels)\n        block_width         : int      # (pixels)\n        block_depth         : int      # (pixels)\n        block_count_y       : int      # number of blocks tiled in the y direction\n        block_count_x       : int      # number of blocks tiled in the x direction\n        block_count_z       : int      # number of blocks tiled in the z direction\n        \"\"\"\n\n    class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n        Attributes:\n            NonRigidMotionCorrection (foreign key): Primary key from\n                NonRigidMotionCorrection.\n            block_id (int): Unique block ID.\n            block_y (longblob): y_start and y_end in pixels for this block\n            block_x (longblob): x_start and x_end in pixels for this block\n            block_z (longblob): z_start and z_end in pixels for this block\n            y_shifts (longblob): y motion correction shifts for every frame in pixels\n            x_shifts (longblob): x motion correction shifts for every frame in pixels\n            z_shift=null (longblob, optional): x motion correction shifts for every frame\n                in pixels\n            y_std (float): standard deviation of y shifts across all frames in pixels\n            x_std (float): standard deviation of x shifts across all frames in pixels\n            z_std=null (float, optional): standard deviation of z shifts across all frames\n                in pixels\n        \"\"\"\n\n        definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n        -&gt; master.NonRigidMotionCorrection\n        block_id        : int\n        ---\n        block_y         : longblob  # (y_start, y_end) in pixel of this block\n        block_x         : longblob  # (x_start, x_end) in pixel of this block\n        block_z         : longblob  # (z_start, z_end) in pixel of this block\n        y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n        x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n        z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n        y_std           : float     # (pixels) standard deviation of y shifts across all frames\n        x_std           : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n            ref_image (longblob): Image used as alignment template.\n            average_image (longblob): Mean of registered frames.\n            correlation_image (longblob, optional): Correlation map (computed during\n                cell detection).\n            max_proj_image (longblob, optional): Max of registered frames.\n        \"\"\"\n\n        definition = \"\"\"# Summary images for each field and channel after corrections\n        -&gt; master\n        -&gt; scan.ScanInfo.Field\n        ---\n        ref_image               : longblob  # image used as alignment template\n        average_image           : longblob  # mean of registered frames\n        correlation_image=null  : longblob  # correlation map (computed during cell detection)\n        max_proj_image=null     : longblob  # max of registered frames\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n            \"KEY\", \"field_z\", order_by=\"field_z\"\n        )\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n            # ---- iterate through all s2p plane outputs ----\n            rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n            summary_images = []\n            for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n                # -- rigid motion correction --\n                if idx == 0:\n                    rigid_correction = {\n                        **key,\n                        \"y_shifts\": s2p.ops[\"yoff\"],\n                        \"x_shifts\": s2p.ops[\"xoff\"],\n                        \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                        \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                        \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                        \"z_std\": np.nan,\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    rigid_correction[\"y_shifts\"] = np.vstack(\n                        [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                    )\n                    rigid_correction[\"y_std\"] = np.nanstd(\n                        rigid_correction[\"y_shifts\"].flatten()\n                    )\n                    rigid_correction[\"x_shifts\"] = np.vstack(\n                        [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                    )\n                    rigid_correction[\"x_std\"] = np.nanstd(\n                        rigid_correction[\"x_shifts\"].flatten()\n                    )\n                    rigid_correction[\"outlier_frames\"] = np.logical_or(\n                        rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                    )\n                # -- non-rigid motion correction --\n                if s2p.ops[\"nonrigid\"]:\n                    if idx == 0:\n                        nonrigid_correction = {\n                            **key,\n                            \"block_height\": s2p.ops[\"block_size\"][0],\n                            \"block_width\": s2p.ops[\"block_size\"][1],\n                            \"block_depth\": 1,\n                            \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                            \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                            \"block_count_z\": len(suite2p_dataset.planes),\n                            \"outlier_frames\": s2p.ops[\"badframes\"],\n                        }\n                    else:\n                        nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                            nonrigid_correction[\"outlier_frames\"],\n                            s2p.ops[\"badframes\"],\n                        )\n                    for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                        zip(\n                            s2p.ops[\"xblock\"],\n                            s2p.ops[\"yblock\"],\n                            s2p.ops[\"yoff1\"].T,\n                            s2p.ops[\"xoff1\"].T,\n                        )\n                    ):\n                        if b_id in nonrigid_blocks:\n                            nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                            )\n                            nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                            )\n                            nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                            )\n                            nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                            )\n                        else:\n                            nonrigid_blocks[b_id] = {\n                                **key,\n                                \"block_id\": b_id,\n                                \"block_y\": b_y,\n                                \"block_x\": b_x,\n                                \"block_z\": np.full_like(b_x, plane),\n                                \"y_shifts\": bshift_y,\n                                \"x_shifts\": bshift_x,\n                                \"z_shifts\": np.full(\n                                    (\n                                        len(suite2p_dataset.planes),\n                                        len(bshift_x),\n                                    ),\n                                    0,\n                                ),\n                                \"y_std\": np.nanstd(bshift_y),\n                                \"x_std\": np.nanstd(bshift_x),\n                                \"z_std\": np.nan,\n                            }\n\n                # -- summary images --\n                motion_correction_key = (\n                    scan.ScanInfo.Field * Processing &amp; key &amp; field_keys[plane]\n                ).fetch1(\"KEY\")\n                summary_images.append(\n                    {\n                        **motion_correction_key,\n                        \"ref_image\": s2p.ref_image,\n                        \"average_image\": s2p.mean_image,\n                        \"correlation_image\": s2p.correlation_map,\n                        \"max_proj_image\": s2p.max_proj_image,\n                    }\n                )\n\n            self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n            if rigid_correction:\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            if nonrigid_correction:\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks.values())\n            self.Summary.insert(summary_images)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            self.insert1(\n                {\n                    **key,\n                    \"motion_correct_channel\": caiman_dataset.alignment_channel,\n                }\n            )\n\n            is3D = caiman_dataset.params.motion[\"is3D\"]\n            if not caiman_dataset.params.motion[\"pw_rigid\"]:\n                # -- rigid motion correction --\n                rigid_correction = {\n                    **key,\n                    \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                    \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                    \"z_shifts\": (\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                        if is3D\n                        else np.full_like(\n                            caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                            0,\n                        )\n                    ),\n                    \"x_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                    ),\n                    \"y_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                    ),\n                    \"z_std\": (\n                        np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                        if is3D\n                        else np.nan\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            else:\n                # -- non-rigid motion correction --\n                nonrigid_correction = {\n                    **key,\n                    \"block_height\": (\n                        caiman_dataset.params.motion[\"strides\"][0]\n                        + caiman_dataset.params.motion[\"overlaps\"][0]\n                    ),\n                    \"block_width\": (\n                        caiman_dataset.params.motion[\"strides\"][1]\n                        + caiman_dataset.params.motion[\"overlaps\"][1]\n                    ),\n                    \"block_depth\": (\n                        caiman_dataset.params.motion[\"strides\"][2]\n                        + caiman_dataset.params.motion[\"overlaps\"][2]\n                        if is3D\n                        else 1\n                    ),\n                    \"block_count_x\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                    ),\n                    \"block_count_y\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                    ),\n                    \"block_count_z\": (\n                        len(\n                            set(\n                                caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    :, 4\n                                ]\n                            )\n                        )\n                        if is3D\n                        else 1\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                nonrigid_blocks = []\n                for b_id in range(\n                    len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n                ):\n                    nonrigid_blocks.append(\n                        {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_x\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 0:2\n                                ]\n                            ),\n                            \"block_y\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 2:4\n                                ]\n                            ),\n                            \"block_z\": (\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 4:6]\n                                )\n                                if is3D\n                                else np.full_like(\n                                    np.arange(\n                                        *caiman_dataset.motion_correction[\n                                            \"coord_shifts_els\"\n                                        ][b_id, 0:2]\n                                    ),\n                                    0,\n                                )\n                            ),\n                            \"x_shifts\": caiman_dataset.motion_correction[\n                                \"x_shifts_els\"\n                            ][:, b_id],\n                            \"y_shifts\": caiman_dataset.motion_correction[\n                                \"y_shifts_els\"\n                            ][:, b_id],\n                            \"z_shifts\": (\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                                if is3D\n                                else np.full_like(\n                                    caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                        :, b_id\n                                    ],\n                                    0,\n                                )\n                            ),\n                            \"x_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"y_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"z_std\": (\n                                np.nanstd(\n                                    caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                        :, b_id\n                                    ]\n                                )\n                                if is3D\n                                else np.nan\n                            ),\n                        }\n                    )\n\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks)\n\n            # -- summary images --\n            summary_images = [\n                {\n                    **key,\n                    **fkey,\n                    \"ref_image\": ref_image,\n                    \"average_image\": ave_img,\n                    \"correlation_image\": corr_img,\n                    \"max_proj_image\": max_img,\n                }\n                for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                    field_keys,\n                    caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"reference_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"average_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"max_image\"][...][\n                        np.newaxis, ...\n                    ],\n                )\n            ]\n            self.Summary.insert(summary_images)\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.Block", "title": "<code>Block</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>FOV-tiled blocks used for non-rigid motion correction.</p> <p>Attributes:</p> Name Type Description <code>NonRigidMotionCorrection</code> <code>foreign key</code> <p>Primary key from NonRigidMotionCorrection.</p> <code>block_id</code> <code>int</code> <p>Unique block ID.</p> <code>block_y</code> <code>longblob</code> <p>y_start and y_end in pixels for this block</p> <code>block_x</code> <code>longblob</code> <p>x_start and x_end in pixels for this block</p> <code>block_z</code> <code>longblob</code> <p>z_start and z_end in pixels for this block</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts for every frame in pixels</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>z_shift=null</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames in pixels</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames in pixels</p> <code>z_std=null</code> <code>float</code> <p>standard deviation of z shifts across all frames in pixels</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n    Attributes:\n        NonRigidMotionCorrection (foreign key): Primary key from\n            NonRigidMotionCorrection.\n        block_id (int): Unique block ID.\n        block_y (longblob): y_start and y_end in pixels for this block\n        block_x (longblob): x_start and x_end in pixels for this block\n        block_z (longblob): z_start and z_end in pixels for this block\n        y_shifts (longblob): y motion correction shifts for every frame in pixels\n        x_shifts (longblob): x motion correction shifts for every frame in pixels\n        z_shift=null (longblob, optional): x motion correction shifts for every frame\n            in pixels\n        y_std (float): standard deviation of y shifts across all frames in pixels\n        x_std (float): standard deviation of x shifts across all frames in pixels\n        z_std=null (float, optional): standard deviation of z shifts across all frames\n            in pixels\n    \"\"\"\n\n    definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n    -&gt; master.NonRigidMotionCorrection\n    block_id        : int\n    ---\n    block_y         : longblob  # (y_start, y_end) in pixel of this block\n    block_x         : longblob  # (x_start, x_end) in pixel of this block\n    block_z         : longblob  # (z_start, z_end) in pixel of this block\n    y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n    x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n    z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n    y_std           : float     # (pixels) standard deviation of y shifts across all frames\n    x_std           : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.NonRigidMotionCorrection", "title": "<code>NonRigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob, null</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>block_height</code> <code>int</code> <p>Block height in pixels.</p> <code>block_width</code> <code>int</code> <p>Block width in pixels.</p> <code>block_depth</code> <code>int</code> <p>Block depth in pixels.</p> <code>block_count_y</code> <code>int</code> <p>Number of blocks tiled in the y direction.</p> <code>block_count_x</code> <code>int</code> <p>Number of blocks tiled in the x direction.</p> <code>block_count_z</code> <code>int</code> <p>Number of blocks tiled in the z direction.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n    blocks/patches.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob, null): Mask with true for frames with outlier\n            shifts (already corrected).\n        block_height (int): Block height in pixels.\n        block_width (int): Block width in pixels.\n        block_depth (int): Block depth in pixels.\n        block_count_y (int): Number of blocks tiled in the y direction.\n        block_count_x (int): Number of blocks tiled in the x direction.\n        block_count_z (int): Number of blocks tiled in the z direction.\n    \"\"\"\n\n    definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n    block_height        : int      # (pixels)\n    block_width         : int      # (pixels)\n    block_depth         : int      # (pixels)\n    block_count_y       : int      # number of blocks tiled in the y direction\n    block_count_x       : int      # number of blocks tiled in the x direction\n    block_count_z       : int      # number of blocks tiled in the z direction\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.RigidMotionCorrection", "title": "<code>RigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of rigid motion correction performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts (pixels).</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts (pixels).</p> <code>z_shifts</code> <code>longblob</code> <p>z motion correction shifts (z-drift, pixels).</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames (pixels).</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames (pixels).</p> <code>z_std</code> <code>float</code> <p>standard deviation of z shifts across all frames (pixels).</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob): Mask with true for frames with outlier shifts\n            (already corrected).\n        y_shifts (longblob): y motion correction shifts (pixels).\n        x_shifts (longblob): x motion correction shifts (pixels).\n        z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n        y_std (float): standard deviation of y shifts across all frames (pixels).\n        x_std (float): standard deviation of x shifts across all frames (pixels).\n        z_std (float, optional): standard deviation of z shifts across all frames\n            (pixels).\n    \"\"\"\n\n    definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n    y_shifts            : longblob  # (pixels) y motion correction shifts\n    x_shifts            : longblob  # (pixels) x motion correction shifts\n    z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift) \n    y_std               : float     # (pixels) standard deviation of y shifts across all frames\n    x_std               : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.Summary", "title": "<code>Summary</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Summary images for each field and channel after corrections.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>scan.ScanInfo.Field</code> <code>foreign key</code> <p>Primary key from scan.ScanInfo.Field.</p> <code>ref_image</code> <code>longblob</code> <p>Image used as alignment template.</p> <code>average_image</code> <code>longblob</code> <p>Mean of registered frames.</p> <code>correlation_image</code> <code>longblob</code> <p>Correlation map (computed during cell detection).</p> <code>max_proj_image</code> <code>longblob</code> <p>Max of registered frames.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n        ref_image (longblob): Image used as alignment template.\n        average_image (longblob): Mean of registered frames.\n        correlation_image (longblob, optional): Correlation map (computed during\n            cell detection).\n        max_proj_image (longblob, optional): Max of registered frames.\n    \"\"\"\n\n    definition = \"\"\"# Summary images for each field and channel after corrections\n    -&gt; master\n    -&gt; scan.ScanInfo.Field\n    ---\n    ref_image               : longblob  # image used as alignment template\n    average_image           : longblob  # mean of registered frames\n    correlation_image=null  : longblob  # correlation map (computed during cell detection)\n    max_proj_image=null     : longblob  # max of registered frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.make", "title": "<code>make(key)</code>", "text": "<p>Populate MotionCorrection with results parsed from analysis outputs</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n        \"KEY\", \"field_z\", order_by=\"field_z\"\n    )\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n        # ---- iterate through all s2p plane outputs ----\n        rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n        summary_images = []\n        for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n            # -- rigid motion correction --\n            if idx == 0:\n                rigid_correction = {\n                    **key,\n                    \"y_shifts\": s2p.ops[\"yoff\"],\n                    \"x_shifts\": s2p.ops[\"xoff\"],\n                    \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                    \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                    \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                    \"z_std\": np.nan,\n                    \"outlier_frames\": s2p.ops[\"badframes\"],\n                }\n            else:\n                rigid_correction[\"y_shifts\"] = np.vstack(\n                    [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                )\n                rigid_correction[\"y_std\"] = np.nanstd(\n                    rigid_correction[\"y_shifts\"].flatten()\n                )\n                rigid_correction[\"x_shifts\"] = np.vstack(\n                    [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                )\n                rigid_correction[\"x_std\"] = np.nanstd(\n                    rigid_correction[\"x_shifts\"].flatten()\n                )\n                rigid_correction[\"outlier_frames\"] = np.logical_or(\n                    rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                )\n            # -- non-rigid motion correction --\n            if s2p.ops[\"nonrigid\"]:\n                if idx == 0:\n                    nonrigid_correction = {\n                        **key,\n                        \"block_height\": s2p.ops[\"block_size\"][0],\n                        \"block_width\": s2p.ops[\"block_size\"][1],\n                        \"block_depth\": 1,\n                        \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                        \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                        \"block_count_z\": len(suite2p_dataset.planes),\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                        nonrigid_correction[\"outlier_frames\"],\n                        s2p.ops[\"badframes\"],\n                    )\n                for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                    zip(\n                        s2p.ops[\"xblock\"],\n                        s2p.ops[\"yblock\"],\n                        s2p.ops[\"yoff1\"].T,\n                        s2p.ops[\"xoff1\"].T,\n                    )\n                ):\n                    if b_id in nonrigid_blocks:\n                        nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                        )\n                        nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                        )\n                        nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                        )\n                        nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                        )\n                    else:\n                        nonrigid_blocks[b_id] = {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_y\": b_y,\n                            \"block_x\": b_x,\n                            \"block_z\": np.full_like(b_x, plane),\n                            \"y_shifts\": bshift_y,\n                            \"x_shifts\": bshift_x,\n                            \"z_shifts\": np.full(\n                                (\n                                    len(suite2p_dataset.planes),\n                                    len(bshift_x),\n                                ),\n                                0,\n                            ),\n                            \"y_std\": np.nanstd(bshift_y),\n                            \"x_std\": np.nanstd(bshift_x),\n                            \"z_std\": np.nan,\n                        }\n\n            # -- summary images --\n            motion_correction_key = (\n                scan.ScanInfo.Field * Processing &amp; key &amp; field_keys[plane]\n            ).fetch1(\"KEY\")\n            summary_images.append(\n                {\n                    **motion_correction_key,\n                    \"ref_image\": s2p.ref_image,\n                    \"average_image\": s2p.mean_image,\n                    \"correlation_image\": s2p.correlation_map,\n                    \"max_proj_image\": s2p.max_proj_image,\n                }\n            )\n\n        self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n        if rigid_correction:\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        if nonrigid_correction:\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks.values())\n        self.Summary.insert(summary_images)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        self.insert1(\n            {\n                **key,\n                \"motion_correct_channel\": caiman_dataset.alignment_channel,\n            }\n        )\n\n        is3D = caiman_dataset.params.motion[\"is3D\"]\n        if not caiman_dataset.params.motion[\"pw_rigid\"]:\n            # -- rigid motion correction --\n            rigid_correction = {\n                **key,\n                \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                \"z_shifts\": (\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                    if is3D\n                    else np.full_like(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                        0,\n                    )\n                ),\n                \"x_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                ),\n                \"y_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                ),\n                \"z_std\": (\n                    np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                    if is3D\n                    else np.nan\n                ),\n                \"outlier_frames\": None,\n            }\n\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        else:\n            # -- non-rigid motion correction --\n            nonrigid_correction = {\n                **key,\n                \"block_height\": (\n                    caiman_dataset.params.motion[\"strides\"][0]\n                    + caiman_dataset.params.motion[\"overlaps\"][0]\n                ),\n                \"block_width\": (\n                    caiman_dataset.params.motion[\"strides\"][1]\n                    + caiman_dataset.params.motion[\"overlaps\"][1]\n                ),\n                \"block_depth\": (\n                    caiman_dataset.params.motion[\"strides\"][2]\n                    + caiman_dataset.params.motion[\"overlaps\"][2]\n                    if is3D\n                    else 1\n                ),\n                \"block_count_x\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                ),\n                \"block_count_y\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                ),\n                \"block_count_z\": (\n                    len(\n                        set(\n                            caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                :, 4\n                            ]\n                        )\n                    )\n                    if is3D\n                    else 1\n                ),\n                \"outlier_frames\": None,\n            }\n\n            nonrigid_blocks = []\n            for b_id in range(\n                len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n            ):\n                nonrigid_blocks.append(\n                    {\n                        **key,\n                        \"block_id\": b_id,\n                        \"block_x\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 0:2\n                            ]\n                        ),\n                        \"block_y\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 2:4\n                            ]\n                        ),\n                        \"block_z\": (\n                            np.arange(\n                                *caiman_dataset.motion_correction[\n                                    \"coord_shifts_els\"\n                                ][b_id, 4:6]\n                            )\n                            if is3D\n                            else np.full_like(\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 0:2]\n                                ),\n                                0,\n                            )\n                        ),\n                        \"x_shifts\": caiman_dataset.motion_correction[\n                            \"x_shifts_els\"\n                        ][:, b_id],\n                        \"y_shifts\": caiman_dataset.motion_correction[\n                            \"y_shifts_els\"\n                        ][:, b_id],\n                        \"z_shifts\": (\n                            caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                :, b_id\n                            ]\n                            if is3D\n                            else np.full_like(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ],\n                                0,\n                            )\n                        ),\n                        \"x_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"y_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"z_std\": (\n                            np.nanstd(\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                            )\n                            if is3D\n                            else np.nan\n                        ),\n                    }\n                )\n\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks)\n\n        # -- summary images --\n        summary_images = [\n            {\n                **key,\n                **fkey,\n                \"ref_image\": ref_image,\n                \"average_image\": ave_img,\n                \"correlation_image\": corr_img,\n                \"max_proj_image\": max_img,\n            }\n            for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                field_keys,\n                caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"reference_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"average_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"max_image\"][...][\n                    np.newaxis, ...\n                ],\n            )\n        ]\n        self.Summary.insert(summary_images)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing", "title": "<code>Processing</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted.</p> <p>Attributes:</p> Name Type Description <code>ProcessingTask</code> <code>foreign key</code> <p>Primary key from ProcessingTask.</p> <code>processing_time</code> <code>datetime</code> <p>Process completion datetime.</p> <code>package_version</code> <code>str</code> <p>Version of the analysis package used in processing the data.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass Processing(dj.Computed):\n\"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table.\n    The computation is performed only on the scans with ScanInfo inserted.\n\n    Attributes:\n        ProcessingTask (foreign key): Primary key from ProcessingTask.\n        processing_time (datetime): Process completion datetime.\n        package_version (str, optional): Version of the analysis package used in\n            processing the data.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ProcessingTask\n    ---\n    processing_time     : datetime  # Time of generation of this set of processed, segmented results\n    package_version=''  : varchar(16)\n    \"\"\"\n\n    # Run processing only on Scan with ScanInfo inserted\n    @property\n    def key_source(self):\n\"\"\"Limit the Processing to Scans that have their metadata ingested to the\n        database.\"\"\"\n\n        return ProcessingTask &amp; scan.ScanInfo\n\n    def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n        task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n            \"task_mode\", \"processing_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update processing_output_dir\n            ProcessingTask.update1(\n                {**key, \"processing_output_dir\": output_dir.as_posix()}\n            )\n        output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n        if task_mode == \"load\":\n            method, imaging_dataset = get_loader_result(key, ProcessingTask)\n            if method == \"suite2p\":\n                if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                    raise NotImplementedError(\n                        f\"Suite2p ingestion error - Unable to handle\"\n                        f\" ScanImage multi-ROI scanning mode yet\"\n                    )\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n            elif method == \"caiman\":\n                caiman_dataset = imaging_dataset\n                key = {**key, \"processing_time\": caiman_dataset.creation_time}\n            elif method == \"extract\":\n                raise NotImplementedError(\n                    \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n                )\n            else:\n                raise NotImplementedError(\"Unknown method: {}\".format(method))\n        elif task_mode == \"trigger\":\n\n            method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n                \"processing_method\"\n            )\n\n            image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n            image_files = [\n                find_full_path(get_imaging_root_data_dir(), image_file)\n                for image_file in image_files\n            ]\n\n            if method == \"suite2p\":\n                import suite2p\n\n                suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                suite2p_params[\"save_path0\"] = output_dir\n                (\n                    suite2p_params[\"fs\"],\n                    suite2p_params[\"nplanes\"],\n                    suite2p_params[\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                suite2p_params[\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n            elif method == \"caiman\":\n                from element_interface.run_caiman import run_caiman\n                from element_interface.caiman_loader import (\n                    _process_scanimage_tiff,\n                )\n\n                caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                    \"fps\", \"ndepths\", \"nchannels\"\n                )\n\n                is3D = bool(ndepths &gt; 1)\n                if is3D:\n                    raise NotImplementedError(\n                        \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                    )\n\n                # handle multi-channel tiff image before running CaImAn\n                if nchannels &gt; 1:\n                    channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                    tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                    tmp_dir.mkdir(exist_ok=True)\n                    _process_scanimage_tiff(\n                        [f.as_posix() for f in image_files], output_dir=tmp_dir\n                    )\n                    image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n                run_caiman(\n                    file_paths=[f.as_posix() for f in image_files],\n                    parameters=caiman_params,\n                    sampling_rate=sampling_rate,\n                    output_dir=output_dir,\n                    is3D=is3D,\n                )\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                caiman_dataset = imaging_dataset\n                key[\"processing_time\"] = caiman_dataset.creation_time\n\n            elif method == \"extract\":\n                import suite2p\n                from scipy.io import savemat\n                from element_interface.extract_trigger import EXTRACT_trigger\n\n                # Motion Correction with Suite2p\n                params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n                params[\"suite2p\"][\"save_path0\"] = output_dir\n                (\n                    params[\"suite2p\"][\"fs\"],\n                    params[\"suite2p\"][\"nplanes\"],\n                    params[\"suite2p\"][\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n                # Convert data.bin to registered_scans.mat\n                scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n                data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                    \"nframes\", \"px_height\", \"px_width\"\n                )\n                data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n                scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n                # Save the motion corrected movie (data.bin) in a .mat file\n                savemat(\n                    scan_matlab_fullpath,\n                    {\"M\": np.transpose(data, axes=[1, 2, 0])},\n                )\n\n                # Execute EXTRACT\n\n                ex = EXTRACT_trigger(\n                    scan_matlab_fullpath, params[\"extract\"], output_dir\n                )\n                ex.run()\n\n                _, extract_dataset = get_loader_result(key, ProcessingTask)\n                key[\"processing_time\"] = extract_dataset.creation_time\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the Processing to Scans that have their metadata ingested to the database.</p>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing.make", "title": "<code>make(key)</code>", "text": "<p>Execute the calcium imaging analysis defined by the ProcessingTask.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n    task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n        \"task_mode\", \"processing_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update processing_output_dir\n        ProcessingTask.update1(\n            {**key, \"processing_output_dir\": output_dir.as_posix()}\n        )\n    output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n    if task_mode == \"load\":\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n        if method == \"suite2p\":\n            if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                raise NotImplementedError(\n                    f\"Suite2p ingestion error - Unable to handle\"\n                    f\" ScanImage multi-ROI scanning mode yet\"\n                )\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            key = {**key, \"processing_time\": caiman_dataset.creation_time}\n        elif method == \"extract\":\n            raise NotImplementedError(\n                \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n            )\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n    elif task_mode == \"trigger\":\n\n        method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n            \"processing_method\"\n        )\n\n        image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n        image_files = [\n            find_full_path(get_imaging_root_data_dir(), image_file)\n            for image_file in image_files\n        ]\n\n        if method == \"suite2p\":\n            import suite2p\n\n            suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            suite2p_params[\"save_path0\"] = output_dir\n            (\n                suite2p_params[\"fs\"],\n                suite2p_params[\"nplanes\"],\n                suite2p_params[\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            suite2p_params[\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n        elif method == \"caiman\":\n            from element_interface.run_caiman import run_caiman\n            from element_interface.caiman_loader import (\n                _process_scanimage_tiff,\n            )\n\n            caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                \"fps\", \"ndepths\", \"nchannels\"\n            )\n\n            is3D = bool(ndepths &gt; 1)\n            if is3D:\n                raise NotImplementedError(\n                    \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                )\n\n            # handle multi-channel tiff image before running CaImAn\n            if nchannels &gt; 1:\n                channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                tmp_dir.mkdir(exist_ok=True)\n                _process_scanimage_tiff(\n                    [f.as_posix() for f in image_files], output_dir=tmp_dir\n                )\n                image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n            run_caiman(\n                file_paths=[f.as_posix() for f in image_files],\n                parameters=caiman_params,\n                sampling_rate=sampling_rate,\n                output_dir=output_dir,\n                is3D=is3D,\n            )\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            caiman_dataset = imaging_dataset\n            key[\"processing_time\"] = caiman_dataset.creation_time\n\n        elif method == \"extract\":\n            import suite2p\n            from scipy.io import savemat\n            from element_interface.extract_trigger import EXTRACT_trigger\n\n            # Motion Correction with Suite2p\n            params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n            params[\"suite2p\"][\"save_path0\"] = output_dir\n            (\n                params[\"suite2p\"][\"fs\"],\n                params[\"suite2p\"][\"nplanes\"],\n                params[\"suite2p\"][\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n            # Convert data.bin to registered_scans.mat\n            scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n            data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                \"nframes\", \"px_height\", \"px_width\"\n            )\n            data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n            scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n            # Save the motion corrected movie (data.bin) in a .mat file\n            savemat(\n                scan_matlab_fullpath,\n                {\"M\": np.transpose(data, axes=[1, 2, 0])},\n            )\n\n            # Execute EXTRACT\n\n            ex = EXTRACT_trigger(\n                scan_matlab_fullpath, params[\"extract\"], output_dir\n            )\n            ex.run()\n\n            _, extract_dataset = get_loader_result(key, ProcessingTask)\n            key[\"processing_time\"] = extract_dataset.creation_time\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingMethod", "title": "<code>ProcessingMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Method, package, or analysis suite used for processing of calcium imaging data     (e.g. Suite2p, CaImAn, etc.).</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method.</p> <code>processing_method_desc</code> <code>str</code> <p>Processing method description.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ProcessingMethod(dj.Lookup):\n\"\"\"Method, package, or analysis suite used for processing of calcium imaging data\n        (e.g. Suite2p, CaImAn, etc.).\n\n    Attributes:\n        processing_method (str): Processing method.\n        processing_method_desc (str): Processing method description.\n    \"\"\"\n\n    definition = \"\"\"# Method for calcium imaging processing\n    processing_method: char(8)\n    ---\n    processing_method_desc: varchar(1000)  # Processing method description\n    \"\"\"\n\n    contents = [\n        (\"suite2p\", \"suite2p analysis suite\"),\n        (\"caiman\", \"caiman analysis suite\"),\n        (\"extract\", \"extract analysis suite\"),\n    ]\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingParamSet", "title": "<code>ProcessingParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters.</p> <p>A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Uniqiue parameter set ID.</p> <code>ProcessingMethod</code> <code>foreign key</code> <p>A primary key from ProcessingMethod.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>param_set_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameter Set, a dictionary of all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ProcessingParamSet(dj.Lookup):\n\"\"\"Parameter set used for the processing of the calcium imaging scans,\n    including both the analysis suite and its respective input parameters.\n\n    A hash of the parameters of the analysis suite is also stored in order\n    to avoid duplicated entries.\n\n    Attributes:\n        paramset_idx (int): Uniqiue parameter set ID.\n        ProcessingMethod (foreign key): A primary key from ProcessingMethod.\n        paramset_desc (str): Parameter set description.\n        param_set_hash (uuid): A universally unique identifier for the parameter set.\n        params (longblob): Parameter Set, a dictionary of all applicable parameters to\n            the analysis suite.\n    \"\"\"\n\n    definition = \"\"\"# Processing Parameter Set\n    paramset_idx: smallint  # Uniqiue parameter set ID.\n    ---\n    -&gt; ProcessingMethod\n    paramset_desc: varchar(1280)  # Parameter-set description\n    param_set_hash: uuid  # A universally unique identifier for the parameter set\n    unique index (param_set_hash)\n    params: longblob  # Parameter Set, a dictionary of all applicable parameters to the analysis suite.\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        processing_method: str,\n        paramset_idx: int,\n        paramset_desc: str,\n        params: dict,\n    ):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n        This function automizes the parameter set hashing and avoids insertion of an\n            existing parameter set.\n\n        Attributes:\n            processing_method (str): Processing method/package used for processing of\n                calcium imaging.\n            paramset_idx (int): Uniqiue parameter set ID.\n            paramset_desc (str): Parameter set description.\n            params (dict): Parameter Set, all applicable parameters to the analysis\n                suite.\n        \"\"\"\n        if processing_method == \"extract\":\n            assert (\n                params.get(\"extract\") != None and params.get(\"suite2p\") != None\n            ), ValueError(\n                \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n            )\n\n            # Force Suite2p to only run motion correction.\n            params[\"suite2p\"][\"do_registration\"] = True\n            params[\"suite2p\"][\"roidetect\"] = False\n\n        param_dict = {\n            \"processing_method\": processing_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if q_param:  # If the specified param-set already exists\n            pname = q_param.fetch1(\"paramset_idx\")\n            if pname == paramset_idx:  # If the existed set has the same name: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set already exists - name: {}\".format(pname)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingParamSet.insert_new_params", "title": "<code>insert_new_params(processing_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Insert a parameter set into ProcessingParamSet table.</p> <p>This function automizes the parameter set hashing and avoids insertion of an     existing parameter set.</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method/package used for processing of calcium imaging.</p> <code>paramset_idx</code> <code>int</code> <p>Uniqiue parameter set ID.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>params</code> <code>dict</code> <p>Parameter Set, all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    processing_method: str,\n    paramset_idx: int,\n    paramset_desc: str,\n    params: dict,\n):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n    This function automizes the parameter set hashing and avoids insertion of an\n        existing parameter set.\n\n    Attributes:\n        processing_method (str): Processing method/package used for processing of\n            calcium imaging.\n        paramset_idx (int): Uniqiue parameter set ID.\n        paramset_desc (str): Parameter set description.\n        params (dict): Parameter Set, all applicable parameters to the analysis\n            suite.\n    \"\"\"\n    if processing_method == \"extract\":\n        assert (\n            params.get(\"extract\") != None and params.get(\"suite2p\") != None\n        ), ValueError(\n            \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n        )\n\n        # Force Suite2p to only run motion correction.\n        params[\"suite2p\"][\"do_registration\"] = True\n        params[\"suite2p\"][\"roidetect\"] = False\n\n    param_dict = {\n        \"processing_method\": processing_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if q_param:  # If the specified param-set already exists\n        pname = q_param.fetch1(\"paramset_idx\")\n        if pname == paramset_idx:  # If the existed set has the same name: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set already exists - name: {}\".format(pname)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask", "title": "<code>ProcessingTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A pairing of processing params and scans to be loaded or triggered</p> <p>This table defines a calcium imaging processing task for a combination of a <code>Scan</code> and a <code>ProcessingParamSet</code> entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods</p> <p>Attributes:</p> Name Type Description <code>scan.Scan</code> <code>foreign key</code> <code>ProcessingParamSet</code> <code>foreign key</code> <code>processing_output_dir</code> <code>str</code> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results) or 'trigger' (trigger computation).</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ProcessingTask(dj.Manual):\n\"\"\"A pairing of processing params and scans to be loaded or triggered\n\n    This table defines a calcium imaging processing task for a combination of a\n    `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method,\n    method's parameters). The task defined here is then run in the downstream table\n    Processing. This table supports definitions of both loading of pre-generated results\n    and the triggering of new analysis for all supported analysis methods\n\n    Attributes:\n        scan.Scan (foreign key):\n        ProcessingParamSet (foreign key):\n        processing_output_dir (str):\n        task_mode (str): One of 'load' (load computed analysis results) or 'trigger'\n            (trigger computation).\n    \"\"\"\n\n    definition = \"\"\"# Manual table for defining a processing task ready to be run\n    -&gt; scan.Scan\n    -&gt; ProcessingParamSet\n    ---\n    processing_output_dir: varchar(255)  #  Output directory of the processed scan relative to root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n        Args:\n            key (dict): Primary key from the ProcessingTask table.\n            relative (bool): If True, processing_output_dir is returned relative to\n                imaging_root_dir. Default False.\n            mkdir (bool): If True, create the processing_output_dir directory.\n                Default True.\n\n        Returns:\n            dir (str): A default output directory for the processed results (processed_output_dir\n                in ProcessingTask) based on the following convention:\n                processed_dir / scan_dir / {processing_method}_{paramset_idx}\n                e.g.: sub4/sess1/scan0/suite2p_0\n        \"\"\"\n        image_locators = {\n            \"NIS\": get_nd2_files,\n            \"ScanImage\": get_scan_image_files,\n            \"Scanbox\": get_scan_box_files,\n            \"PrairieView\": get_prairieview_files,\n        }\n        image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n        scan_dir = find_full_path(\n            get_imaging_root_data_dir(), image_locator(key)[0]\n        ).parent\n        root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n        method = (\n            (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n        )\n\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        output_dir = (\n            processed_dir\n            / scan_dir.relative_to(root_dir)\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n        Generate an entry in the ProcessingTask table for a particular scan using an\n        existing parameter set from the ProcessingParamSet table.\n\n        Args:\n            scan_key (dict): Primary key from Scan table.\n            paramset_idx (int): Unique parameter set ID.\n        \"\"\"\n        key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n        method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n            \"processing_method\"\n        )\n\n        try:\n            if method == \"suite2p\":\n                from element_interface import suite2p_loader\n\n                suite2p_loader.Suite2p(output_dir)\n            elif method == \"caiman\":\n                from element_interface import caiman_loader\n\n                caiman_loader.CaImAn(output_dir)\n            elif method == \"extract\":\n                from element_interface import extract_loader\n\n                extract_loader.EXTRACT(output_dir)\n\n            else:\n                raise NotImplementedError(\n                    \"Unknown/unimplemented method: {}\".format(method)\n                )\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"processing_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n\n    auto_generate_entries = generate\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask.generate", "title": "<code>generate(scan_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet</p> <p>Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan table.</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>0</code> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@classmethod\ndef generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n    Generate an entry in the ProcessingTask table for a particular scan using an\n    existing parameter set from the ProcessingParamSet table.\n\n    Args:\n        scan_key (dict): Primary key from Scan table.\n        paramset_idx (int): Unique parameter set ID.\n    \"\"\"\n    key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n    method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n        \"processing_method\"\n    )\n\n    try:\n        if method == \"suite2p\":\n            from element_interface import suite2p_loader\n\n            suite2p_loader.Suite2p(output_dir)\n        elif method == \"caiman\":\n            from element_interface import caiman_loader\n\n            caiman_loader.CaImAn(output_dir)\n        elif method == \"extract\":\n            from element_interface import extract_loader\n\n            extract_loader.EXTRACT(output_dir)\n\n        else:\n            raise NotImplementedError(\n                \"Unknown/unimplemented method: {}\".format(method)\n            )\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"processing_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer an output directory for an entry in ProcessingTask table.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from the ProcessingTask table.</p> required <code>relative</code> <code>bool</code> <p>If True, processing_output_dir is returned relative to imaging_root_dir. Default False.</p> <code>False</code> <code>mkdir</code> <code>bool</code> <p>If True, create the processing_output_dir directory. Default True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dir</code> <code>str</code> <p>A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n    Args:\n        key (dict): Primary key from the ProcessingTask table.\n        relative (bool): If True, processing_output_dir is returned relative to\n            imaging_root_dir. Default False.\n        mkdir (bool): If True, create the processing_output_dir directory.\n            Default True.\n\n    Returns:\n        dir (str): A default output directory for the processed results (processed_output_dir\n            in ProcessingTask) based on the following convention:\n            processed_dir / scan_dir / {processing_method}_{paramset_idx}\n            e.g.: sub4/sess1/scan0/suite2p_0\n    \"\"\"\n    image_locators = {\n        \"NIS\": get_nd2_files,\n        \"ScanImage\": get_scan_image_files,\n        \"Scanbox\": get_scan_box_files,\n        \"PrairieView\": get_prairieview_files,\n    }\n    image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n    scan_dir = find_full_path(\n        get_imaging_root_data_dir(), image_locator(key)[0]\n    ).parent\n    root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n    method = (\n        (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n    )\n\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    output_dir = (\n        processed_dir\n        / scan_dir.relative_to(root_dir)\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation", "title": "<code>Segmentation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Result of the Segmentation process.</p> <p>Attributes:</p> Name Type Description <code>Processing</code> <code>foreign key</code> <p>Primary key from Processing.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass Segmentation(dj.Computed):\n\"\"\"Result of the Segmentation process.\n\n    Attributes:\n        Processing (foreign key): Primary key from Processing.\n    \"\"\"\n\n    definition = \"\"\"# Different mask segmentations.\n    -&gt; Processing\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n        Attributes:\n            Segmentation (foreign key): Primary key from Segmentation.\n            mask (int): Unique mask ID.\n            scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n                used for segmentation.\n            mask_npix (int): Number of pixels in ROIs.\n            mask_center_x (int): Center x coordinate in pixel.\n            mask_center_y (int): Center y coordinate in pixel.\n            mask_center_z (int): Center z coordinate in pixel.\n            mask_xpix (longblob): X coordinates in pixels.\n            mask_ypix (longblob): Y coordinates in pixels.\n            mask_zpix (longblob): Z coordinates in pixels.\n            mask_weights (longblob): Weights of the mask at the indices above.\n        \"\"\"\n\n        definition = \"\"\" # A mask produced by segmentation.\n        -&gt; master\n        mask            : smallint\n        ---\n        -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n        mask_npix       : int       # number of pixels in ROIs\n        mask_center_x   : int       # center x coordinate in pixel\n        mask_center_y   : int       # center y coordinate in pixel\n        mask_center_z   : int       # center z coordinate in pixel\n        mask_xpix       : longblob  # x coordinates in pixels\n        mask_ypix       : longblob  # y coordinates in pixels      \n        mask_zpix       : longblob  # z coordinates in pixels        \n        mask_weights    : longblob  # weights of the mask at the indices above\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            masks, cells = [], []\n            for plane, s2p in suite2p_dataset.planes.items():\n                mask_count = len(masks)  # increment mask id from all \"plane\"\n                for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                    zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n                ):\n                    masks.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"segmentation_channel\": s2p.segmentation_channel,\n                            \"mask_npix\": mask_stat[\"npix\"],\n                            \"mask_center_x\": mask_stat[\"med\"][1],\n                            \"mask_center_y\": mask_stat[\"med\"][0],\n                            \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                            \"mask_xpix\": mask_stat[\"xpix\"],\n                            \"mask_ypix\": mask_stat[\"ypix\"],\n                            \"mask_zpix\": np.full(\n                                mask_stat[\"npix\"],\n                                mask_stat.get(\"iplane\", plane),\n                            ),\n                            \"mask_weights\": mask_stat[\"lam\"],\n                        }\n                    )\n                    if is_cell:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"suite2p_default_classifier\",\n                                \"mask\": mask_idx + mask_count,\n                                \"mask_type\": \"soma\",\n                                \"confidence\": cell_prob,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"suite2p_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            masks, cells = [], []\n            for mask in caiman_dataset.masks:\n                masks.append(\n                    {\n                        **key,\n                        \"segmentation_channel\": segmentation_channel,\n                        \"mask\": mask[\"mask_id\"],\n                        \"mask_npix\": mask[\"mask_npix\"],\n                        \"mask_center_x\": mask[\"mask_center_x\"],\n                        \"mask_center_y\": mask[\"mask_center_y\"],\n                        \"mask_center_z\": mask[\"mask_center_z\"],\n                        \"mask_xpix\": mask[\"mask_xpix\"],\n                        \"mask_ypix\": mask[\"mask_ypix\"],\n                        \"mask_zpix\": mask[\"mask_zpix\"],\n                        \"mask_weights\": mask[\"mask_weights\"],\n                    }\n                )\n                if caiman_dataset.cnmf.estimates.idx_components is not None:\n                    if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"caiman_default_classifier\",\n                                \"mask\": mask[\"mask_id\"],\n                                \"mask_type\": \"soma\",\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"caiman_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            masks = [\n                dict(\n                    **key,\n                    segmentation_channel=0,\n                    mask=mask[\"mask_id\"],\n                    mask_npix=mask[\"mask_npix\"],\n                    mask_center_x=mask[\"mask_center_x\"],\n                    mask_center_y=mask[\"mask_center_y\"],\n                    mask_center_z=mask[\"mask_center_z\"],\n                    mask_xpix=mask[\"mask_xpix\"],\n                    mask_ypix=mask[\"mask_ypix\"],\n                    mask_zpix=mask[\"mask_zpix\"],\n                    mask_weights=mask[\"mask_weights\"],\n                )\n                for mask in extract_dataset.load_results()\n            ]\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n        else:\n            raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of the masks identified from the Segmentation procedure.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>mask</code> <code>int</code> <p>Unique mask ID.</p> <code>scan.Channel.proj(segmentation_channel='channel')</code> <code>foreign key</code> <p>Channel used for segmentation.</p> <code>mask_npix</code> <code>int</code> <p>Number of pixels in ROIs.</p> <code>mask_center_x</code> <code>int</code> <p>Center x coordinate in pixel.</p> <code>mask_center_y</code> <code>int</code> <p>Center y coordinate in pixel.</p> <code>mask_center_z</code> <code>int</code> <p>Center z coordinate in pixel.</p> <code>mask_xpix</code> <code>longblob</code> <p>X coordinates in pixels.</p> <code>mask_ypix</code> <code>longblob</code> <p>Y coordinates in pixels.</p> <code>mask_zpix</code> <code>longblob</code> <p>Z coordinates in pixels.</p> <code>mask_weights</code> <code>longblob</code> <p>Weights of the mask at the indices above.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        mask (int): Unique mask ID.\n        scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n            used for segmentation.\n        mask_npix (int): Number of pixels in ROIs.\n        mask_center_x (int): Center x coordinate in pixel.\n        mask_center_y (int): Center y coordinate in pixel.\n        mask_center_z (int): Center z coordinate in pixel.\n        mask_xpix (longblob): X coordinates in pixels.\n        mask_ypix (longblob): Y coordinates in pixels.\n        mask_zpix (longblob): Z coordinates in pixels.\n        mask_weights (longblob): Weights of the mask at the indices above.\n    \"\"\"\n\n    definition = \"\"\" # A mask produced by segmentation.\n    -&gt; master\n    mask            : smallint\n    ---\n    -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n    mask_npix       : int       # number of pixels in ROIs\n    mask_center_x   : int       # center x coordinate in pixel\n    mask_center_y   : int       # center y coordinate in pixel\n    mask_center_z   : int       # center z coordinate in pixel\n    mask_xpix       : longblob  # x coordinates in pixels\n    mask_ypix       : longblob  # y coordinates in pixels      \n    mask_zpix       : longblob  # z coordinates in pixels        \n    mask_weights    : longblob  # weights of the mask at the indices above\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Segmentation with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        masks, cells = [], []\n        for plane, s2p in suite2p_dataset.planes.items():\n            mask_count = len(masks)  # increment mask id from all \"plane\"\n            for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n            ):\n                masks.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"segmentation_channel\": s2p.segmentation_channel,\n                        \"mask_npix\": mask_stat[\"npix\"],\n                        \"mask_center_x\": mask_stat[\"med\"][1],\n                        \"mask_center_y\": mask_stat[\"med\"][0],\n                        \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                        \"mask_xpix\": mask_stat[\"xpix\"],\n                        \"mask_ypix\": mask_stat[\"ypix\"],\n                        \"mask_zpix\": np.full(\n                            mask_stat[\"npix\"],\n                            mask_stat.get(\"iplane\", plane),\n                        ),\n                        \"mask_weights\": mask_stat[\"lam\"],\n                    }\n                )\n                if is_cell:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"suite2p_default_classifier\",\n                            \"mask\": mask_idx + mask_count,\n                            \"mask_type\": \"soma\",\n                            \"confidence\": cell_prob,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"suite2p_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        masks, cells = [], []\n        for mask in caiman_dataset.masks:\n            masks.append(\n                {\n                    **key,\n                    \"segmentation_channel\": segmentation_channel,\n                    \"mask\": mask[\"mask_id\"],\n                    \"mask_npix\": mask[\"mask_npix\"],\n                    \"mask_center_x\": mask[\"mask_center_x\"],\n                    \"mask_center_y\": mask[\"mask_center_y\"],\n                    \"mask_center_z\": mask[\"mask_center_z\"],\n                    \"mask_xpix\": mask[\"mask_xpix\"],\n                    \"mask_ypix\": mask[\"mask_ypix\"],\n                    \"mask_zpix\": mask[\"mask_zpix\"],\n                    \"mask_weights\": mask[\"mask_weights\"],\n                }\n            )\n            if caiman_dataset.cnmf.estimates.idx_components is not None:\n                if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"caiman_default_classifier\",\n                            \"mask\": mask[\"mask_id\"],\n                            \"mask_type\": \"soma\",\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"caiman_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        masks = [\n            dict(\n                **key,\n                segmentation_channel=0,\n                mask=mask[\"mask_id\"],\n                mask_npix=mask[\"mask_npix\"],\n                mask_center_x=mask[\"mask_center_x\"],\n                mask_center_y=mask[\"mask_center_y\"],\n                mask_center_z=mask[\"mask_center_z\"],\n                mask_xpix=mask[\"mask_xpix\"],\n                mask_ypix=mask[\"mask_ypix\"],\n                mask_zpix=mask[\"mask_zpix\"],\n                mask_weights=mask[\"mask_weights\"],\n            )\n            for mask in extract_dataset.load_results()\n        ]\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n    else:\n        raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.activate", "title": "<code>activate(imaging_schema_name, scan_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>imaging_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>imaging</code> module.</p> required <code>scan_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>scan</code> module. Omitted, if the <code>scan</code> module is already activated.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>imaging</code> module: + all that are required by the <code>scan</code> module.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <ul> <li>Session: A parent table to Scan, identifying a scanning session.</li> <li>Equipment: A parent table to Scan, identifying a scanning device.</li> </ul> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def activate(\n    imaging_schema_name,\n    scan_schema_name=None,\n    *,\n    create_schema=True,\n    create_tables=True,\n    linking_module=None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        imaging_schema_name (str): Schema name on the database server to activate the\n            `imaging` module.\n        scan_schema_name (str): Schema name on the database server to activate the\n            `scan` module. Omitted, if the `scan` module is already activated.\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `imaging` module: + all that are required by\n            the `scan` module.\n\n    Dependencies:\n    Upstream tables:\n        + Session: A parent table to Scan, identifying a scanning session.\n        + Equipment: A parent table to Scan, identifying a scanning device.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    scan.activate(\n        scan_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        linking_module=linking_module,\n    )\n    schema.activate(\n        imaging_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    imaging_report.activate(f\"{imaging_schema_name}_report\", imaging_schema_name)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_loader_result", "title": "<code>get_loader_result(key, table)</code>", "text": "<p>Retrieve the processed imaging results from a suite2p or caiman loader.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>The <code>key</code> to one entry of ProcessingTask or Curation</p> required <code>table</code> <code>dj.Table</code> <p>A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation)</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the processing_method is different than 'suite2p' or 'caiman'.</p> <p>Returns:</p> Type Description <p>A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,</p> <p>see element-interface for more information on the loaders.)</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def get_loader_result(key: dict, table: dj.Table):\n\"\"\"Retrieve the processed imaging results from a suite2p or caiman loader.\n\n    Args:\n        key (dict): The `key` to one entry of ProcessingTask or Curation\n        table (dj.Table): A datajoint table to retrieve the loaded results from (e.g.\n            ProcessingTask, Curation)\n\n    Raises:\n        NotImplementedError: If the processing_method is different than 'suite2p' or\n            'caiman'.\n\n    Returns:\n        A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,\n        see element-interface for more information on the loaders.)\n    \"\"\"\n    method, output_dir = (ProcessingParamSet * table &amp; key).fetch1(\n        \"processing_method\", _table_attribute_mapper[table.__name__]\n    )\n\n    output_path = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n    if method == \"suite2p\" or (\n        method == \"extract\" and table.__name__ == \"MotionCorrection\"\n    ):\n        from element_interface import suite2p_loader\n\n        loaded_dataset = suite2p_loader.Suite2p(output_path)\n    elif method == \"caiman\":\n        from element_interface import caiman_loader\n\n        loaded_dataset = caiman_loader.CaImAn(output_path)\n    elif method == \"extract\":\n        from element_interface import extract_loader\n\n        loaded_dataset = extract_loader.EXTRACT(output_path)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n\n    return method, loaded_dataset\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/", "title": "imaging_preprocess.py", "text": ""}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity", "title": "<code>Activity</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>ActivityExtractionMethod</code> <code>foreign key</code> <p>Primary key from ActivityExtractionMethod.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Activity(dj.Computed):\n\"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        ActivityExtractionMethod (foreign key): Primary key from\n            ActivityExtractionMethod.\n    \"\"\"\n\n    definition = \"\"\"# Neural Activity\n    -&gt; Fluorescence\n    -&gt; ActivityExtractionMethod\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n        Attributes:\n            Activity (foreign key): Primary key from Activity.\n            Fluorescence.Trace (foreign key): Fluorescence.Trace.\n            activity_trace (longblob): Neural activity from fluoresence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        activity_trace: longblob\n        \"\"\"\n\n    @property\n    def key_source(self):\n        suite2p_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"suite2p\"'\n            &amp; 'extraction_method LIKE \"suite2p%\"'\n        )\n        caiman_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"caiman\"'\n            &amp; 'extraction_method LIKE \"caiman%\"'\n        )\n        return suite2p_key_source.proj() + caiman_key_source.proj()\n\n    def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis\n        outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n                suite2p_dataset = imaging_dataset\n                # ---- iterate through all s2p plane outputs ----\n                spikes = [\n                    dict(\n                        key,\n                        mask=mask_idx,\n                        fluo_channel=0,\n                        activity_trace=spks,\n                    )\n                    for mask_idx, spks in enumerate(\n                        s\n                        for plane in suite2p_dataset.planes.values()\n                        for s in plane.spks\n                    )\n                ]\n\n                self.insert1(key)\n                self.Trace.insert(spikes)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            if key[\"extraction_method\"] in (\n                \"caiman_deconvolution\",\n                \"caiman_dff\",\n            ):\n                attr_mapper = {\n                    \"caiman_deconvolution\": \"spikes\",\n                    \"caiman_dff\": \"dff\",\n                }\n\n                # infer \"segmentation_channel\" - from params if available, else from caiman loader\n                params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n                segmentation_channel = params.get(\n                    \"segmentation_channel\", caiman_dataset.segmentation_channel\n                )\n\n                self.insert1(key)\n                self.Trace.insert(\n                    dict(\n                        key,\n                        mask=mask[\"mask_id\"],\n                        fluo_channel=segmentation_channel,\n                        activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                    )\n                    for mask in caiman_dataset.masks\n                )\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trace(s) for each mask.</p> <p>Attributes:</p> Name Type Description <code>Activity</code> <code>foreign key</code> <p>Primary key from Activity.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Fluorescence.Trace.</p> <code>activity_trace</code> <code>longblob</code> <p>Neural activity from fluoresence trace.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n    Attributes:\n        Activity (foreign key): Primary key from Activity.\n        Fluorescence.Trace (foreign key): Fluorescence.Trace.\n        activity_trace (longblob): Neural activity from fluoresence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    activity_trace: longblob\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Activity with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis\n    outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n            suite2p_dataset = imaging_dataset\n            # ---- iterate through all s2p plane outputs ----\n            spikes = [\n                dict(\n                    key,\n                    mask=mask_idx,\n                    fluo_channel=0,\n                    activity_trace=spks,\n                )\n                for mask_idx, spks in enumerate(\n                    s\n                    for plane in suite2p_dataset.planes.values()\n                    for s in plane.spks\n                )\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(spikes)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        if key[\"extraction_method\"] in (\n            \"caiman_deconvolution\",\n            \"caiman_dff\",\n        ):\n            attr_mapper = {\n                \"caiman_deconvolution\": \"spikes\",\n                \"caiman_dff\": \"dff\",\n            }\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            self.insert1(key)\n            self.Trace.insert(\n                dict(\n                    key,\n                    mask=mask[\"mask_id\"],\n                    fluo_channel=segmentation_channel,\n                    activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                )\n                for mask in caiman_dataset.masks\n            )\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ActivityExtractionMethod", "title": "<code>ActivityExtractionMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available activity extraction methods.</p> <p>Attributes:</p> Name Type Description <code>extraction_method</code> <code>str</code> <p>Extraction method.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass ActivityExtractionMethod(dj.Lookup):\n\"\"\"Available activity extraction methods.\n\n    Attributes:\n        extraction_method (str): Extraction method.\n    \"\"\"\n\n    definition = \"\"\"# Activity extraction method \n    extraction_method: varchar(32)\n    \"\"\"\n\n    contents = zip([\"suite2p_deconvolution\", \"caiman_deconvolution\", \"caiman_dff\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.CellCompartment", "title": "<code>CellCompartment</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)</p> <p>Attributes:</p> Name Type Description <code>cell_compartment</code> <code>str</code> <p>Cell compartment.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass CellCompartment(dj.Lookup):\n\"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)\n\n    Attributes:\n        cell_compartment (str): Cell compartment.\n    \"\"\"\n\n    definition = \"\"\"# Cell compartments\n    cell_compartment: char(16)\n    \"\"\"\n\n    contents = zip([\"axon\", \"soma\", \"bouton\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Curated results</p> <p>If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir.</p> <p>Attributes:</p> Name Type Description <code>Processing</code> <code>foreign key</code> <p>Primary key from Processing.</p> <code>curation_id</code> <code>int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time of generation of this set of curated results.</p> <code>curation_output_dir</code> <code>str</code> <p>Output directory of the curated results, relative to root data directory.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this result.</p> <code>curation_note</code> <code>str</code> <p>Notes about the curation task.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curated results\n\n    If no curation is applied, the curation_output_dir can be set to\n    the value of processing_output_dir.\n\n    Attributes:\n        Processing (foreign key): Primary key from Processing.\n        curation_id (int): Unique curation ID.\n        curation_time (datetime): Time of generation of this set of curated results.\n        curation_output_dir (str): Output directory of the curated results, relative to\n            root data directory.\n        manual_curation (bool): If True, manual curation has been performed on this\n            result.\n        curation_note (str, optional): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"# Curation(s) results\n    -&gt; Processing\n    curation_id: int\n    ---\n    curation_time: datetime  # Time of generation of this set of curated results \n    curation_output_dir: varchar(255)  # Output directory of the curated results, relative to root data directory\n    manual_curation: bool  # Has manual curation been performed on this result?\n    curation_note='': varchar(2000)  \n    \"\"\"\n\n    def create1_from_processing_task(self, key, is_curated=False, curation_note=\"\"):\n\"\"\"Create a Curation entry for a given ProcessingTask key.\n\n        Args:\n            key (dict): Primary key set of an entry in the ProcessingTask table.\n            is_curated (bool): When True, indicates a manual curation.\n            curation_note (str): User's note on the specifics of the curation.\n        \"\"\"\n        if key not in Processing():\n            raise ValueError(\n                f\"No corresponding entry in Processing available for: {key};\"\n                f\"Please run `Processing.populate(key)`\"\n            )\n\n        output_dir = (ProcessingTask &amp; key).fetch1(\"processing_output_dir\")\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n            curation_time = suite2p_dataset.creation_time\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            curation_time = caiman_dataset.creation_time\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            curation_time = extract_dataset.creation_time\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": curation_time,\n                \"curation_output_dir\": output_dir,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Curation.create1_from_processing_task", "title": "<code>create1_from_processing_task(key, is_curated=False, curation_note='')</code>", "text": "<p>Create a Curation entry for a given ProcessingTask key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key set of an entry in the ProcessingTask table.</p> required <code>is_curated</code> <code>bool</code> <p>When True, indicates a manual curation.</p> <code>False</code> <code>curation_note</code> <code>str</code> <p>User's note on the specifics of the curation.</p> <code>''</code> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def create1_from_processing_task(self, key, is_curated=False, curation_note=\"\"):\n\"\"\"Create a Curation entry for a given ProcessingTask key.\n\n    Args:\n        key (dict): Primary key set of an entry in the ProcessingTask table.\n        is_curated (bool): When True, indicates a manual curation.\n        curation_note (str): User's note on the specifics of the curation.\n    \"\"\"\n    if key not in Processing():\n        raise ValueError(\n            f\"No corresponding entry in Processing available for: {key};\"\n            f\"Please run `Processing.populate(key)`\"\n        )\n\n    output_dir = (ProcessingTask &amp; key).fetch1(\"processing_output_dir\")\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n        curation_time = suite2p_dataset.creation_time\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n        curation_time = caiman_dataset.creation_time\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        curation_time = extract_dataset.creation_time\n    else:\n        raise NotImplementedError(\"Unknown method: {}\".format(method))\n\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": curation_time,\n            \"curation_output_dir\": output_dir,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence", "title": "<code>Fluorescence</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Fluorescence(dj.Computed):\n\"\"\"Fluorescence traces.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n    \"\"\"\n\n    definition = \"\"\"# Fluorescence traces before spike extraction or filtering\n    -&gt; Segmentation\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n                comes from.\n            fluorescence (longblob): Fluorescence trace associated with this mask.\n            neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from         \n        ---\n        fluorescence                : longblob  # Fluorescence trace associated with this mask\n        neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            fluo_traces, fluo_chn2_traces = [], []\n            for s2p in suite2p_dataset.planes.values():\n                mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n                for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                    fluo_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"fluo_channel\": 0,\n                            \"fluorescence\": f,\n                            \"neuropil_fluorescence\": fneu,\n                        }\n                    )\n                if len(s2p.F_chan2):\n                    mask_chn2_count = len(\n                        fluo_chn2_traces\n                    )  # increment mask id from all planes\n                    for mask_idx, (f2, fneu2) in enumerate(\n                        zip(s2p.F_chan2, s2p.Fneu_chan2)\n                    ):\n                        fluo_chn2_traces.append(\n                            {\n                                **key,\n                                \"mask\": mask_idx + mask_chn2_count,\n                                \"fluo_channel\": 1,\n                                \"fluorescence\": f2,\n                                \"neuropil_fluorescence\": fneu2,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces + fluo_chn2_traces)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            fluo_traces = []\n            for mask in caiman_dataset.masks:\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask[\"mask_id\"],\n                        \"fluo_channel\": segmentation_channel,\n                        \"fluorescence\": mask[\"inferred_trace\"],\n                    }\n                )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n\n            fluo_traces = [\n                {\n                    **key,\n                    \"mask\": mask_id,\n                    \"fluo_channel\": 0,\n                    \"fluorescence\": fluorescence,\n                }\n                for mask_id, fluorescence in enumerate(extract_dataset.T)\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Traces obtained from segmented region of interests.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>scan.Channel.proj(fluo_channel='channel')</code> <code>int</code> <p>The channel that this trace comes from.</p> <code>fluorescence</code> <code>longblob</code> <p>Fluorescence trace associated with this mask.</p> <code>neuropil_fluorescence</code> <code>longblob</code> <p>Neuropil fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n            comes from.\n        fluorescence (longblob): Fluorescence trace associated with this mask.\n        neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from         \n    ---\n    fluorescence                : longblob  # Fluorescence trace associated with this mask\n    neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Fluorescence with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        fluo_traces, fluo_chn2_traces = [], []\n        for s2p in suite2p_dataset.planes.values():\n            mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n            for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"fluo_channel\": 0,\n                        \"fluorescence\": f,\n                        \"neuropil_fluorescence\": fneu,\n                    }\n                )\n            if len(s2p.F_chan2):\n                mask_chn2_count = len(\n                    fluo_chn2_traces\n                )  # increment mask id from all planes\n                for mask_idx, (f2, fneu2) in enumerate(\n                    zip(s2p.F_chan2, s2p.Fneu_chan2)\n                ):\n                    fluo_chn2_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_chn2_count,\n                            \"fluo_channel\": 1,\n                            \"fluorescence\": f2,\n                            \"neuropil_fluorescence\": fneu2,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces + fluo_chn2_traces)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        fluo_traces = []\n        for mask in caiman_dataset.masks:\n            fluo_traces.append(\n                {\n                    **key,\n                    \"mask\": mask[\"mask_id\"],\n                    \"fluo_channel\": segmentation_channel,\n                    \"fluorescence\": mask[\"inferred_trace\"],\n                }\n            )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n\n        fluo_traces = [\n            {\n                **key,\n                \"mask\": mask_id,\n                \"fluo_channel\": 0,\n                \"fluorescence\": fluorescence,\n            }\n            for mask_id, fluorescence in enumerate(extract_dataset.T)\n        ]\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassification", "title": "<code>MaskClassification</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Classes assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>MaskClassificationMethod</code> <code>foreign key</code> <p>Primary key from MaskClassificationMethod.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass MaskClassification(dj.Computed):\n\"\"\"Classes assigned to each mask.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        MaskClassificationMethod (foreign key): Primary key from\n            MaskClassificationMethod.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Segmentation\n    -&gt; MaskClassificationMethod\n    \"\"\"\n\n    class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n        Attributes:\n            MaskClassification (foreign key): Primary key from MaskClassification.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            MaskType: Primary key from MaskType.\n            confidence (float, optional): Confidence level of the mask classification.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        -&gt; MaskType\n        confidence=null: float\n        \"\"\"\n\n    def make(self, key):\n        pass\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassification.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Type assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>MaskClassification</code> <code>foreign key</code> <p>Primary key from MaskClassification.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>MaskType</code> <code>foreign key</code> <p>Primary key from MaskType.</p> <code>confidence</code> <code>float</code> <p>Confidence level of the mask classification.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n    Attributes:\n        MaskClassification (foreign key): Primary key from MaskClassification.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        MaskType: Primary key from MaskType.\n        confidence (float, optional): Confidence level of the mask classification.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    -&gt; MaskType\n    confidence=null: float\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassificationMethod", "title": "<code>MaskClassificationMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available mask classification methods.</p> <p>Attributes:</p> Name Type Description <code>mask_classification_method</code> <code>str</code> <p>Mask classification method.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass MaskClassificationMethod(dj.Lookup):\n\"\"\"Available mask classification methods.\n\n    Attributes:\n        mask_classification_method (str): Mask classification method.\n    \"\"\"\n\n    definition = \"\"\"\n    mask_classification_method: varchar(48)\n    \"\"\"\n\n    contents = zip([\"suite2p_default_classifier\", \"caiman_default_classifier\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').</p> <p>Attributes:</p> Name Type Description <code>masky_type</code> <code>str</code> <p>Mask type.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass MaskType(dj.Lookup):\n\"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').\n\n    Attributes:\n        masky_type (str): Mask type.\n    \"\"\"\n\n    definition = \"\"\"# Possible types of a segmented mask\n    mask_type: varchar(16)\n    \"\"\"\n\n    contents = zip([\"soma\", \"axon\", \"dendrite\", \"neuropil\", \"artefact\", \"unknown\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection", "title": "<code>MotionCorrection</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Results of motion correction shifts performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Primary key from Curation.</p> <code>scan.Channel.proj(motion_correct_channel='channel')</code> <code>int</code> <p>Channel used for motion correction in this processing task.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass MotionCorrection(dj.Imported):\n\"\"\"Results of motion correction shifts performed on the imaging data.\n\n    Attributes:\n        Curation (foreign key): Primary key from Curation.\n        scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for\n            motion correction in this processing task.\n    \"\"\"\n\n    definition = \"\"\"# Results of motion correction\n    -&gt; Curation\n    ---\n    -&gt; scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task\n    \"\"\"\n\n    class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob): Mask with true for frames with outlier shifts\n                (already corrected).\n            y_shifts (longblob): y motion correction shifts (pixels).\n            x_shifts (longblob): x motion correction shifts (pixels).\n            z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n            y_std (float): standard deviation of y shifts across all frames (pixels).\n            x_std (float): standard deviation of x shifts across all frames (pixels).\n            z_std (float, optional): standard deviation of z shifts across all frames\n                (pixels).\n        \"\"\"\n\n        definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n        y_shifts            : longblob  # (pixels) y motion correction shifts\n        x_shifts            : longblob  # (pixels) x motion correction shifts\n        z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift) \n        y_std               : float     # (pixels) standard deviation of y shifts across all frames\n        x_std               : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n        blocks/patches.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob, null): Mask with true for frames with outlier\n                shifts (already corrected).\n            block_height (int): Block height in pixels.\n            block_width (int): Block width in pixels.\n            block_depth (int): Block depth in pixels.\n            block_count_y (int): Number of blocks tiled in the y direction.\n            block_count_x (int): Number of blocks tiled in the x direction.\n            block_count_z (int): Number of blocks tiled in the z direction.\n        \"\"\"\n\n        definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n        block_height        : int      # (pixels)\n        block_width         : int      # (pixels)\n        block_depth         : int      # (pixels)\n        block_count_y       : int      # number of blocks tiled in the y direction\n        block_count_x       : int      # number of blocks tiled in the x direction\n        block_count_z       : int      # number of blocks tiled in the z direction\n        \"\"\"\n\n    class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n        Attributes:\n            NonRigidMotionCorrection (foreign key): Primary key from\n                NonRigidMotionCorrection.\n            block_id (int): Unique block ID.\n            block_y (longblob): y_start and y_end in pixels for this block\n            block_x (longblob): x_start and x_end in pixels for this block\n            block_z (longblob): z_start and z_end in pixels for this block\n            y_shifts (longblob): y motion correction shifts for every frame in pixels\n            x_shifts (longblob): x motion correction shifts for every frame in pixels\n            z_shift=null (longblob, optional): x motion correction shifts for every frame\n                in pixels\n            y_std (float): standard deviation of y shifts across all frames in pixels\n            x_std (float): standard deviation of x shifts across all frames in pixels\n            z_std=null (float, optional): standard deviation of z shifts across all frames\n                in pixels\n        \"\"\"\n\n        definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n        -&gt; master.NonRigidMotionCorrection\n        block_id        : int\n        ---\n        block_y         : longblob  # (y_start, y_end) in pixel of this block\n        block_x         : longblob  # (x_start, x_end) in pixel of this block\n        block_z         : longblob  # (z_start, z_end) in pixel of this block\n        y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n        x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n        z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n        y_std           : float     # (pixels) standard deviation of y shifts across all frames\n        x_std           : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n            ref_image (longblob): Image used as alignment template.\n            average_image (longblob): Mean of registered frames.\n            correlation_image (longblob, optional): Correlation map (computed during\n                cell detection).\n            max_proj_image (longblob, optional): Max of registered frames.\n        \"\"\"\n\n        definition = \"\"\"# Summary images for each field and channel after corrections\n        -&gt; master\n        -&gt; scan.ScanInfo.Field\n        ---\n        ref_image               : longblob  # image used as alignment template\n        average_image           : longblob  # mean of registered frames\n        correlation_image=null  : longblob  # correlation map (computed during cell detection)\n        max_proj_image=null     : longblob  # max of registered frames\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n            \"KEY\", \"field_z\", order_by=\"field_z\"\n        )\n\n        if method in [\"suite2p\", \"extract\"]:\n            suite2p_dataset = imaging_dataset\n\n            motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n            # ---- iterate through all s2p plane outputs ----\n            rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n            summary_images = []\n            for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n                # -- rigid motion correction --\n                if idx == 0:\n                    rigid_correction = {\n                        **key,\n                        \"y_shifts\": s2p.ops[\"yoff\"],\n                        \"x_shifts\": s2p.ops[\"xoff\"],\n                        \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                        \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                        \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                        \"z_std\": np.nan,\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    rigid_correction[\"y_shifts\"] = np.vstack(\n                        [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                    )\n                    rigid_correction[\"y_std\"] = np.nanstd(\n                        rigid_correction[\"y_shifts\"].flatten()\n                    )\n                    rigid_correction[\"x_shifts\"] = np.vstack(\n                        [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                    )\n                    rigid_correction[\"x_std\"] = np.nanstd(\n                        rigid_correction[\"x_shifts\"].flatten()\n                    )\n                    rigid_correction[\"outlier_frames\"] = np.logical_or(\n                        rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                    )\n                # -- non-rigid motion correction --\n                if s2p.ops[\"nonrigid\"]:\n                    if idx == 0:\n                        nonrigid_correction = {\n                            **key,\n                            \"block_height\": s2p.ops[\"block_size\"][0],\n                            \"block_width\": s2p.ops[\"block_size\"][1],\n                            \"block_depth\": 1,\n                            \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                            \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                            \"block_count_z\": len(suite2p_dataset.planes),\n                            \"outlier_frames\": s2p.ops[\"badframes\"],\n                        }\n                    else:\n                        nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                            nonrigid_correction[\"outlier_frames\"],\n                            s2p.ops[\"badframes\"],\n                        )\n                    for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                        zip(\n                            s2p.ops[\"xblock\"],\n                            s2p.ops[\"yblock\"],\n                            s2p.ops[\"yoff1\"].T,\n                            s2p.ops[\"xoff1\"].T,\n                        )\n                    ):\n                        if b_id in nonrigid_blocks:\n                            nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                            )\n                            nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                            )\n                            nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                            )\n                            nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                            )\n                        else:\n                            nonrigid_blocks[b_id] = {\n                                **key,\n                                \"block_id\": b_id,\n                                \"block_y\": b_y,\n                                \"block_x\": b_x,\n                                \"block_z\": np.full_like(b_x, plane),\n                                \"y_shifts\": bshift_y,\n                                \"x_shifts\": bshift_x,\n                                \"z_shifts\": np.full(\n                                    (\n                                        len(suite2p_dataset.planes),\n                                        len(bshift_x),\n                                    ),\n                                    0,\n                                ),\n                                \"y_std\": np.nanstd(bshift_y),\n                                \"x_std\": np.nanstd(bshift_x),\n                                \"z_std\": np.nan,\n                            }\n\n                # -- summary images --\n                motion_correction_key = (\n                    scan.ScanInfo.Field * Curation &amp; key &amp; field_keys[plane]\n                ).fetch1(\"KEY\")\n                summary_images.append(\n                    {\n                        **motion_correction_key,\n                        \"ref_image\": s2p.ref_image,\n                        \"average_image\": s2p.mean_image,\n                        \"correlation_image\": s2p.correlation_map,\n                        \"max_proj_image\": s2p.max_proj_image,\n                    }\n                )\n\n            self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n            if rigid_correction:\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            if nonrigid_correction:\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks.values())\n            self.Summary.insert(summary_images)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            self.insert1(\n                {\n                    **key,\n                    \"motion_correct_channel\": caiman_dataset.alignment_channel,\n                }\n            )\n\n            is3D = caiman_dataset.params.motion[\"is3D\"]\n            if not caiman_dataset.params.motion[\"pw_rigid\"]:\n                # -- rigid motion correction --\n                rigid_correction = {\n                    **key,\n                    \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                    \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                    \"z_shifts\": (\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                        if is3D\n                        else np.full_like(\n                            caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                            0,\n                        )\n                    ),\n                    \"x_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                    ),\n                    \"y_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                    ),\n                    \"z_std\": (\n                        np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                        if is3D\n                        else np.nan\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            else:\n                # -- non-rigid motion correction --\n                nonrigid_correction = {\n                    **key,\n                    \"block_height\": (\n                        caiman_dataset.params.motion[\"strides\"][0]\n                        + caiman_dataset.params.motion[\"overlaps\"][0]\n                    ),\n                    \"block_width\": (\n                        caiman_dataset.params.motion[\"strides\"][1]\n                        + caiman_dataset.params.motion[\"overlaps\"][1]\n                    ),\n                    \"block_depth\": (\n                        caiman_dataset.params.motion[\"strides\"][2]\n                        + caiman_dataset.params.motion[\"overlaps\"][2]\n                        if is3D\n                        else 1\n                    ),\n                    \"block_count_x\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                    ),\n                    \"block_count_y\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                    ),\n                    \"block_count_z\": (\n                        len(\n                            set(\n                                caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    :, 4\n                                ]\n                            )\n                        )\n                        if is3D\n                        else 1\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                nonrigid_blocks = []\n                for b_id in range(\n                    len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n                ):\n                    nonrigid_blocks.append(\n                        {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_x\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 0:2\n                                ]\n                            ),\n                            \"block_y\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 2:4\n                                ]\n                            ),\n                            \"block_z\": (\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 4:6]\n                                )\n                                if is3D\n                                else np.full_like(\n                                    np.arange(\n                                        *caiman_dataset.motion_correction[\n                                            \"coord_shifts_els\"\n                                        ][b_id, 0:2]\n                                    ),\n                                    0,\n                                )\n                            ),\n                            \"x_shifts\": caiman_dataset.motion_correction[\n                                \"x_shifts_els\"\n                            ][:, b_id],\n                            \"y_shifts\": caiman_dataset.motion_correction[\n                                \"y_shifts_els\"\n                            ][:, b_id],\n                            \"z_shifts\": (\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                                if is3D\n                                else np.full_like(\n                                    caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                        :, b_id\n                                    ],\n                                    0,\n                                )\n                            ),\n                            \"x_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"y_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"z_std\": (\n                                np.nanstd(\n                                    caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                        :, b_id\n                                    ]\n                                )\n                                if is3D\n                                else np.nan\n                            ),\n                        }\n                    )\n\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks)\n\n            # -- summary images --\n            summary_images = [\n                {\n                    **key,\n                    **fkey,\n                    \"ref_image\": ref_image,\n                    \"average_image\": ave_img,\n                    \"correlation_image\": corr_img,\n                    \"max_proj_image\": max_img,\n                }\n                for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                    field_keys,\n                    caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"reference_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"average_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"max_image\"][...][\n                        np.newaxis, ...\n                    ],\n                )\n            ]\n            self.Summary.insert(summary_images)\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.Block", "title": "<code>Block</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>FOV-tiled blocks used for non-rigid motion correction.</p> <p>Attributes:</p> Name Type Description <code>NonRigidMotionCorrection</code> <code>foreign key</code> <p>Primary key from NonRigidMotionCorrection.</p> <code>block_id</code> <code>int</code> <p>Unique block ID.</p> <code>block_y</code> <code>longblob</code> <p>y_start and y_end in pixels for this block</p> <code>block_x</code> <code>longblob</code> <p>x_start and x_end in pixels for this block</p> <code>block_z</code> <code>longblob</code> <p>z_start and z_end in pixels for this block</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts for every frame in pixels</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>z_shift=null</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames in pixels</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames in pixels</p> <code>z_std=null</code> <code>float</code> <p>standard deviation of z shifts across all frames in pixels</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n    Attributes:\n        NonRigidMotionCorrection (foreign key): Primary key from\n            NonRigidMotionCorrection.\n        block_id (int): Unique block ID.\n        block_y (longblob): y_start and y_end in pixels for this block\n        block_x (longblob): x_start and x_end in pixels for this block\n        block_z (longblob): z_start and z_end in pixels for this block\n        y_shifts (longblob): y motion correction shifts for every frame in pixels\n        x_shifts (longblob): x motion correction shifts for every frame in pixels\n        z_shift=null (longblob, optional): x motion correction shifts for every frame\n            in pixels\n        y_std (float): standard deviation of y shifts across all frames in pixels\n        x_std (float): standard deviation of x shifts across all frames in pixels\n        z_std=null (float, optional): standard deviation of z shifts across all frames\n            in pixels\n    \"\"\"\n\n    definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n    -&gt; master.NonRigidMotionCorrection\n    block_id        : int\n    ---\n    block_y         : longblob  # (y_start, y_end) in pixel of this block\n    block_x         : longblob  # (x_start, x_end) in pixel of this block\n    block_z         : longblob  # (z_start, z_end) in pixel of this block\n    y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n    x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n    z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n    y_std           : float     # (pixels) standard deviation of y shifts across all frames\n    x_std           : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.NonRigidMotionCorrection", "title": "<code>NonRigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob, null</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>block_height</code> <code>int</code> <p>Block height in pixels.</p> <code>block_width</code> <code>int</code> <p>Block width in pixels.</p> <code>block_depth</code> <code>int</code> <p>Block depth in pixels.</p> <code>block_count_y</code> <code>int</code> <p>Number of blocks tiled in the y direction.</p> <code>block_count_x</code> <code>int</code> <p>Number of blocks tiled in the x direction.</p> <code>block_count_z</code> <code>int</code> <p>Number of blocks tiled in the z direction.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n    blocks/patches.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob, null): Mask with true for frames with outlier\n            shifts (already corrected).\n        block_height (int): Block height in pixels.\n        block_width (int): Block width in pixels.\n        block_depth (int): Block depth in pixels.\n        block_count_y (int): Number of blocks tiled in the y direction.\n        block_count_x (int): Number of blocks tiled in the x direction.\n        block_count_z (int): Number of blocks tiled in the z direction.\n    \"\"\"\n\n    definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n    block_height        : int      # (pixels)\n    block_width         : int      # (pixels)\n    block_depth         : int      # (pixels)\n    block_count_y       : int      # number of blocks tiled in the y direction\n    block_count_x       : int      # number of blocks tiled in the x direction\n    block_count_z       : int      # number of blocks tiled in the z direction\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.RigidMotionCorrection", "title": "<code>RigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of rigid motion correction performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts (pixels).</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts (pixels).</p> <code>z_shifts</code> <code>longblob</code> <p>z motion correction shifts (z-drift, pixels).</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames (pixels).</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames (pixels).</p> <code>z_std</code> <code>float</code> <p>standard deviation of z shifts across all frames (pixels).</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob): Mask with true for frames with outlier shifts\n            (already corrected).\n        y_shifts (longblob): y motion correction shifts (pixels).\n        x_shifts (longblob): x motion correction shifts (pixels).\n        z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n        y_std (float): standard deviation of y shifts across all frames (pixels).\n        x_std (float): standard deviation of x shifts across all frames (pixels).\n        z_std (float, optional): standard deviation of z shifts across all frames\n            (pixels).\n    \"\"\"\n\n    definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n    y_shifts            : longblob  # (pixels) y motion correction shifts\n    x_shifts            : longblob  # (pixels) x motion correction shifts\n    z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift) \n    y_std               : float     # (pixels) standard deviation of y shifts across all frames\n    x_std               : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.Summary", "title": "<code>Summary</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Summary images for each field and channel after corrections.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>scan.ScanInfo.Field</code> <code>foreign key</code> <p>Primary key from scan.ScanInfo.Field.</p> <code>ref_image</code> <code>longblob</code> <p>Image used as alignment template.</p> <code>average_image</code> <code>longblob</code> <p>Mean of registered frames.</p> <code>correlation_image</code> <code>longblob</code> <p>Correlation map (computed during cell detection).</p> <code>max_proj_image</code> <code>longblob</code> <p>Max of registered frames.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n        ref_image (longblob): Image used as alignment template.\n        average_image (longblob): Mean of registered frames.\n        correlation_image (longblob, optional): Correlation map (computed during\n            cell detection).\n        max_proj_image (longblob, optional): Max of registered frames.\n    \"\"\"\n\n    definition = \"\"\"# Summary images for each field and channel after corrections\n    -&gt; master\n    -&gt; scan.ScanInfo.Field\n    ---\n    ref_image               : longblob  # image used as alignment template\n    average_image           : longblob  # mean of registered frames\n    correlation_image=null  : longblob  # correlation map (computed during cell detection)\n    max_proj_image=null     : longblob  # max of registered frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.make", "title": "<code>make(key)</code>", "text": "<p>Populate MotionCorrection with results parsed from analysis outputs</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n        \"KEY\", \"field_z\", order_by=\"field_z\"\n    )\n\n    if method in [\"suite2p\", \"extract\"]:\n        suite2p_dataset = imaging_dataset\n\n        motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n        # ---- iterate through all s2p plane outputs ----\n        rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n        summary_images = []\n        for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n            # -- rigid motion correction --\n            if idx == 0:\n                rigid_correction = {\n                    **key,\n                    \"y_shifts\": s2p.ops[\"yoff\"],\n                    \"x_shifts\": s2p.ops[\"xoff\"],\n                    \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                    \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                    \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                    \"z_std\": np.nan,\n                    \"outlier_frames\": s2p.ops[\"badframes\"],\n                }\n            else:\n                rigid_correction[\"y_shifts\"] = np.vstack(\n                    [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                )\n                rigid_correction[\"y_std\"] = np.nanstd(\n                    rigid_correction[\"y_shifts\"].flatten()\n                )\n                rigid_correction[\"x_shifts\"] = np.vstack(\n                    [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                )\n                rigid_correction[\"x_std\"] = np.nanstd(\n                    rigid_correction[\"x_shifts\"].flatten()\n                )\n                rigid_correction[\"outlier_frames\"] = np.logical_or(\n                    rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                )\n            # -- non-rigid motion correction --\n            if s2p.ops[\"nonrigid\"]:\n                if idx == 0:\n                    nonrigid_correction = {\n                        **key,\n                        \"block_height\": s2p.ops[\"block_size\"][0],\n                        \"block_width\": s2p.ops[\"block_size\"][1],\n                        \"block_depth\": 1,\n                        \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                        \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                        \"block_count_z\": len(suite2p_dataset.planes),\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                        nonrigid_correction[\"outlier_frames\"],\n                        s2p.ops[\"badframes\"],\n                    )\n                for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                    zip(\n                        s2p.ops[\"xblock\"],\n                        s2p.ops[\"yblock\"],\n                        s2p.ops[\"yoff1\"].T,\n                        s2p.ops[\"xoff1\"].T,\n                    )\n                ):\n                    if b_id in nonrigid_blocks:\n                        nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                        )\n                        nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                        )\n                        nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                        )\n                        nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                        )\n                    else:\n                        nonrigid_blocks[b_id] = {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_y\": b_y,\n                            \"block_x\": b_x,\n                            \"block_z\": np.full_like(b_x, plane),\n                            \"y_shifts\": bshift_y,\n                            \"x_shifts\": bshift_x,\n                            \"z_shifts\": np.full(\n                                (\n                                    len(suite2p_dataset.planes),\n                                    len(bshift_x),\n                                ),\n                                0,\n                            ),\n                            \"y_std\": np.nanstd(bshift_y),\n                            \"x_std\": np.nanstd(bshift_x),\n                            \"z_std\": np.nan,\n                        }\n\n            # -- summary images --\n            motion_correction_key = (\n                scan.ScanInfo.Field * Curation &amp; key &amp; field_keys[plane]\n            ).fetch1(\"KEY\")\n            summary_images.append(\n                {\n                    **motion_correction_key,\n                    \"ref_image\": s2p.ref_image,\n                    \"average_image\": s2p.mean_image,\n                    \"correlation_image\": s2p.correlation_map,\n                    \"max_proj_image\": s2p.max_proj_image,\n                }\n            )\n\n        self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n        if rigid_correction:\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        if nonrigid_correction:\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks.values())\n        self.Summary.insert(summary_images)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        self.insert1(\n            {\n                **key,\n                \"motion_correct_channel\": caiman_dataset.alignment_channel,\n            }\n        )\n\n        is3D = caiman_dataset.params.motion[\"is3D\"]\n        if not caiman_dataset.params.motion[\"pw_rigid\"]:\n            # -- rigid motion correction --\n            rigid_correction = {\n                **key,\n                \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                \"z_shifts\": (\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                    if is3D\n                    else np.full_like(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                        0,\n                    )\n                ),\n                \"x_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                ),\n                \"y_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                ),\n                \"z_std\": (\n                    np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                    if is3D\n                    else np.nan\n                ),\n                \"outlier_frames\": None,\n            }\n\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        else:\n            # -- non-rigid motion correction --\n            nonrigid_correction = {\n                **key,\n                \"block_height\": (\n                    caiman_dataset.params.motion[\"strides\"][0]\n                    + caiman_dataset.params.motion[\"overlaps\"][0]\n                ),\n                \"block_width\": (\n                    caiman_dataset.params.motion[\"strides\"][1]\n                    + caiman_dataset.params.motion[\"overlaps\"][1]\n                ),\n                \"block_depth\": (\n                    caiman_dataset.params.motion[\"strides\"][2]\n                    + caiman_dataset.params.motion[\"overlaps\"][2]\n                    if is3D\n                    else 1\n                ),\n                \"block_count_x\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                ),\n                \"block_count_y\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                ),\n                \"block_count_z\": (\n                    len(\n                        set(\n                            caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                :, 4\n                            ]\n                        )\n                    )\n                    if is3D\n                    else 1\n                ),\n                \"outlier_frames\": None,\n            }\n\n            nonrigid_blocks = []\n            for b_id in range(\n                len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n            ):\n                nonrigid_blocks.append(\n                    {\n                        **key,\n                        \"block_id\": b_id,\n                        \"block_x\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 0:2\n                            ]\n                        ),\n                        \"block_y\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 2:4\n                            ]\n                        ),\n                        \"block_z\": (\n                            np.arange(\n                                *caiman_dataset.motion_correction[\n                                    \"coord_shifts_els\"\n                                ][b_id, 4:6]\n                            )\n                            if is3D\n                            else np.full_like(\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 0:2]\n                                ),\n                                0,\n                            )\n                        ),\n                        \"x_shifts\": caiman_dataset.motion_correction[\n                            \"x_shifts_els\"\n                        ][:, b_id],\n                        \"y_shifts\": caiman_dataset.motion_correction[\n                            \"y_shifts_els\"\n                        ][:, b_id],\n                        \"z_shifts\": (\n                            caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                :, b_id\n                            ]\n                            if is3D\n                            else np.full_like(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ],\n                                0,\n                            )\n                        ),\n                        \"x_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"y_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"z_std\": (\n                            np.nanstd(\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                            )\n                            if is3D\n                            else np.nan\n                        ),\n                    }\n                )\n\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks)\n\n        # -- summary images --\n        summary_images = [\n            {\n                **key,\n                **fkey,\n                \"ref_image\": ref_image,\n                \"average_image\": ave_img,\n                \"correlation_image\": corr_img,\n                \"max_proj_image\": max_img,\n            }\n            for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                field_keys,\n                caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"reference_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"average_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"max_image\"][...][\n                    np.newaxis, ...\n                ],\n            )\n        ]\n        self.Summary.insert(summary_images)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Preprocess", "title": "<code>Preprocess</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Perform the computation of an entry (task) defined in the PreprocessTask table.</p> <ul> <li>If <code>task_mode == \"none\"</code>: no pre-processing performed</li> <li>If <code>task_mode == \"trigger\"</code>: Not implemented</li> <li>If <code>task_mode == \"load\"</code>: Not implemented</li> </ul> <p>Attributes:</p> Name Type Description <code>PreprocessTask</code> <code>foreign key</code> <code>preprocess_time</code> <code>datetime</code> <code>package_version</code> <code>str</code> <p>Version of the analysis package used in processing the data.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Preprocess(dj.Imported):\n\"\"\"Perform the computation of an entry (task) defined in the PreprocessTask table.\n\n    + If `task_mode == \"none\"`: no pre-processing performed\n    + If `task_mode == \"trigger\"`: Not implemented\n    + If `task_mode == \"load\"`: Not implemented\n\n    Attributes:\n        PreprocessTask (foreign key):\n        preprocess_time (datetime, optional):\n        package_version (str, optional): Version of the analysis package used in\n            processing the data.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PreprocessTask\n    ---\n    preprocess_time=null: datetime  # Time of generation of pre-processing results \n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Execute the preprocessing analysis steps defined in PreprocessTask.\"\"\"\n\n        task_mode, output_dir = (PreprocessTask &amp; key).fetch1(\n            \"task_mode\", \"preprocess_output_dir\"\n        )\n        preprocess_output_dir = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n        if task_mode == \"none\":\n            print(f\"No pre-processing run on entry: {key}\")\n        elif task_mode in [\"load\", \"trigger\"]:\n            raise NotImplementedError(\n                \"Pre-processing steps are not implemented.\"\n                \"Please overwrite this `make` function with\"\n                \"desired pre-processing steps.\"\n            )\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Preprocess.make", "title": "<code>make(key)</code>", "text": "<p>Execute the preprocessing analysis steps defined in PreprocessTask.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Execute the preprocessing analysis steps defined in PreprocessTask.\"\"\"\n\n    task_mode, output_dir = (PreprocessTask &amp; key).fetch1(\n        \"task_mode\", \"preprocess_output_dir\"\n    )\n    preprocess_output_dir = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n    if task_mode == \"none\":\n        print(f\"No pre-processing run on entry: {key}\")\n    elif task_mode in [\"load\", \"trigger\"]:\n        raise NotImplementedError(\n            \"Pre-processing steps are not implemented.\"\n            \"Please overwrite this `make` function with\"\n            \"desired pre-processing steps.\"\n        )\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessMethod", "title": "<code>PreprocessMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Method(s) used for preprocessing of calcium imaging data.</p> <p>Attributes:</p> Name Type Description <code>preprocess_method</code> <code>str</code> <p>Preprocessing method.</p> <code>preprocess_method_desc</code> <code>str</code> <p>Processing method description.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass PreprocessMethod(dj.Lookup):\n\"\"\"Method(s) used for preprocessing of calcium imaging data.\n\n    Attributes:\n        preprocess_method (str): Preprocessing method.\n        preprocess_method_desc (str): Processing method description.\n    \"\"\"\n\n    definition = \"\"\"  #  Method/package used for pre-processing\n    preprocess_method: varchar(16)\n    ---\n    preprocess_method_desc: varchar(1000)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSet", "title": "<code>PreprocessParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for the preprocessing of the calcium imaging scans.</p> <p>A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Uniqiue parameter set ID.</p> <code>PreprocessMethod</code> <code>foreign key</code> <p>A primary key from PreprocessMethod.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>param_set_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameter Set, a dictionary of all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass PreprocessParamSet(dj.Lookup):\n\"\"\"Parameter set used for the preprocessing of the calcium imaging scans.\n\n    A hash of the parameters of the analysis suite is also stored in order\n    to avoid duplicated entries.\n\n    Attributes:\n        paramset_idx (int): Uniqiue parameter set ID.\n        PreprocessMethod (foreign key): A primary key from PreprocessMethod.\n        paramset_desc (str): Parameter set description.\n        param_set_hash (uuid): A universally unique identifier for the parameter set.\n        params (longblob): Parameter Set, a dictionary of all applicable parameters to\n            the analysis suite.\n    \"\"\"\n\n    definition = \"\"\"  #  Parameter set used for pre-processing of calcium imaging data\n    paramset_idx:  smallint\n    ---\n    -&gt; PreprocessMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        preprocess_method: str,\n        paramset_idx: int,\n        paramset_desc: str,\n        params: dict,\n    ):\n\"\"\"Insert a parameter set into PreprocessParamSet table.\n        This function automizes the parameter set hashing and avoids insertion of an\n            existing parameter set.\n\n        Attributes:\n            preprocess_method (str): Method used for processing of calcium imaging scans.\n            paramset_idx (int): Uniqiue parameter set ID.\n            paramset_desc (str): Parameter set description.\n            params (dict): Parameter Set, all applicable parameters.\n        \"\"\"\n        param_dict = {\n            \"preprocess_method\": preprocess_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if q_param:  # If the specified param-set already exists\n            pname = q_param.fetch1(\"paramset_idx\")\n            if pname == paramset_idx:  # If the existed set has the same name: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set already exists - name: {}\".format(pname)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSet.insert_new_params", "title": "<code>insert_new_params(preprocess_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Insert a parameter set into PreprocessParamSet table. This function automizes the parameter set hashing and avoids insertion of an     existing parameter set.</p> <p>Attributes:</p> Name Type Description <code>preprocess_method</code> <code>str</code> <p>Method used for processing of calcium imaging scans.</p> <code>paramset_idx</code> <code>int</code> <p>Uniqiue parameter set ID.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>params</code> <code>dict</code> <p>Parameter Set, all applicable parameters.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    preprocess_method: str,\n    paramset_idx: int,\n    paramset_desc: str,\n    params: dict,\n):\n\"\"\"Insert a parameter set into PreprocessParamSet table.\n    This function automizes the parameter set hashing and avoids insertion of an\n        existing parameter set.\n\n    Attributes:\n        preprocess_method (str): Method used for processing of calcium imaging scans.\n        paramset_idx (int): Uniqiue parameter set ID.\n        paramset_desc (str): Parameter set description.\n        params (dict): Parameter Set, all applicable parameters.\n    \"\"\"\n    param_dict = {\n        \"preprocess_method\": preprocess_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if q_param:  # If the specified param-set already exists\n        pname = q_param.fetch1(\"paramset_idx\")\n        if pname == paramset_idx:  # If the existed set has the same name: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set already exists - name: {}\".format(pname)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSteps", "title": "<code>PreprocessParamSteps</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Ordered list of paramset_idx that will be run.</p> <p>When pre-processing is not performed, do not create an entry in <code>Step</code> Part table</p> <p>Attributes:</p> Name Type Description <code>preprocess_param_steps_id</code> <code>int</code> <code>preprocess_param_steps_name</code> <code>str</code> <code>preprocess_param_steps_desc</code> <code>str</code> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass PreprocessParamSteps(dj.Manual):\n\"\"\"Ordered list of paramset_idx that will be run.\n\n    When pre-processing is not performed, do not create an entry in `Step` Part table\n\n    Attributes:\n        preprocess_param_steps_id (int):\n        preprocess_param_steps_name (str):\n        preprocess_param_steps_desc (str):\n    \"\"\"\n\n    definition = \"\"\"\n    preprocess_param_steps_id: smallint\n    ---\n    preprocess_param_steps_name: varchar(32)\n    preprocess_param_steps_desc: varchar(128)\n    \"\"\"\n\n    class Step(dj.Part):\n\"\"\"ADD DEFINITION\n\n        Attributes:\n            PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps.\n            step_number (int):\n            PreprocessParamSet (foreign key): A primary key from PreprocessParamSet.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        step_number: smallint                  # Order of operations\n        ---\n        -&gt; PreprocessParamSet\n        \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSteps.Step", "title": "<code>Step</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>ADD DEFINITION</p> <p>Attributes:</p> Name Type Description <code>PreprocessParamSteps</code> <code>foreign key</code> <p>A primary key from PreprocessParamSteps.</p> <code>step_number</code> <code>int</code> <code>PreprocessParamSet</code> <code>foreign key</code> <p>A primary key from PreprocessParamSet.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Step(dj.Part):\n\"\"\"ADD DEFINITION\n\n    Attributes:\n        PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps.\n        step_number (int):\n        PreprocessParamSet (foreign key): A primary key from PreprocessParamSet.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    step_number: smallint                  # Order of operations\n    ---\n    -&gt; PreprocessParamSet\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessTask", "title": "<code>PreprocessTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>This table defines a calcium imaging preprocessing task for a combination of a <code>Scan</code> and a <code>PreprocessParamSteps</code> entries, including all the inputs (scan, method, steps). The task defined here is then run in the downstream table Preprocess. This table supports definitions of both loading of pre-generated, results, triggering of new analysis, or skipping of preprocessing step.</p> <p>Attributes:</p> Name Type Description <code>Scan</code> <code>foreign key</code> <p>A primary key from Scan.</p> <code>PreprocessParamSteps</code> <code>foreign key</code> <p>A primary key from PreprocessParamSteps.</p> <code>preprocess_output_dir</code> <code>str</code> <p>Output directory for the results of preprocessing.</p> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results), 'trigger' (trigger computation), 'none' (no pre-processing). Default none.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass PreprocessTask(dj.Manual):\n\"\"\"This table defines a calcium imaging preprocessing task for a combination of a\n    `Scan` and a `PreprocessParamSteps` entries, including all the inputs (scan, method,\n    steps). The task defined here is then run in the downstream table\n    Preprocess. This table supports definitions of both loading of pre-generated,\n    results, triggering of new analysis, or skipping of preprocessing step.\n\n    Attributes:\n        Scan (foreign key): A primary key from Scan.\n        PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps.\n        preprocess_output_dir (str): Output directory for the results of preprocessing.\n        task_mode (str, optional): One of 'load' (load computed analysis results), 'trigger'\n            (trigger computation), 'none' (no pre-processing). Default none.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a pre-processing task ready to be run\n    -&gt; scan.Scan\n    -&gt; PreprocessParamSteps\n    ---\n    preprocess_output_dir: varchar(255)  # Pre-processing output directory relative \n                                         # to the root data directory\n    task_mode='none': enum('none','load', 'trigger') # 'none': no pre-processing\n                                                     # 'load': load analysis results\n                                                     # 'trigger': trigger computation\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing", "title": "<code>Processing</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted.</p> <p>Attributes:</p> Name Type Description <code>ProcessingTask</code> <code>foreign key</code> <p>Primary key from ProcessingTask.</p> <code>processing_time</code> <code>datetime</code> <p>Process completion datetime.</p> <code>package_version</code> <code>str</code> <p>Version of the analysis package used in processing the data.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Processing(dj.Computed):\n\"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table.\n    The computation is performed only on the scans with ScanInfo inserted.\n\n    Attributes:\n        ProcessingTask (foreign key): Primary key from ProcessingTask.\n        processing_time (datetime): Process completion datetime.\n        package_version (str, optional): Version of the analysis package used in\n            processing the data.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ProcessingTask\n    ---\n    processing_time     : datetime  # Time of generation of this set of processed, segmented results\n    package_version=''  : varchar(16)\n    \"\"\"\n\n    # Run processing only on Scan with ScanInfo inserted\n    @property\n    def key_source(self):\n\"\"\"Limit the Processing to Scans that have their metadata ingested to the\n        database.\"\"\"\n\n        return ProcessingTask &amp; scan.ScanInfo\n\n    def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n        task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n            \"task_mode\", \"processing_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update processing_output_dir\n            ProcessingTask.update1(\n                {**key, \"processing_output_dir\": output_dir.as_posix()}\n            )\n        output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n        if task_mode == \"load\":\n            method, imaging_dataset = get_loader_result(key, ProcessingTask)\n            if method == \"suite2p\":\n                if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                    raise NotImplementedError(\n                        f\"Suite2p ingestion error - Unable to handle\"\n                        f\" ScanImage multi-ROI scanning mode yet\"\n                    )\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n            elif method == \"caiman\":\n                caiman_dataset = imaging_dataset\n                key = {**key, \"processing_time\": caiman_dataset.creation_time}\n            elif method == \"extract\":\n                raise NotImplementedError(\n                    \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n                )\n            else:\n                raise NotImplementedError(\"Unknown method: {}\".format(method))\n        elif task_mode == \"trigger\":\n\n            method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n                \"processing_method\"\n            )\n\n            preprocess_paramsets = (\n                PreprocessParamSteps.Step()\n                &amp; dict(preprocess_param_steps_id=key[\"preprocess_param_steps_id\"])\n            ).fetch(\"paramset_idx\")\n\n            if len(preprocess_paramsets) == 0:\n                # No pre-processing steps were performed on the acquired dataset, so process the raw/acquired files.\n                image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n                image_files = [\n                    find_full_path(get_imaging_root_data_dir(), image_file)\n                    for image_file in image_files\n                ]\n\n            else:\n                preprocess_output_dir = (PreprocessTask &amp; key).fetch1(\n                    \"preprocess_output_dir\"\n                )\n\n                preprocess_output_dir = find_full_path(\n                    get_imaging_root_data_dir(), preprocess_output_dir\n                )\n\n                if not preprocess_output_dir.exists():\n                    raise FileNotFoundError(\n                        f\"Pre-processed output directory not found ({preprocess_output_dir})\"\n                    )\n\n                image_files = list(preprocess_output_dir.glob(\"*.tif\"))\n\n            if method == \"suite2p\":\n                import suite2p\n\n                suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                suite2p_params[\"save_path0\"] = output_dir\n                (\n                    suite2p_params[\"fs\"],\n                    suite2p_params[\"nplanes\"],\n                    suite2p_params[\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                suite2p_params[\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n            elif method == \"caiman\":\n                from element_interface.run_caiman import run_caiman\n                from element_interface.caiman_loader import (\n                    _process_scanimage_tiff,\n                )\n\n                caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                    \"fps\", \"ndepths\", \"nchannels\"\n                )\n\n                is3D = bool(ndepths &gt; 1)\n                if is3D:\n                    raise NotImplementedError(\n                        \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                    )\n\n                # handle multi-channel tiff image before running CaImAn\n                if nchannels &gt; 1:\n                    channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                    tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                    tmp_dir.mkdir(exist_ok=True)\n                    _process_scanimage_tiff(\n                        [f.as_posix() for f in image_files], output_dir=tmp_dir\n                    )\n                    image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n                run_caiman(\n                    file_paths=[f.as_posix() for f in image_files],\n                    parameters=caiman_params,\n                    sampling_rate=sampling_rate,\n                    output_dir=output_dir,\n                    is3D=is3D,\n                )\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                caiman_dataset = imaging_dataset\n                key[\"processing_time\"] = caiman_dataset.creation_time\n\n            elif method == \"extract\":\n                import suite2p\n                from scipy.io import savemat\n                from element_interface.extract_trigger import EXTRACT_trigger\n\n                # Motion Correction with Suite2p\n                params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n                params[\"suite2p\"][\"save_path0\"] = output_dir\n                (\n                    params[\"suite2p\"][\"fs\"],\n                    params[\"suite2p\"][\"nplanes\"],\n                    params[\"suite2p\"][\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n                # Convert data.bin to registered_scans.mat\n                scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n                data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                    \"nframes\", \"px_height\", \"px_width\"\n                )\n                data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n                scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n                # Save the motion corrected movie (data.bin) in a .mat file\n                savemat(\n                    scan_matlab_fullpath,\n                    {\"M\": np.transpose(data, axes=[1, 2, 0])},\n                )\n\n                # Execute EXTRACT\n\n                ex = EXTRACT_trigger(\n                    scan_matlab_fullpath, params[\"extract\"], output_dir\n                )\n                ex.run()\n\n                _, extract_dataset = get_loader_result(key, ProcessingTask)\n                key[\"processing_time\"] = extract_dataset.creation_time\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the Processing to Scans that have their metadata ingested to the database.</p>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing.make", "title": "<code>make(key)</code>", "text": "<p>Execute the calcium imaging analysis defined by the ProcessingTask.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n    task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n        \"task_mode\", \"processing_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update processing_output_dir\n        ProcessingTask.update1(\n            {**key, \"processing_output_dir\": output_dir.as_posix()}\n        )\n    output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n    if task_mode == \"load\":\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n        if method == \"suite2p\":\n            if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                raise NotImplementedError(\n                    f\"Suite2p ingestion error - Unable to handle\"\n                    f\" ScanImage multi-ROI scanning mode yet\"\n                )\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            key = {**key, \"processing_time\": caiman_dataset.creation_time}\n        elif method == \"extract\":\n            raise NotImplementedError(\n                \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n            )\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n    elif task_mode == \"trigger\":\n\n        method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n            \"processing_method\"\n        )\n\n        preprocess_paramsets = (\n            PreprocessParamSteps.Step()\n            &amp; dict(preprocess_param_steps_id=key[\"preprocess_param_steps_id\"])\n        ).fetch(\"paramset_idx\")\n\n        if len(preprocess_paramsets) == 0:\n            # No pre-processing steps were performed on the acquired dataset, so process the raw/acquired files.\n            image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n            image_files = [\n                find_full_path(get_imaging_root_data_dir(), image_file)\n                for image_file in image_files\n            ]\n\n        else:\n            preprocess_output_dir = (PreprocessTask &amp; key).fetch1(\n                \"preprocess_output_dir\"\n            )\n\n            preprocess_output_dir = find_full_path(\n                get_imaging_root_data_dir(), preprocess_output_dir\n            )\n\n            if not preprocess_output_dir.exists():\n                raise FileNotFoundError(\n                    f\"Pre-processed output directory not found ({preprocess_output_dir})\"\n                )\n\n            image_files = list(preprocess_output_dir.glob(\"*.tif\"))\n\n        if method == \"suite2p\":\n            import suite2p\n\n            suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            suite2p_params[\"save_path0\"] = output_dir\n            (\n                suite2p_params[\"fs\"],\n                suite2p_params[\"nplanes\"],\n                suite2p_params[\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            suite2p_params[\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n        elif method == \"caiman\":\n            from element_interface.run_caiman import run_caiman\n            from element_interface.caiman_loader import (\n                _process_scanimage_tiff,\n            )\n\n            caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                \"fps\", \"ndepths\", \"nchannels\"\n            )\n\n            is3D = bool(ndepths &gt; 1)\n            if is3D:\n                raise NotImplementedError(\n                    \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                )\n\n            # handle multi-channel tiff image before running CaImAn\n            if nchannels &gt; 1:\n                channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                tmp_dir.mkdir(exist_ok=True)\n                _process_scanimage_tiff(\n                    [f.as_posix() for f in image_files], output_dir=tmp_dir\n                )\n                image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n            run_caiman(\n                file_paths=[f.as_posix() for f in image_files],\n                parameters=caiman_params,\n                sampling_rate=sampling_rate,\n                output_dir=output_dir,\n                is3D=is3D,\n            )\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            caiman_dataset = imaging_dataset\n            key[\"processing_time\"] = caiman_dataset.creation_time\n\n        elif method == \"extract\":\n            import suite2p\n            from scipy.io import savemat\n            from element_interface.extract_trigger import EXTRACT_trigger\n\n            # Motion Correction with Suite2p\n            params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n            params[\"suite2p\"][\"save_path0\"] = output_dir\n            (\n                params[\"suite2p\"][\"fs\"],\n                params[\"suite2p\"][\"nplanes\"],\n                params[\"suite2p\"][\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n            # Convert data.bin to registered_scans.mat\n            scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n            data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                \"nframes\", \"px_height\", \"px_width\"\n            )\n            data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n            scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n            # Save the motion corrected movie (data.bin) in a .mat file\n            savemat(\n                scan_matlab_fullpath,\n                {\"M\": np.transpose(data, axes=[1, 2, 0])},\n            )\n\n            # Execute EXTRACT\n\n            ex = EXTRACT_trigger(\n                scan_matlab_fullpath, params[\"extract\"], output_dir\n            )\n            ex.run()\n\n            _, extract_dataset = get_loader_result(key, ProcessingTask)\n            key[\"processing_time\"] = extract_dataset.creation_time\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingParamSet", "title": "<code>ProcessingParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters.</p> <p>A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Uniqiue parameter set ID.</p> <code>ProcessingMethod</code> <code>foreign key</code> <p>A primary key from ProcessingMethod.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>param_set_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameter Set, a dictionary of all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass ProcessingParamSet(dj.Lookup):\n\"\"\"Parameter set used for the processing of the calcium imaging scans,\n    including both the analysis suite and its respective input parameters.\n\n    A hash of the parameters of the analysis suite is also stored in order\n    to avoid duplicated entries.\n\n    Attributes:\n        paramset_idx (int): Uniqiue parameter set ID.\n        ProcessingMethod (foreign key): A primary key from ProcessingMethod.\n        paramset_desc (str): Parameter set description.\n        param_set_hash (uuid): A universally unique identifier for the parameter set.\n        params (longblob): Parameter Set, a dictionary of all applicable parameters to\n            the analysis suite.\n    \"\"\"\n\n    definition = \"\"\"# Processing Parameter Set\n    paramset_idx: smallint  # Uniqiue parameter set ID.\n    ---\n    -&gt; ProcessingMethod\n    paramset_desc: varchar(1280)  # Parameter-set description\n    param_set_hash: uuid  # A universally unique identifier for the parameter set\n    unique index (param_set_hash)\n    params: longblob  # Parameter Set, a dictionary of all applicable parameters to the analysis suite.\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        processing_method: str,\n        paramset_idx: int,\n        paramset_desc: str,\n        params: dict,\n    ):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n        This function automizes the parameter set hashing and avoids insertion of an\n            existing parameter set.\n\n        Attributes:\n            processing_method (str): Processing method/package used for processing of\n                calcium imaging.\n            paramset_idx (int): Uniqiue parameter set ID.\n            paramset_desc (str): Parameter set description.\n            params (dict): Parameter Set, all applicable parameters to the analysis\n                suite.\n        \"\"\"\n        if processing_method == \"extract\":\n            assert (\n                params.get(\"extract\") != None and params.get(\"suite2p\") != None\n            ), ValueError(\n                \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n            )\n\n            # Force Suite2p to only run motion correction.\n            params[\"suite2p\"][\"do_registration\"] = True\n            params[\"suite2p\"][\"roidetect\"] = False\n\n        param_dict = {\n            \"processing_method\": processing_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if q_param:  # If the specified param-set already exists\n            pname = q_param.fetch1(\"paramset_idx\")\n            if pname == paramset_idx:  # If the existed set has the same name: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set already exists - name: {}\".format(pname)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingParamSet.insert_new_params", "title": "<code>insert_new_params(processing_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Insert a parameter set into ProcessingParamSet table.</p> <p>This function automizes the parameter set hashing and avoids insertion of an     existing parameter set.</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method/package used for processing of calcium imaging.</p> <code>paramset_idx</code> <code>int</code> <p>Uniqiue parameter set ID.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>params</code> <code>dict</code> <p>Parameter Set, all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    processing_method: str,\n    paramset_idx: int,\n    paramset_desc: str,\n    params: dict,\n):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n    This function automizes the parameter set hashing and avoids insertion of an\n        existing parameter set.\n\n    Attributes:\n        processing_method (str): Processing method/package used for processing of\n            calcium imaging.\n        paramset_idx (int): Uniqiue parameter set ID.\n        paramset_desc (str): Parameter set description.\n        params (dict): Parameter Set, all applicable parameters to the analysis\n            suite.\n    \"\"\"\n    if processing_method == \"extract\":\n        assert (\n            params.get(\"extract\") != None and params.get(\"suite2p\") != None\n        ), ValueError(\n            \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n        )\n\n        # Force Suite2p to only run motion correction.\n        params[\"suite2p\"][\"do_registration\"] = True\n        params[\"suite2p\"][\"roidetect\"] = False\n\n    param_dict = {\n        \"processing_method\": processing_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if q_param:  # If the specified param-set already exists\n        pname = q_param.fetch1(\"paramset_idx\")\n        if pname == paramset_idx:  # If the existed set has the same name: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set already exists - name: {}\".format(pname)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask", "title": "<code>ProcessingTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A pairing of processing params and scans to be loaded or triggered</p> <p>This table defines a calcium imaging processing task for a combination of a <code>Scan</code> and a <code>ProcessingParamSet</code> entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods</p> <p>Attributes:</p> Name Type Description <code>Preprocess</code> <code>foreign key</code> <code>ProcessingParamSet</code> <code>foreign key</code> <code>processing_output_dir</code> <code>str</code> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results) or 'trigger' (trigger computation).</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass ProcessingTask(dj.Manual):\n\"\"\"A pairing of processing params and scans to be loaded or triggered\n\n    This table defines a calcium imaging processing task for a combination of a\n    `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method,\n    method's parameters). The task defined here is then run in the downstream table\n    Processing. This table supports definitions of both loading of pre-generated results\n    and the triggering of new analysis for all supported analysis methods\n\n    Attributes:\n        Preprocess (foreign key):\n        ProcessingParamSet (foreign key):\n        processing_output_dir (str):\n        task_mode (str): One of 'load' (load computed analysis results) or 'trigger'\n            (trigger computation).\n    \"\"\"\n\n    definition = \"\"\"# Manual table for defining a processing task ready to be run\n    -&gt; Preprocess\n    -&gt; ProcessingParamSet\n    ---\n    processing_output_dir: varchar(255)  #  Output directory of the processed scan relative to root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n        Args:\n            key (dict): Primary key from the ProcessingTask table.\n            relative (bool): If True, processing_output_dir is returned relative to\n                imaging_root_dir. Default False.\n            mkdir (bool): If True, create the processing_output_dir directory.\n                Default True.\n\n        Returns:\n            dir (str): A default output directory for the processed results (processed_output_dir\n                in ProcessingTask) based on the following convention:\n                processed_dir / scan_dir / {processing_method}_{paramset_idx}\n                e.g.: sub4/sess1/scan0/suite2p_0\n        \"\"\"\n        image_locators = {\n            \"NIS\": get_nd2_files,\n            \"ScanImage\": get_scan_image_files,\n            \"Scanbox\": get_scan_box_files,\n            \"PrairieView\": get_prairieview_files,\n        }\n        image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n        scan_dir = find_full_path(\n            get_imaging_root_data_dir(), image_locator(key)[0]\n        ).parent\n        root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n        method = (\n            (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n        )\n\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        output_dir = (\n            processed_dir\n            / scan_dir.relative_to(root_dir)\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n        Generate an entry in the ProcessingTask table for a particular scan using an\n        existing parameter set from the ProcessingParamSet table.\n\n        Args:\n            scan_key (dict): Primary key from Scan table.\n            paramset_idx (int): Unique parameter set ID.\n        \"\"\"\n        key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n        method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n            \"processing_method\"\n        )\n\n        try:\n            if method == \"suite2p\":\n                from element_interface import suite2p_loader\n\n                suite2p_loader.Suite2p(output_dir)\n            elif method == \"caiman\":\n                from element_interface import caiman_loader\n\n                caiman_loader.CaImAn(output_dir)\n            elif method == \"extract\":\n                from element_interface import extract_loader\n\n                extract_loader.EXTRACT(output_dir)\n\n            else:\n                raise NotImplementedError(\n                    \"Unknown/unimplemented method: {}\".format(method)\n                )\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"processing_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n\n    auto_generate_entries = generate\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask.generate", "title": "<code>generate(scan_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet</p> <p>Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan table.</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>0</code> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@classmethod\ndef generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n    Generate an entry in the ProcessingTask table for a particular scan using an\n    existing parameter set from the ProcessingParamSet table.\n\n    Args:\n        scan_key (dict): Primary key from Scan table.\n        paramset_idx (int): Unique parameter set ID.\n    \"\"\"\n    key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n    method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n        \"processing_method\"\n    )\n\n    try:\n        if method == \"suite2p\":\n            from element_interface import suite2p_loader\n\n            suite2p_loader.Suite2p(output_dir)\n        elif method == \"caiman\":\n            from element_interface import caiman_loader\n\n            caiman_loader.CaImAn(output_dir)\n        elif method == \"extract\":\n            from element_interface import extract_loader\n\n            extract_loader.EXTRACT(output_dir)\n\n        else:\n            raise NotImplementedError(\n                \"Unknown/unimplemented method: {}\".format(method)\n            )\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"processing_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer an output directory for an entry in ProcessingTask table.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from the ProcessingTask table.</p> required <code>relative</code> <code>bool</code> <p>If True, processing_output_dir is returned relative to imaging_root_dir. Default False.</p> <code>False</code> <code>mkdir</code> <code>bool</code> <p>If True, create the processing_output_dir directory. Default True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dir</code> <code>str</code> <p>A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n    Args:\n        key (dict): Primary key from the ProcessingTask table.\n        relative (bool): If True, processing_output_dir is returned relative to\n            imaging_root_dir. Default False.\n        mkdir (bool): If True, create the processing_output_dir directory.\n            Default True.\n\n    Returns:\n        dir (str): A default output directory for the processed results (processed_output_dir\n            in ProcessingTask) based on the following convention:\n            processed_dir / scan_dir / {processing_method}_{paramset_idx}\n            e.g.: sub4/sess1/scan0/suite2p_0\n    \"\"\"\n    image_locators = {\n        \"NIS\": get_nd2_files,\n        \"ScanImage\": get_scan_image_files,\n        \"Scanbox\": get_scan_box_files,\n        \"PrairieView\": get_prairieview_files,\n    }\n    image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n    scan_dir = find_full_path(\n        get_imaging_root_data_dir(), image_locator(key)[0]\n    ).parent\n    root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n    method = (\n        (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n    )\n\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    output_dir = (\n        processed_dir\n        / scan_dir.relative_to(root_dir)\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation", "title": "<code>Segmentation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Result of the Segmentation process.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Primary key from Curation.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Segmentation(dj.Computed):\n\"\"\"Result of the Segmentation process.\n\n    Attributes:\n        Curation (foreign key): Primary key from Curation.\n    \"\"\"\n\n    definition = \"\"\"# Different mask segmentations.\n    -&gt; Curation\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n        Attributes:\n            Segmentation (foreign key): Primary key from Segmentation.\n            mask (int): Unique mask ID.\n            scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n                used for segmentation.\n            mask_npix (int): Number of pixels in ROIs.\n            mask_center_x (int): Center x coordinate in pixel.\n            mask_center_y (int): Center y coordinate in pixel.\n            mask_center_z (int): Center z coordinate in pixel.\n            mask_xpix (longblob): X coordinates in pixels.\n            mask_ypix (longblob): Y coordinates in pixels.\n            mask_zpix (longblob): Z coordinates in pixels.\n            mask_weights (longblob): Weights of the mask at the indices above.\n        \"\"\"\n\n        definition = \"\"\" # A mask produced by segmentation.\n        -&gt; master\n        mask               : smallint\n        ---\n        -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n        mask_npix          : int       # number of pixels in ROIs\n        mask_center_x      : int       # center x coordinate in pixel\n        mask_center_y      : int       # center y coordinate in pixel\n        mask_center_z=null : int       # center z coordinate in pixel\n        mask_xpix          : longblob  # x coordinates in pixels\n        mask_ypix          : longblob  # y coordinates in pixels      \n        mask_zpix=null     : longblob  # z coordinates in pixels        \n        mask_weights       : longblob  # weights of the mask at the indices above\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            masks, cells = [], []\n            for plane, s2p in suite2p_dataset.planes.items():\n                mask_count = len(masks)  # increment mask id from all \"plane\"\n                for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                    zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n                ):\n                    masks.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"segmentation_channel\": s2p.segmentation_channel,\n                            \"mask_npix\": mask_stat[\"npix\"],\n                            \"mask_center_x\": mask_stat[\"med\"][1],\n                            \"mask_center_y\": mask_stat[\"med\"][0],\n                            \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                            \"mask_xpix\": mask_stat[\"xpix\"],\n                            \"mask_ypix\": mask_stat[\"ypix\"],\n                            \"mask_zpix\": np.full(\n                                mask_stat[\"npix\"],\n                                mask_stat.get(\"iplane\", plane),\n                            ),\n                            \"mask_weights\": mask_stat[\"lam\"],\n                        }\n                    )\n                    if is_cell:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"suite2p_default_classifier\",\n                                \"mask\": mask_idx + mask_count,\n                                \"mask_type\": \"soma\",\n                                \"confidence\": cell_prob,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"suite2p_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            masks, cells = [], []\n            for mask in caiman_dataset.masks:\n                masks.append(\n                    {\n                        **key,\n                        \"segmentation_channel\": segmentation_channel,\n                        \"mask\": mask[\"mask_id\"],\n                        \"mask_npix\": mask[\"mask_npix\"],\n                        \"mask_center_x\": mask[\"mask_center_x\"],\n                        \"mask_center_y\": mask[\"mask_center_y\"],\n                        \"mask_center_z\": mask[\"mask_center_z\"],\n                        \"mask_xpix\": mask[\"mask_xpix\"],\n                        \"mask_ypix\": mask[\"mask_ypix\"],\n                        \"mask_zpix\": mask[\"mask_zpix\"],\n                        \"mask_weights\": mask[\"mask_weights\"],\n                    }\n                )\n                if caiman_dataset.cnmf.estimates.idx_components is not None:\n                    if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"caiman_default_classifier\",\n                                \"mask\": mask[\"mask_id\"],\n                                \"mask_type\": \"soma\",\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"caiman_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n            elif method == \"extract\":\n                extract_dataset = imaging_dataset\n                masks = [\n                    dict(\n                        **key,\n                        segmentation_channel=0,\n                        mask=mask[\"mask_id\"],\n                        mask_npix=mask[\"mask_npix\"],\n                        mask_center_x=mask[\"mask_center_x\"],\n                        mask_center_y=mask[\"mask_center_y\"],\n                        mask_center_z=mask[\"mask_center_z\"],\n                        mask_xpix=mask[\"mask_xpix\"],\n                        mask_ypix=mask[\"mask_ypix\"],\n                        mask_zpix=mask[\"mask_zpix\"],\n                        mask_weights=mask[\"mask_weights\"],\n                    )\n                    for mask in extract_dataset.load_results()\n                ]\n\n                self.insert1(key)\n                self.Mask.insert(masks, ignore_extra_fields=True)\n        else:\n            raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of the masks identified from the Segmentation procedure.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>mask</code> <code>int</code> <p>Unique mask ID.</p> <code>scan.Channel.proj(segmentation_channel='channel')</code> <code>foreign key</code> <p>Channel used for segmentation.</p> <code>mask_npix</code> <code>int</code> <p>Number of pixels in ROIs.</p> <code>mask_center_x</code> <code>int</code> <p>Center x coordinate in pixel.</p> <code>mask_center_y</code> <code>int</code> <p>Center y coordinate in pixel.</p> <code>mask_center_z</code> <code>int</code> <p>Center z coordinate in pixel.</p> <code>mask_xpix</code> <code>longblob</code> <p>X coordinates in pixels.</p> <code>mask_ypix</code> <code>longblob</code> <p>Y coordinates in pixels.</p> <code>mask_zpix</code> <code>longblob</code> <p>Z coordinates in pixels.</p> <code>mask_weights</code> <code>longblob</code> <p>Weights of the mask at the indices above.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        mask (int): Unique mask ID.\n        scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n            used for segmentation.\n        mask_npix (int): Number of pixels in ROIs.\n        mask_center_x (int): Center x coordinate in pixel.\n        mask_center_y (int): Center y coordinate in pixel.\n        mask_center_z (int): Center z coordinate in pixel.\n        mask_xpix (longblob): X coordinates in pixels.\n        mask_ypix (longblob): Y coordinates in pixels.\n        mask_zpix (longblob): Z coordinates in pixels.\n        mask_weights (longblob): Weights of the mask at the indices above.\n    \"\"\"\n\n    definition = \"\"\" # A mask produced by segmentation.\n    -&gt; master\n    mask               : smallint\n    ---\n    -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n    mask_npix          : int       # number of pixels in ROIs\n    mask_center_x      : int       # center x coordinate in pixel\n    mask_center_y      : int       # center y coordinate in pixel\n    mask_center_z=null : int       # center z coordinate in pixel\n    mask_xpix          : longblob  # x coordinates in pixels\n    mask_ypix          : longblob  # y coordinates in pixels      \n    mask_zpix=null     : longblob  # z coordinates in pixels        \n    mask_weights       : longblob  # weights of the mask at the indices above\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Segmentation with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        masks, cells = [], []\n        for plane, s2p in suite2p_dataset.planes.items():\n            mask_count = len(masks)  # increment mask id from all \"plane\"\n            for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n            ):\n                masks.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"segmentation_channel\": s2p.segmentation_channel,\n                        \"mask_npix\": mask_stat[\"npix\"],\n                        \"mask_center_x\": mask_stat[\"med\"][1],\n                        \"mask_center_y\": mask_stat[\"med\"][0],\n                        \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                        \"mask_xpix\": mask_stat[\"xpix\"],\n                        \"mask_ypix\": mask_stat[\"ypix\"],\n                        \"mask_zpix\": np.full(\n                            mask_stat[\"npix\"],\n                            mask_stat.get(\"iplane\", plane),\n                        ),\n                        \"mask_weights\": mask_stat[\"lam\"],\n                    }\n                )\n                if is_cell:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"suite2p_default_classifier\",\n                            \"mask\": mask_idx + mask_count,\n                            \"mask_type\": \"soma\",\n                            \"confidence\": cell_prob,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"suite2p_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        masks, cells = [], []\n        for mask in caiman_dataset.masks:\n            masks.append(\n                {\n                    **key,\n                    \"segmentation_channel\": segmentation_channel,\n                    \"mask\": mask[\"mask_id\"],\n                    \"mask_npix\": mask[\"mask_npix\"],\n                    \"mask_center_x\": mask[\"mask_center_x\"],\n                    \"mask_center_y\": mask[\"mask_center_y\"],\n                    \"mask_center_z\": mask[\"mask_center_z\"],\n                    \"mask_xpix\": mask[\"mask_xpix\"],\n                    \"mask_ypix\": mask[\"mask_ypix\"],\n                    \"mask_zpix\": mask[\"mask_zpix\"],\n                    \"mask_weights\": mask[\"mask_weights\"],\n                }\n            )\n            if caiman_dataset.cnmf.estimates.idx_components is not None:\n                if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"caiman_default_classifier\",\n                            \"mask\": mask[\"mask_id\"],\n                            \"mask_type\": \"soma\",\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"caiman_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            masks = [\n                dict(\n                    **key,\n                    segmentation_channel=0,\n                    mask=mask[\"mask_id\"],\n                    mask_npix=mask[\"mask_npix\"],\n                    mask_center_x=mask[\"mask_center_x\"],\n                    mask_center_y=mask[\"mask_center_y\"],\n                    mask_center_z=mask[\"mask_center_z\"],\n                    mask_xpix=mask[\"mask_xpix\"],\n                    mask_ypix=mask[\"mask_ypix\"],\n                    mask_zpix=mask[\"mask_zpix\"],\n                    mask_weights=mask[\"mask_weights\"],\n                )\n                for mask in extract_dataset.load_results()\n            ]\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n    else:\n        raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.activate", "title": "<code>activate(imaging_schema_name, scan_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>imaging_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>imaging</code> module.</p> required <code>scan_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>scan</code> module. Omitted, if the <code>scan</code> module is already activated.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>imaging</code> module: + all that are required by the <code>scan</code> module.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <ul> <li>Session: A parent table to Scan, identifying a scanning session.</li> <li>Equipment: A parent table to Scan, identifying a scanning device.</li> </ul> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def activate(\n    imaging_schema_name,\n    scan_schema_name=None,\n    *,\n    create_schema=True,\n    create_tables=True,\n    linking_module=None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        imaging_schema_name (str): Schema name on the database server to activate the\n            `imaging` module.\n        scan_schema_name (str): Schema name on the database server to activate the\n            `scan` module. Omitted, if the `scan` module is already activated.\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `imaging` module: + all that are required by\n            the `scan` module.\n\n    Dependencies:\n    Upstream tables:\n        + Session: A parent table to Scan, identifying a scanning session.\n        + Equipment: A parent table to Scan, identifying a scanning device.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    scan.activate(\n        scan_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        linking_module=linking_module,\n    )\n    schema.activate(\n        imaging_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    imaging_report.activate(f\"{imaging_schema_name}_report\", imaging_schema_name)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_loader_result", "title": "<code>get_loader_result(key, table)</code>", "text": "<p>Retrieve the processed imaging results from a suite2p or caiman loader.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>The <code>key</code> to one entry of ProcessingTask or Curation</p> required <code>table</code> <code>dj.Table</code> <p>A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation)</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the processing_method is different than 'suite2p' or 'caiman'.</p> <p>Returns:</p> Type Description <p>A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,</p> <p>see element-interface for more information on the loaders.)</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def get_loader_result(key: dict, table: dj.Table):\n\"\"\"Retrieve the processed imaging results from a suite2p or caiman loader.\n\n    Args:\n        key (dict): The `key` to one entry of ProcessingTask or Curation\n        table (dj.Table): A datajoint table to retrieve the loaded results from (e.g.\n            ProcessingTask, Curation)\n\n    Raises:\n        NotImplementedError: If the processing_method is different than 'suite2p' or\n            'caiman'.\n\n    Returns:\n        A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,\n        see element-interface for more information on the loaders.)\n    \"\"\"\n    method, output_dir = (ProcessingParamSet * table &amp; key).fetch1(\n        \"processing_method\", _table_attribute_mapper[table.__name__]\n    )\n\n    output_path = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n    if method == \"suite2p\" or (\n        method == \"extract\" and table.__name__ == \"MotionCorrection\"\n    ):\n        from element_interface import suite2p_loader\n\n        loaded_dataset = suite2p_loader.Suite2p(output_path)\n    elif method == \"caiman\":\n        from element_interface import caiman_loader\n\n        loaded_dataset = caiman_loader.CaImAn(output_path)\n    elif method == \"extract\":\n        from element_interface import extract_loader\n\n        loaded_dataset = extract_loader.EXTRACT(output_path)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n\n    return method, loaded_dataset\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/", "title": "imaging_report.py", "text": ""}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.ScanLevelReport", "title": "<code>ScanLevelReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Scan level report with figures.</p> <p>Attributes:</p> Name Type Description <code>imaging.Segmentation</code> <code>foreign key</code> <p>Primary key from imaging.Segmentation.</p> <code>cell_overlayed_image</code> <code>longblob</code> <p>Plotly figure object showing the segmented cells on the average image.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>@schema\nclass ScanLevelReport(dj.Computed):\n\"\"\"Scan level report with figures.\n\n    Attributes:\n        imaging.Segmentation (foreign key): Primary key from imaging.Segmentation.\n        cell_overlayed_image (longblob): Plotly figure object showing the segmented\n            cells on the average image.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; imaging.Segmentation\n    ---\n    cell_overlayed_image: longblob\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n        image_fig = cell_plot.plot_cell_overlayed_image(imaging, key)\n        self.insert1({**key, \"cell_overlayed_image\": image_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.ScanLevelReport.make", "title": "<code>make(key)</code>", "text": "<p>Compute and ingest the plotly figure objects.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n    image_fig = cell_plot.plot_cell_overlayed_image(imaging, key)\n    self.insert1({**key, \"cell_overlayed_image\": image_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.TraceReport", "title": "<code>TraceReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Figures of traces.</p> <p>Attributes:</p> Name Type Description <code>imaging.Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from imaging.Segmentation.Mask.</p> <code>cell_traces</code> <code>longblob</code> <p>Plotly figure object showing the cell traces.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>@schema\nclass TraceReport(dj.Computed):\n\"\"\"Figures of traces.\n\n    Attributes:\n        imaging.Segmentation.Mask (foreign key): Primary key from\n            imaging.Segmentation.Mask.\n        cell_traces (longblob): Plotly figure object showing the cell traces.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; imaging.Segmentation.Mask\n    ---\n    cell_traces: longblob\n    \"\"\"\n\n    @property\n    def key_source(self):\n\"\"\"Limit the TraceReport to Masks that have Activity table populated.\n        database.\"\"\"\n\n        return imaging.Segmentation.Mask &amp; imaging.Activity\n\n    def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n        trace_fig = cell_plot.plot_cell_traces(imaging, key)\n        self.insert1({**key, \"cell_traces\": trace_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.TraceReport.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the TraceReport to Masks that have Activity table populated. database.</p>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.TraceReport.make", "title": "<code>make(key)</code>", "text": "<p>Compute and ingest the plotly figure objects.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n    trace_fig = cell_plot.plot_cell_traces(imaging, key)\n    self.insert1({**key, \"cell_traces\": trace_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.activate", "title": "<code>activate(schema_name, imaging_schema_name, *, create_schema=True, create_tables=True)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>imaging_report</code> schema</p> required <code>imaging_schema_name</code> <code>str</code> <p>Schema name of the activated imaging element for which this imaging_report schema will be downstream from</p> required <code>create_schema</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>def activate(\n    schema_name, imaging_schema_name, *, create_schema=True, create_tables=True\n):\n\"\"\"Activate this schema.\n\n    Args:\n        schema_name (str): Schema name on the database server to activate the\n            `imaging_report` schema\n        imaging_schema_name (str): Schema name of the activated imaging element for\n            which this imaging_report schema will be downstream from\n        create_schema: When True (default), create schema in the database if it does not\n            yet exist.\n        create_tables: When True (default), create tables in the database if they do not\n            yet exist.\n    \"\"\"\n    global imaging\n    imaging = dj.create_virtual_module(\"imaging\", imaging_schema_name)\n\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=imaging.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/", "title": "scan.py", "text": ""}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.AcquisitionSoftware", "title": "<code>AcquisitionSoftware</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>A list of acquisition softwares supported by the Element.</p> <p>Required to define a scan.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code>str</code> <p>Acquistion software</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"A list of acquisition softwares supported by the Element.\n\n    Required to define a scan.\n\n    Attributes:\n        acq_software (str): Acquistion software\n    \"\"\"\n\n    definition = \"\"\"  # Acquisition softwares\n    acq_software: varchar(24)    \n    \"\"\"\n    contents = zip([\"ScanImage\", \"Scanbox\", \"NIS\", \"PrairieView\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.Channel", "title": "<code>Channel</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Recording channels for the imaging wavelengths.</p> <p>Attributes:</p> Name Type Description <code>channel</code> <code>int</code> <p>Channel index</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass Channel(dj.Lookup):\n\"\"\"Recording channels for the imaging wavelengths.\n\n    Attributes:\n        channel (int): Channel index\n    \"\"\"\n\n    definition = \"\"\"  # A recording channel\n    channel     : tinyint  # 0-based indexing\n    \"\"\"\n    contents = zip(range(5))\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.Scan", "title": "<code>Scan</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Scan defined by a measurement done using a scanner and an acquisition software.</p> <p>The details of the scanning data is placed in other tables, including, ScanLocation, ScanInfo, and ScanInfo's part tables.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>A primary key from Session.</p> <code>scan_id</code> <code>int</code> <p>Unique Scan ID.</p> <code>Equipment</code> <code>foreign key</code> <p>A primary key from Equipment.</p> <code>AcquisitionSoftware</code> <code>foreign key</code> <p>A primary key from AcquisitonSoftware.</p> <code>scan_notes</code> <code>str</code> <p>Notes of the experimenter regarding the scan.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass Scan(dj.Manual):\n\"\"\"Scan defined by a measurement done using a scanner and an acquisition software.\n\n    The details of the scanning data is placed in other tables, including,\n    ScanLocation, ScanInfo, and ScanInfo's part tables.\n\n    Attributes:\n        Session (foreign key): A primary key from Session.\n        scan_id (int): Unique Scan ID.\n        Equipment (foreign key, optional): A primary key from Equipment.\n        AcquisitionSoftware (foreign key): A primary key from AcquisitonSoftware.\n        scan_notes (str, optional): Notes of the experimenter regarding the scan.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Session\n    scan_id: int\n    ---\n    -&gt; [nullable] Equipment\n    -&gt; AcquisitionSoftware\n    scan_notes='' : varchar(4095)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo", "title": "<code>ScanInfo</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Information about the scan extracted from the recorded files.</p> <p>Attributes:</p> Name Type Description <code>Scan</code> <code>foreign key</code> <p>A primary key from Scan.</p> <code>nfields</code> <code>int</code> <p>Number of fields.</p> <code>nchannels</code> <code>int</code> <p>Number of channels.</p> <code>ndepths</code> <code>int</code> <p>Number of scanning depths (planes).</p> <code>nframes</code> <code>int</code> <p>Number of recorded frames.</p> <code>nrois</code> <code>int</code> <p>Number of ROIs (see scanimage's multi ROI imaging).</p> <code>x</code> <code>float</code> <p>ScanImage's 0 point in the motor coordinate system (um).</p> <code>y</code> <code>float</code> <p>ScanImage's 0 point in the motor coordinate system (um).</p> <code>z</code> <code>float</code> <p>ScanImage's 0 point in the motor coordinate system (um).</p> <code>fps</code> <code>float) </code> <p>Frames per second (Hz) - Volumetric Scan Rate.</p> <code>bidirectional</code> <code>bool</code> <p>True = bidirectional scanning.</p> <code>usecs_per_line</code> <code>float</code> <p>Microseconds per scan line.</p> <code>fill_fraction</code> <code>float</code> <p>Raster scan temporal fill fraction (see scanimage)</p> <code>scan_datetime</code> <code>datetime</code> <p>Datetime of the scan.</p> <code>scan_duration</code> <code>float</code> <p>Duration of the scan (s).</p> <code>bidirectional_z</code> <code>bool</code> <p>True = bidirectional z-scan.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass ScanInfo(dj.Imported):\n\"\"\"\n    Information about the scan extracted from the recorded files.\n\n    Attributes:\n        Scan (foreign key): A primary key from Scan.\n        nfields (int): Number of fields.\n        nchannels (int): Number of channels.\n        ndepths (int): Number of scanning depths (planes).\n        nframes (int): Number of recorded frames.\n        nrois (int): Number of ROIs (see scanimage's multi ROI imaging).\n        x (float, optional): ScanImage's 0 point in the motor coordinate system (um).\n        y (float, optional): ScanImage's 0 point in the motor coordinate system (um).\n        z (float, optional): ScanImage's 0 point in the motor coordinate system (um).\n        fps (float) : Frames per second (Hz) - Volumetric Scan Rate.\n        bidirectional (bool): True = bidirectional scanning.\n        usecs_per_line (float, optional): Microseconds per scan line.\n        fill_fraction (float, optional): Raster scan temporal fill fraction (see\n            scanimage)\n        scan_datetime (datetime, optional): Datetime of the scan.\n        scan_duration (float, optional): Duration of the scan (s).\n        bidirectional_z (bool, optional): True = bidirectional z-scan.\n    \"\"\"\n\n    definition = \"\"\" # General data about the reso/meso scans from header\n    -&gt; Scan\n    ---\n    nfields              : tinyint   # number of fields\n    nchannels            : tinyint   # number of channels\n    ndepths              : int       # Number of scanning depths (planes)\n    nframes              : int       # number of recorded frames\n    nrois                : tinyint   # number of ROIs (see scanimage's multi ROI imaging)\n    x=null               : float     # (um) ScanImage's 0 point in the motor coordinate system\n    y=null               : float     # (um) ScanImage's 0 point in the motor coordinate system\n    z=null               : float     # (um) ScanImage's 0 point in the motor coordinate system\n    fps                  : float     # (Hz) frames per second - Volumetric Scan Rate \n    bidirectional        : boolean   # true = bidirectional scanning\n    usecs_per_line=null  : float     # microseconds per scan line\n    fill_fraction=null   : float     # raster scan temporal fill fraction (see scanimage)\n    scan_datetime=null   : datetime  # datetime of the scan\n    scan_duration=null   : float     # (seconds) duration of the scan\n    bidirectional_z=null : boolean   # true = bidirectional z-scan\n    \"\"\"\n\n    class Field(dj.Part):\n\"\"\"Stores field information of scan, including its coordinates, size, pixel\n        pitch, etc.\n\n        Attributes:\n            ScanInfo (foreign key): A primary key from ScanInfo.\n            field_idx (int): Unique field index.\n            px_height (int): Image height in pixels.\n            px_width (int): Image width in pixels.\n            um_height (float, optional): Image height in microns.\n            um_width (float, optional): Image width in microns.\n            field_x (float, optional): X coordinate of the center of field in the motor\n                coordinate system (um).\n            field_y (float, optional): Y coordinate of the center of field in the motor\n                coordinate system (um).\n            field_z (float, optional): Relative depth of field (um).\n            delay_image (longblob, optional): Delay between the start of the scan and\n                pixels in this field (ms).\n            roi (int, optional): The scanning roi (as recorded in the acquisition\n                software) containing this field - only relevant to mesoscale scans.\n        \"\"\"\n\n        definition = \"\"\" # field-specific scan information\n        -&gt; master\n        field_idx         : int\n        ---\n        px_height         : smallint  # height in pixels\n        px_width          : smallint  # width in pixels\n        um_height=null    : float     # height in microns\n        um_width=null     : float     # width in microns\n        field_x=null      : float     # (um) center of field in the motor coordinate system\n        field_y=null      : float     # (um) center of field in the motor coordinate system\n        field_z=null      : float     # (um) relative depth of field\n        delay_image=null  : longblob  # (ms) delay between the start of the scan and pixels in this field\n        roi=null          : int       # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans\n        \"\"\"\n\n    class ScanFile(dj.Part):\n\"\"\"Filepath of the scan relative to root data directory.\n\n        Attributes:\n            ScanInfo (foreign key): A primary key from ScanInfo.\n            file_path (str): Path of the scan file relative to the root data directory.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        file_path: varchar(255)  # Filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the ScanInfo with the information parsed from image files.\"\"\"\n\n        acq_software = (Scan &amp; key).fetch1(\"acq_software\")\n\n        if acq_software == \"ScanImage\":\n            import scanreader\n\n            # Read the scan\n            scan_filepaths = get_scan_image_files(key)\n            scan = scanreader.read_scan(scan_filepaths)\n\n            # Insert in ScanInfo\n            x_zero = (\n                scan.motor_position_at_zero[0]\n                if scan.motor_position_at_zero\n                else None\n            )\n            y_zero = (\n                scan.motor_position_at_zero[1]\n                if scan.motor_position_at_zero\n                else None\n            )\n            z_zero = (\n                scan.motor_position_at_zero[2]\n                if scan.motor_position_at_zero\n                else None\n            )\n\n            self.insert1(\n                dict(\n                    key,\n                    nfields=scan.num_fields,\n                    nchannels=scan.num_channels,\n                    nframes=scan.num_frames,\n                    ndepths=scan.num_scanning_depths,\n                    x=x_zero,\n                    y=y_zero,\n                    z=z_zero,\n                    fps=scan.fps,\n                    bidirectional=scan.is_bidirectional,\n                    usecs_per_line=scan.seconds_per_line * 1e6,\n                    fill_fraction=scan.temporal_fill_fraction,\n                    nrois=scan.num_rois if scan.is_multiROI else 0,\n                    scan_duration=scan.num_frames / scan.fps,\n                )\n            )\n            # Insert Field(s)\n            if scan.is_multiROI:\n                self.Field.insert(\n                    [\n                        dict(\n                            key,\n                            field_idx=field_id,\n                            px_height=scan.field_heights[field_id],\n                            px_width=scan.field_widths[field_id],\n                            um_height=scan.field_heights_in_microns[field_id],\n                            um_width=scan.field_widths_in_microns[field_id],\n                            field_x=x_zero\n                            + scan._degrees_to_microns(scan.fields[field_id].x)\n                            if x_zero\n                            else None,\n                            field_y=y_zero\n                            + scan._degrees_to_microns(scan.fields[field_id].y)\n                            if y_zero\n                            else None,\n                            field_z=z_zero + scan.fields[field_id].depth\n                            if z_zero\n                            else None,\n                            delay_image=scan.field_offsets[field_id],\n                            roi=scan.field_rois[field_id][0],\n                        )\n                        for field_id in range(scan.num_fields)\n                    ]\n                )\n            else:\n                self.Field.insert(\n                    [\n                        dict(\n                            key,\n                            field_idx=plane_idx,\n                            px_height=scan.image_height,\n                            px_width=scan.image_width,\n                            um_height=getattr(\n                                scan, \"image_height_in_microns\", None\n                            ),\n                            um_width=getattr(\n                                scan, \"image_width_in_microns\", None\n                            ),\n                            field_x=x_zero if x_zero else None,\n                            field_y=y_zero if y_zero else None,\n                            field_z=z_zero + scan.scanning_depths[plane_idx]\n                            if z_zero\n                            else None,\n                            delay_image=scan.field_offsets[plane_idx],\n                        )\n                        for plane_idx in range(scan.num_scanning_depths)\n                    ]\n                )\n        elif acq_software == \"Scanbox\":\n            import sbxreader\n\n            # Read the scan\n            scan_filepaths = get_scan_box_files(key)\n            sbx_meta = sbxreader.sbx_get_metadata(scan_filepaths[0])\n            sbx_matinfo = sbxreader.sbx_get_info(scan_filepaths[0])\n            is_multiROI = bool(\n                sbx_matinfo.mesoscope.enabled\n            )  # currently not handling \"multiROI\" ingestion\n\n            if is_multiROI:\n                raise NotImplementedError(\n                    \"Loading routine not implemented for Scanbox multiROI scan mode\"\n                )\n\n            # Insert in ScanInfo\n            x_zero, y_zero, z_zero = sbx_meta[\"stage_pos\"]\n            self.insert1(\n                dict(\n                    key,\n                    nfields=sbx_meta[\"num_fields\"]\n                    if is_multiROI\n                    else sbx_meta[\"num_planes\"],\n                    nchannels=sbx_meta[\"num_channels\"],\n                    nframes=sbx_meta[\"num_frames\"],\n                    ndepths=sbx_meta[\"num_planes\"],\n                    x=x_zero,\n                    y=y_zero,\n                    z=z_zero,\n                    fps=sbx_meta[\"frame_rate\"],\n                    bidirectional=sbx_meta == \"bidirectional\",\n                    nrois=sbx_meta[\"num_rois\"] if is_multiROI else 0,\n                    scan_duration=(\n                        sbx_meta[\"num_frames\"] / sbx_meta[\"frame_rate\"]\n                    ),\n                )\n            )\n            # Insert Field(s)\n            if not is_multiROI:\n                px_width, px_height = sbx_meta[\"frame_size\"]\n                self.Field.insert(\n                    [\n                        dict(\n                            key,\n                            field_idx=plane_idx,\n                            px_height=px_height,\n                            px_width=px_width,\n                            um_height=px_height * sbx_meta[\"um_per_pixel_y\"]\n                            if sbx_meta[\"um_per_pixel_y\"]\n                            else None,\n                            um_width=px_width * sbx_meta[\"um_per_pixel_x\"]\n                            if sbx_meta[\"um_per_pixel_x\"]\n                            else None,\n                            field_x=x_zero,\n                            field_y=y_zero,\n                            field_z=z_zero + sbx_meta[\"etl_pos\"][plane_idx],\n                        )\n                        for plane_idx in range(sbx_meta[\"num_planes\"])\n                    ]\n                )\n        elif acq_software == \"NIS\":\n            import nd2\n\n            # Read the scan\n            scan_filepaths = get_nd2_files(key)\n            nd2_file = nd2.ND2File(scan_filepaths[0])\n            is_multiROI = False  # MultiROI to be implemented later\n\n            # Frame per second\n            try:\n                fps = (\n                    1000\n                    / nd2_file.experiment[0]\n                    .parameters.periods[0]\n                    .periodDiff.avg\n                )\n            except:\n                fps = 1000 / nd2_file.experiment[0].parameters.periodDiff.avg\n\n            # Estimate ND2 file scan duration\n            def estimate_nd2_scan_duration(nd2_scan_obj):\n                # Calculates scan duration for Nikon images\n                ti = (\n                    nd2_scan_obj.frame_metadata(0)\n                    .channels[0]\n                    .time.absoluteJulianDayNumber\n                )  # Initial frame's JD.\n                tf = (\n                    nd2_scan_obj.frame_metadata(nd2_scan_obj.shape[0] - 1)\n                    .channels[0]\n                    .time.absoluteJulianDayNumber\n                )  # Final frame's JD.\n\n                return (tf - ti) * 86400 + 1 / fps\n\n            scan_duration = sum(\n                estimate_nd2_scan_duration(nd2.ND2File(f))\n                for f in scan_filepaths\n            )\n\n            try:\n                scan_datetime = nd2_file.text_info[\"date\"]\n                scan_datetime = datetime.strptime(\n                    scan_datetime,\n                    \"%m/%d/%Y %H:%M:%S %p\"\n                    if re.search((\"AM|PM\"), scan_datetime)\n                    else \"%m/%d/%Y %H:%M:%S\",\n                )\n                scan_datetime = datetime.strftime(\n                    scan_datetime, \"%Y-%m-%d %H:%M:%S\"\n                )\n            except:\n                scan_datetime = None\n\n            # Insert in ScanInfo\n            self.insert1(\n                dict(\n                    key,\n                    nfields=nd2_file.sizes.get(\"P\", 1),\n                    nchannels=nd2_file.attributes.channelCount,\n                    nframes=nd2_file.metadata.contents.frameCount,\n                    ndepths=nd2_file.sizes.get(\"Z\", 1),\n                    x=None,\n                    y=None,\n                    z=None,\n                    fps=fps,\n                    bidirectional=bool(\n                        nd2_file.custom_data[\"GrabberCameraSettingsV1_0\"][\n                            \"GrabberCameraSettings\"\n                        ][\"PropertiesQuality\"][\"ScanDirection\"]\n                    ),\n                    nrois=0,\n                    scan_datetime=scan_datetime,\n                    scan_duration=scan_duration,\n                )\n            )\n\n            # MultiROI to be implemented later\n\n            # Insert in Field\n            if not is_multiROI:\n                self.Field.insert(\n                    [\n                        dict(\n                            key,\n                            field_idx=plane_idx,\n                            px_height=nd2_file.attributes.heightPx,\n                            px_width=nd2_file.attributes.widthPx,\n                            um_height=nd2_file.attributes.heightPx\n                            * nd2_file.voxel_size().y,\n                            um_width=nd2_file.attributes.widthPx\n                            * nd2_file.voxel_size().x,\n                            field_x=None,\n                            field_y=None,\n                            field_z=None,\n                        )\n                        for plane_idx in range(nd2_file.sizes.get(\"Z\", 1))\n                    ]\n                )\n        elif acq_software == \"PrairieView\":\n            from element_interface import prairieviewreader\n\n            scan_filepaths = get_prairieview_files(key)\n            pvscan_info = prairieviewreader.get_pv_metadata(scan_filepaths[0])\n            self.insert1(\n                dict(\n                    key,\n                    nfields=pvscan_info[\"num_fields\"],\n                    nchannels=pvscan_info[\"num_channels\"],\n                    ndepths=pvscan_info[\"num_planes\"],\n                    nframes=pvscan_info[\"num_frames\"],\n                    nrois=pvscan_info[\"num_rois\"],\n                    x=pvscan_info[\"x_pos\"],\n                    y=pvscan_info[\"y_pos\"],\n                    z=pvscan_info[\"z_pos\"],\n                    fps=pvscan_info[\"frame_rate\"],\n                    bidirectional=pvscan_info[\"bidirectional\"],\n                    bidirectional_z=pvscan_info[\"bidirectional_z\"],\n                    usecs_per_line=pvscan_info[\"usecs_per_line\"],\n                    scan_datetime=pvscan_info[\"scan_datetime\"],\n                    scan_duration=pvscan_info[\"scan_duration\"],\n                )\n            )\n\n            self.Field.insert(\n                    dict(\n                        key,\n                        field_idx=plane_idx,\n                        px_height=pvscan_info[\"height_in_pixels\"],\n                        px_width=pvscan_info[\"width_in_pixels\"],\n                        um_height=pvscan_info[\"height_in_um\"],\n                        um_width=pvscan_info[\"width_in_um\"],\n                        field_x=pvscan_info[\"fieldX\"],\n                        field_y=pvscan_info[\"fieldY\"],\n                        field_z=pvscan_info[\"fieldZ\"] if pvscan_info[\"num_planes\"] == 1 else pvscan_info[\"fieldZ\"][plane_idx],\n                    )\n                    for plane_idx in range(pvscan_info[\"num_planes\"])\n            )\n        else:\n            raise NotImplementedError(\n                f\"Loading routine not implemented for {acq_software} \"\n                \"acquisition software\"\n            )\n\n        # Insert file(s)\n        root_dir = find_root_directory(\n            get_imaging_root_data_dir(), scan_filepaths[0]\n        )\n\n        scan_files = [\n            pathlib.Path(f).relative_to(root_dir).as_posix()\n            for f in scan_filepaths\n        ]\n        self.ScanFile.insert([{**key, \"file_path\": f} for f in scan_files])\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.Field", "title": "<code>Field</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Stores field information of scan, including its coordinates, size, pixel pitch, etc.</p> <p>Attributes:</p> Name Type Description <code>ScanInfo</code> <code>foreign key</code> <p>A primary key from ScanInfo.</p> <code>field_idx</code> <code>int</code> <p>Unique field index.</p> <code>px_height</code> <code>int</code> <p>Image height in pixels.</p> <code>px_width</code> <code>int</code> <p>Image width in pixels.</p> <code>um_height</code> <code>float</code> <p>Image height in microns.</p> <code>um_width</code> <code>float</code> <p>Image width in microns.</p> <code>field_x</code> <code>float</code> <p>X coordinate of the center of field in the motor coordinate system (um).</p> <code>field_y</code> <code>float</code> <p>Y coordinate of the center of field in the motor coordinate system (um).</p> <code>field_z</code> <code>float</code> <p>Relative depth of field (um).</p> <code>delay_image</code> <code>longblob</code> <p>Delay between the start of the scan and pixels in this field (ms).</p> <code>roi</code> <code>int</code> <p>The scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>class Field(dj.Part):\n\"\"\"Stores field information of scan, including its coordinates, size, pixel\n    pitch, etc.\n\n    Attributes:\n        ScanInfo (foreign key): A primary key from ScanInfo.\n        field_idx (int): Unique field index.\n        px_height (int): Image height in pixels.\n        px_width (int): Image width in pixels.\n        um_height (float, optional): Image height in microns.\n        um_width (float, optional): Image width in microns.\n        field_x (float, optional): X coordinate of the center of field in the motor\n            coordinate system (um).\n        field_y (float, optional): Y coordinate of the center of field in the motor\n            coordinate system (um).\n        field_z (float, optional): Relative depth of field (um).\n        delay_image (longblob, optional): Delay between the start of the scan and\n            pixels in this field (ms).\n        roi (int, optional): The scanning roi (as recorded in the acquisition\n            software) containing this field - only relevant to mesoscale scans.\n    \"\"\"\n\n    definition = \"\"\" # field-specific scan information\n    -&gt; master\n    field_idx         : int\n    ---\n    px_height         : smallint  # height in pixels\n    px_width          : smallint  # width in pixels\n    um_height=null    : float     # height in microns\n    um_width=null     : float     # width in microns\n    field_x=null      : float     # (um) center of field in the motor coordinate system\n    field_y=null      : float     # (um) center of field in the motor coordinate system\n    field_z=null      : float     # (um) relative depth of field\n    delay_image=null  : longblob  # (ms) delay between the start of the scan and pixels in this field\n    roi=null          : int       # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.ScanFile", "title": "<code>ScanFile</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Filepath of the scan relative to root data directory.</p> <p>Attributes:</p> Name Type Description <code>ScanInfo</code> <code>foreign key</code> <p>A primary key from ScanInfo.</p> <code>file_path</code> <code>str</code> <p>Path of the scan file relative to the root data directory.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>class ScanFile(dj.Part):\n\"\"\"Filepath of the scan relative to root data directory.\n\n    Attributes:\n        ScanInfo (foreign key): A primary key from ScanInfo.\n        file_path (str): Path of the scan file relative to the root data directory.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    file_path: varchar(255)  # Filepath relative to root data directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.make", "title": "<code>make(key)</code>", "text": "<p>Populate the ScanInfo with the information parsed from image files.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the ScanInfo with the information parsed from image files.\"\"\"\n\n    acq_software = (Scan &amp; key).fetch1(\"acq_software\")\n\n    if acq_software == \"ScanImage\":\n        import scanreader\n\n        # Read the scan\n        scan_filepaths = get_scan_image_files(key)\n        scan = scanreader.read_scan(scan_filepaths)\n\n        # Insert in ScanInfo\n        x_zero = (\n            scan.motor_position_at_zero[0]\n            if scan.motor_position_at_zero\n            else None\n        )\n        y_zero = (\n            scan.motor_position_at_zero[1]\n            if scan.motor_position_at_zero\n            else None\n        )\n        z_zero = (\n            scan.motor_position_at_zero[2]\n            if scan.motor_position_at_zero\n            else None\n        )\n\n        self.insert1(\n            dict(\n                key,\n                nfields=scan.num_fields,\n                nchannels=scan.num_channels,\n                nframes=scan.num_frames,\n                ndepths=scan.num_scanning_depths,\n                x=x_zero,\n                y=y_zero,\n                z=z_zero,\n                fps=scan.fps,\n                bidirectional=scan.is_bidirectional,\n                usecs_per_line=scan.seconds_per_line * 1e6,\n                fill_fraction=scan.temporal_fill_fraction,\n                nrois=scan.num_rois if scan.is_multiROI else 0,\n                scan_duration=scan.num_frames / scan.fps,\n            )\n        )\n        # Insert Field(s)\n        if scan.is_multiROI:\n            self.Field.insert(\n                [\n                    dict(\n                        key,\n                        field_idx=field_id,\n                        px_height=scan.field_heights[field_id],\n                        px_width=scan.field_widths[field_id],\n                        um_height=scan.field_heights_in_microns[field_id],\n                        um_width=scan.field_widths_in_microns[field_id],\n                        field_x=x_zero\n                        + scan._degrees_to_microns(scan.fields[field_id].x)\n                        if x_zero\n                        else None,\n                        field_y=y_zero\n                        + scan._degrees_to_microns(scan.fields[field_id].y)\n                        if y_zero\n                        else None,\n                        field_z=z_zero + scan.fields[field_id].depth\n                        if z_zero\n                        else None,\n                        delay_image=scan.field_offsets[field_id],\n                        roi=scan.field_rois[field_id][0],\n                    )\n                    for field_id in range(scan.num_fields)\n                ]\n            )\n        else:\n            self.Field.insert(\n                [\n                    dict(\n                        key,\n                        field_idx=plane_idx,\n                        px_height=scan.image_height,\n                        px_width=scan.image_width,\n                        um_height=getattr(\n                            scan, \"image_height_in_microns\", None\n                        ),\n                        um_width=getattr(\n                            scan, \"image_width_in_microns\", None\n                        ),\n                        field_x=x_zero if x_zero else None,\n                        field_y=y_zero if y_zero else None,\n                        field_z=z_zero + scan.scanning_depths[plane_idx]\n                        if z_zero\n                        else None,\n                        delay_image=scan.field_offsets[plane_idx],\n                    )\n                    for plane_idx in range(scan.num_scanning_depths)\n                ]\n            )\n    elif acq_software == \"Scanbox\":\n        import sbxreader\n\n        # Read the scan\n        scan_filepaths = get_scan_box_files(key)\n        sbx_meta = sbxreader.sbx_get_metadata(scan_filepaths[0])\n        sbx_matinfo = sbxreader.sbx_get_info(scan_filepaths[0])\n        is_multiROI = bool(\n            sbx_matinfo.mesoscope.enabled\n        )  # currently not handling \"multiROI\" ingestion\n\n        if is_multiROI:\n            raise NotImplementedError(\n                \"Loading routine not implemented for Scanbox multiROI scan mode\"\n            )\n\n        # Insert in ScanInfo\n        x_zero, y_zero, z_zero = sbx_meta[\"stage_pos\"]\n        self.insert1(\n            dict(\n                key,\n                nfields=sbx_meta[\"num_fields\"]\n                if is_multiROI\n                else sbx_meta[\"num_planes\"],\n                nchannels=sbx_meta[\"num_channels\"],\n                nframes=sbx_meta[\"num_frames\"],\n                ndepths=sbx_meta[\"num_planes\"],\n                x=x_zero,\n                y=y_zero,\n                z=z_zero,\n                fps=sbx_meta[\"frame_rate\"],\n                bidirectional=sbx_meta == \"bidirectional\",\n                nrois=sbx_meta[\"num_rois\"] if is_multiROI else 0,\n                scan_duration=(\n                    sbx_meta[\"num_frames\"] / sbx_meta[\"frame_rate\"]\n                ),\n            )\n        )\n        # Insert Field(s)\n        if not is_multiROI:\n            px_width, px_height = sbx_meta[\"frame_size\"]\n            self.Field.insert(\n                [\n                    dict(\n                        key,\n                        field_idx=plane_idx,\n                        px_height=px_height,\n                        px_width=px_width,\n                        um_height=px_height * sbx_meta[\"um_per_pixel_y\"]\n                        if sbx_meta[\"um_per_pixel_y\"]\n                        else None,\n                        um_width=px_width * sbx_meta[\"um_per_pixel_x\"]\n                        if sbx_meta[\"um_per_pixel_x\"]\n                        else None,\n                        field_x=x_zero,\n                        field_y=y_zero,\n                        field_z=z_zero + sbx_meta[\"etl_pos\"][plane_idx],\n                    )\n                    for plane_idx in range(sbx_meta[\"num_planes\"])\n                ]\n            )\n    elif acq_software == \"NIS\":\n        import nd2\n\n        # Read the scan\n        scan_filepaths = get_nd2_files(key)\n        nd2_file = nd2.ND2File(scan_filepaths[0])\n        is_multiROI = False  # MultiROI to be implemented later\n\n        # Frame per second\n        try:\n            fps = (\n                1000\n                / nd2_file.experiment[0]\n                .parameters.periods[0]\n                .periodDiff.avg\n            )\n        except:\n            fps = 1000 / nd2_file.experiment[0].parameters.periodDiff.avg\n\n        # Estimate ND2 file scan duration\n        def estimate_nd2_scan_duration(nd2_scan_obj):\n            # Calculates scan duration for Nikon images\n            ti = (\n                nd2_scan_obj.frame_metadata(0)\n                .channels[0]\n                .time.absoluteJulianDayNumber\n            )  # Initial frame's JD.\n            tf = (\n                nd2_scan_obj.frame_metadata(nd2_scan_obj.shape[0] - 1)\n                .channels[0]\n                .time.absoluteJulianDayNumber\n            )  # Final frame's JD.\n\n            return (tf - ti) * 86400 + 1 / fps\n\n        scan_duration = sum(\n            estimate_nd2_scan_duration(nd2.ND2File(f))\n            for f in scan_filepaths\n        )\n\n        try:\n            scan_datetime = nd2_file.text_info[\"date\"]\n            scan_datetime = datetime.strptime(\n                scan_datetime,\n                \"%m/%d/%Y %H:%M:%S %p\"\n                if re.search((\"AM|PM\"), scan_datetime)\n                else \"%m/%d/%Y %H:%M:%S\",\n            )\n            scan_datetime = datetime.strftime(\n                scan_datetime, \"%Y-%m-%d %H:%M:%S\"\n            )\n        except:\n            scan_datetime = None\n\n        # Insert in ScanInfo\n        self.insert1(\n            dict(\n                key,\n                nfields=nd2_file.sizes.get(\"P\", 1),\n                nchannels=nd2_file.attributes.channelCount,\n                nframes=nd2_file.metadata.contents.frameCount,\n                ndepths=nd2_file.sizes.get(\"Z\", 1),\n                x=None,\n                y=None,\n                z=None,\n                fps=fps,\n                bidirectional=bool(\n                    nd2_file.custom_data[\"GrabberCameraSettingsV1_0\"][\n                        \"GrabberCameraSettings\"\n                    ][\"PropertiesQuality\"][\"ScanDirection\"]\n                ),\n                nrois=0,\n                scan_datetime=scan_datetime,\n                scan_duration=scan_duration,\n            )\n        )\n\n        # MultiROI to be implemented later\n\n        # Insert in Field\n        if not is_multiROI:\n            self.Field.insert(\n                [\n                    dict(\n                        key,\n                        field_idx=plane_idx,\n                        px_height=nd2_file.attributes.heightPx,\n                        px_width=nd2_file.attributes.widthPx,\n                        um_height=nd2_file.attributes.heightPx\n                        * nd2_file.voxel_size().y,\n                        um_width=nd2_file.attributes.widthPx\n                        * nd2_file.voxel_size().x,\n                        field_x=None,\n                        field_y=None,\n                        field_z=None,\n                    )\n                    for plane_idx in range(nd2_file.sizes.get(\"Z\", 1))\n                ]\n            )\n    elif acq_software == \"PrairieView\":\n        from element_interface import prairieviewreader\n\n        scan_filepaths = get_prairieview_files(key)\n        pvscan_info = prairieviewreader.get_pv_metadata(scan_filepaths[0])\n        self.insert1(\n            dict(\n                key,\n                nfields=pvscan_info[\"num_fields\"],\n                nchannels=pvscan_info[\"num_channels\"],\n                ndepths=pvscan_info[\"num_planes\"],\n                nframes=pvscan_info[\"num_frames\"],\n                nrois=pvscan_info[\"num_rois\"],\n                x=pvscan_info[\"x_pos\"],\n                y=pvscan_info[\"y_pos\"],\n                z=pvscan_info[\"z_pos\"],\n                fps=pvscan_info[\"frame_rate\"],\n                bidirectional=pvscan_info[\"bidirectional\"],\n                bidirectional_z=pvscan_info[\"bidirectional_z\"],\n                usecs_per_line=pvscan_info[\"usecs_per_line\"],\n                scan_datetime=pvscan_info[\"scan_datetime\"],\n                scan_duration=pvscan_info[\"scan_duration\"],\n            )\n        )\n\n        self.Field.insert(\n                dict(\n                    key,\n                    field_idx=plane_idx,\n                    px_height=pvscan_info[\"height_in_pixels\"],\n                    px_width=pvscan_info[\"width_in_pixels\"],\n                    um_height=pvscan_info[\"height_in_um\"],\n                    um_width=pvscan_info[\"width_in_um\"],\n                    field_x=pvscan_info[\"fieldX\"],\n                    field_y=pvscan_info[\"fieldY\"],\n                    field_z=pvscan_info[\"fieldZ\"] if pvscan_info[\"num_planes\"] == 1 else pvscan_info[\"fieldZ\"][plane_idx],\n                )\n                for plane_idx in range(pvscan_info[\"num_planes\"])\n        )\n    else:\n        raise NotImplementedError(\n            f\"Loading routine not implemented for {acq_software} \"\n            \"acquisition software\"\n        )\n\n    # Insert file(s)\n    root_dir = find_root_directory(\n        get_imaging_root_data_dir(), scan_filepaths[0]\n    )\n\n    scan_files = [\n        pathlib.Path(f).relative_to(root_dir).as_posix()\n        for f in scan_filepaths\n    ]\n    self.ScanFile.insert([{**key, \"file_path\": f} for f in scan_files])\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanLocation", "title": "<code>ScanLocation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Anatomical location of the scanned region in the brain</p> <p>Attributes:</p> Name Type Description <code>Scan</code> <code>foreign key</code> <p>A primary key from Scan.</p> <code>Locaton</code> <code>foreign key</code> <p>A primary key from Location.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass ScanLocation(dj.Manual):\n\"\"\"Anatomical location of the scanned region in the brain\n\n    Attributes:\n        Scan (foreign key): A primary key from Scan.\n        Locaton (foreign key): A primary key from Location.\n    \"\"\"\n\n    definition = \"\"\" # Anatomical location\n    -&gt; Scan   \n    ---\n    -&gt; Location      \n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.activate", "title": "<code>activate(scan_schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>scan_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>scan</code> module</p> required <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>scan</code> module.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <ul> <li>Session: Parent table to Scan, typically identifying a recording session</li> <li>Equipment: Reference table for Scan, specifying the equipment used for the     acquisition of this scan.</li> <li>Location: Reference table for ScanLocation, specifying the scanned regions's     anatomical location in the brain.</li> </ul> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def activate(\n    scan_schema_name,\n    *,\n    create_schema=True,\n    create_tables=True,\n    linking_module=None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        scan_schema_name (str): Schema name on the database server to activate the\n            `scan` module\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `scan` module.\n\n    Dependencies:\n    Upstream tables:\n        + Session: Parent table to Scan, typically identifying a recording session\n        + Equipment: Reference table for Scan, specifying the equipment used for the\n            acquisition of this scan.\n        + Location: Reference table for ScanLocation, specifying the scanned regions's\n            anatomical location in the brain.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    schema.activate(\n        scan_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Return imaging root data director(y/ies)</p> <p>Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dirs</code> <code>list</code> <p>A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies).</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_imaging_root_data_dir() -&gt; list:\n\"\"\"Return imaging root data director(y/ies)\n\n    Retrieve the root data director(y/ies) containing the imaging data\n    for all subjects/sessions (e.g. acquired ScanImage raw files, output files from\n    processing routines, etc.). All data paths and directories in DataJoint Elements are\n    recommended to be stored as relative paths (posix format), with respect to some\n    user-configured \"root\" directory, which varies from machine to machine\n    (e.g. different mounted drive locations).\n\n    Returns:\n        dirs (list): A list of string(s) or Path(s) for the absolute paths of the imaging root data\n            director(y/ies).\n    \"\"\"\n\n    root_directories = _linking_module.get_imaging_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_nd2_files", "title": "<code>get_nd2_files(scan_key)</code>", "text": "<p>Retrieve the list of Nikon files (*.nd2) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Nikon files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_nd2_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Nikon files (*.nd2) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Nikon files' full file-paths.\n    \"\"\"\n    return _linking_module.get_nd2_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_prairieview_files", "title": "<code>get_prairieview_files(scan_key)</code>", "text": "<p>Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Bruker PrairieView files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_prairieview_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Bruker PrairieView files' full file-paths.\n    \"\"\"\n    return _linking_module.get_prairieview_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Retrieve the root directory for all processed data.</p> <p>All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dir</code> <code>str | pathlib.Path</code> <p>Absolute path of the pocessed imaging root data directory.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_processed_root_data_dir() -&gt; Union[str, pathlib.Path]:\n\"\"\"Retrieve the root directory for all processed data.\n\n    All data paths and directories in DataJoint Elements are recommended to be stored as\n    relative paths (posix format), with respect to some user-configured \"root\"\n    directory, which varies from machine to machine (e.g. different mounted drive\n    locations).\n\n    Returns:\n        dir (str| pathlib.Path): Absolute path of the pocessed imaging root data\n            directory.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_imaging_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_scan_box_files", "title": "<code>get_scan_box_files(scan_key)</code>", "text": "<p>Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Scanbox files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_box_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Scanbox files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_box_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_scan_image_files", "title": "<code>get_scan_image_files(scan_key)</code>", "text": "<p>Retrieve the list of ScanImage files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of ScanImage files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_image_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of ScanImage files associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of ScanImage files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_image_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/version/", "title": "version.py", "text": "<p>Package metadata.</p>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/", "title": "cell_plot.py", "text": ""}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.figure_data", "title": "<code>figure_data(imaging, segmentation_key)</code>", "text": "<p>Prepare the images for a given segmentation_key.</p> <p>Parameters:</p> Name Type Description Default <code>imaging</code> <code>dj.Table</code> <p>imaging table.</p> required <code>segmentation_key</code> <code>dict</code> <p>A primary key from Segmentation table.</p> required <p>Returns:</p> Name Type Description <code>background_with_cells</code> <code>np.array</code> <p>Average image with transparently overlayed cells.</p> <code>cells_maskid_image</code> <code>np.array</code> <p>Mask ID image.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def figure_data(imaging, segmentation_key):\n\"\"\"Prepare the images for a given segmentation_key.\n\n    Args:\n        imaging (dj.Table): imaging table.\n        segmentation_key (dict): A primary key from Segmentation table.\n\n    Returns:\n        background_with_cells (np.array): Average image with transparently overlayed\n            cells.\n        cells_maskid_image (np.array): Mask ID image.\n    \"\"\"\n\n    image = (imaging.MotionCorrection.Summary &amp; segmentation_key).fetch1(\n        \"average_image\"\n    )\n\n    cell_mask_ids, mask_xpix, mask_ypix = (\n        imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n        &amp; segmentation_key\n    ).fetch(\"mask\", \"mask_xpix\", \"mask_ypix\")\n\n    background_with_cells, cells_maskid_image = mask_overlayed_image(\n        image, mask_xpix, mask_ypix, cell_mask_ids, low_q=0, high_q=0.99\n    )\n\n    return background_with_cells, cells_maskid_image\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.get_tracelayout", "title": "<code>get_tracelayout(key, width=600, height=600)</code>", "text": "<p>Returns a dictionary of layout settings for the trace figures.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def get_tracelayout(key, width=600, height=600):\n\"\"\"Returns a dictionary of layout settings for the trace figures.\"\"\"\n    text = f\"Trace for Cell {key['mask']}\" if isinstance(key, dict) else \"Trace\"\n\n    return dict(\n        margin=dict(l=0, r=0, b=0, t=65, pad=0),\n        width=width,\n        height=height,\n        transition={\"duration\": 0},\n        title={\n            \"text\": text,\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n            \"y\": 0.97,\n            \"x\": 0.5,\n        },\n        xaxis={\n            \"title\": \"Time (sec)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": True,\n        },\n        yaxis={\n            \"title\": \"Fluorescence (a.u.)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": True,\n            \"anchor\": \"free\",\n            \"overlaying\": \"y\",\n            \"side\": \"left\",\n            \"position\": 0,\n        },\n        yaxis2={\n            \"title\": \"Calcium Event (a.u.)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": True,\n            \"anchor\": \"free\",\n            \"overlaying\": \"y\",\n            \"side\": \"right\",\n            \"position\": 1,\n        },\n        shapes=[\n            go.layout.Shape(\n                type=\"rect\",\n                xref=\"paper\",\n                yref=\"paper\",\n                x0=0,\n                y0=0,\n                x1=1.0,\n                y1=1.0,\n                line={\"width\": 1, \"color\": \"black\"},\n            )\n        ],\n        legend={\n            \"traceorder\": \"normal\",\n            \"yanchor\": \"top\",\n            \"y\": 0.99,\n            \"xanchor\": \"right\",\n            \"x\": 0.99,\n        },\n        plot_bgcolor=\"rgba(0,0,0,0.05)\",\n        modebar_remove=[\n            \"zoom\",\n            \"resetScale\",\n            \"pan\",\n            \"select\",\n            \"zoomIn\",\n            \"zoomOut\",\n            \"autoScale2d\",\n        ],\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.mask_overlayed_image", "title": "<code>mask_overlayed_image(image, mask_xpix, mask_ypix, cell_mask_ids, low_q=0, high_q=0.99)</code>", "text": "<p>Overlay transparent cell masks on average image.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def mask_overlayed_image(\n    image, mask_xpix, mask_ypix, cell_mask_ids, low_q=0, high_q=0.99\n):\n\"\"\"Overlay transparent cell masks on average image.\"\"\"\n\n    q_min, q_max = np.quantile(image, [low_q, high_q])\n    image = np.clip(image, q_min, q_max)\n    image = (image - q_min) / (q_max - q_min)\n\n    SATURATION = 0.7\n    image = image[:, :, None] * np.array([0, 0, 1])\n    maskid_image = np.full(image.shape[:2], -1)\n    for xpix, ypix, roi_id in zip(mask_xpix, mask_ypix, cell_mask_ids):\n        image[ypix, xpix, :2] = [np.random.rand(), SATURATION]\n        maskid_image[ypix, xpix] = roi_id\n    image = (colors.hsv_to_rgb(image) * 255).astype(int)\n    return image, maskid_image\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.plot_cell_overlayed_image", "title": "<code>plot_cell_overlayed_image(imaging, segmentation_key)</code>", "text": "<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>imaging</code> <code>dj.Table</code> <p>imaging table.</p> required <code>segmentation_key</code> <code>dict</code> <p>A primary key from Segmentation table.</p> required <p>Returns:</p> Name Type Description <code>image_fig</code> <code>plotly.Fig</code> <p>Plotly figure object of the average image with transparently overlayed cells.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def plot_cell_overlayed_image(imaging, segmentation_key):\n\"\"\"_summary_\n\n    Args:\n        imaging (dj.Table): imaging table.\n        segmentation_key (dict): A primary key from Segmentation table.\n\n    Returns:\n        image_fig (plotly.Fig): Plotly figure object of the average image with\n            transparently overlayed cells.\n    \"\"\"\n\n    background_with_cells, cells_maskid_image = figure_data(imaging, segmentation_key)\n\n    image_fig = go.Figure(\n        go.Image(\n            z=background_with_cells,\n            hovertemplate=\"x: %{x} &lt;br&gt;y: %{y} &lt;br&gt;mask_id: %{customdata}\",\n            customdata=cells_maskid_image,\n        )\n    )\n    image_fig.update_layout(\n        title=\"Average Image with Cells\",\n        xaxis={\n            \"title\": \"X (px)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": False,\n        },\n        yaxis={\n            \"title\": \"Y (px)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": False,\n        },\n        paper_bgcolor=\"rgba(0,0,0,0)\",\n        plot_bgcolor=\"rgba(0,0,0,0)\",\n    )\n\n    return image_fig\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.plot_cell_traces", "title": "<code>plot_cell_traces(imaging, cell_key)</code>", "text": "<p>Prepare plotly trace figure.</p> <p>Parameters:</p> Name Type Description Default <code>imaging</code> <code>dj.Table</code> <p>imaging table.</p> required <code>cell_key</code> <code>dict</code> <p>A primary key from imaging.Activity.Trace table.</p> required <p>Returns:</p> Name Type Description <code>trace_fig</code> <p>Plotly figure object of the traces.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def plot_cell_traces(imaging, cell_key):\n\"\"\"Prepare plotly trace figure.\n\n    Args:\n        imaging (dj.Table): imaging table.\n        cell_key (dict): A primary key from imaging.Activity.Trace table.\n\n    Returns:\n        trace_fig: Plotly figure object of the traces.\n    \"\"\"\n    activity_trace = (\n        imaging.Activity.Trace &amp; \"extraction_method LIKE '%deconvolution'\" &amp; cell_key\n    ).fetch1(\"activity_trace\")\n    fluorescence, fps = (scan.ScanInfo * imaging.Fluorescence.Trace &amp; cell_key).fetch1(\n        \"fluorescence\", \"fps\"\n    )\n\n    trace_fig = go.Figure(\n        [\n            go.Scatter(\n                x=np.arange(len(fluorescence)) / fps,\n                y=fluorescence,\n                name=\"Fluorescence\",\n                yaxis=\"y1\",\n            ),\n            go.Scatter(\n                x=np.arange(len(activity_trace)) / fps,\n                y=activity_trace,\n                name=\"Calcium Event\",\n                yaxis=\"y2\",\n            ),\n        ]\n    )\n\n    trace_fig.update_layout(get_tracelayout(cell_key))\n\n    return trace_fig\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/widget/", "title": "widget.py", "text": ""}, {"location": "api/element_calcium_imaging/plotting/widget/#element_calcium_imaging.plotting.widget.main", "title": "<code>main(imaging, usedb=False)</code>", "text": "<p>Display the widget.</p> <p>Parameters:</p> Name Type Description Default <code>imaging</code> <code>dj.Table</code> <p>imaging table in the database.</p> required <code>usedb</code> <code>bool</code> <p>Whether to use the figures in the database or compute the figures on the fly.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>widget</code> <p>Widget to display the figures.</p> Source code in <code>element_calcium_imaging/plotting/widget.py</code> <pre><code>def main(imaging, usedb=False):\n\"\"\"Display the widget.\n\n    Args:\n        imaging (dj.Table): imaging table in the database.\n        usedb (bool, optional): Whether to use the figures in the database or compute\n            the figures on the fly.\n\n    Returns:\n        widget: Widget to display the figures.\n    \"\"\"\n\n    motioncorrection_dropdown = wg.Dropdown(\n        options=imaging.Segmentation.fetch(\"KEY\"),\n        description=\"Result:\",\n        description_tooltip='Press \"Load\" to visualize the cells identified.',\n        disabled=False,\n        layout=wg.Layout(\n            width=\"95%\",\n            display=\"flex\",\n            flex_flow=\"row\",\n            justify_content=\"space-between\",\n            grid_area=\"motioncorrection_dropdown\",\n        ),\n        style={\"description_width\": \"80px\"},\n    )\n\n    load_button = wg.Button(\n        description=\"Load Image\",\n        tooltip=\"Load the average image.\",\n        layout=wg.Layout(width=\"120px\", grid_area=\"load_button\"),\n    )\n\n    FIG1_WIDTH = 600\n    FIG1_LAYOUT = go.Layout(\n        margin=dict(l=0, r=40, b=0, t=65, pad=0),\n        width=FIG1_WIDTH,\n        height=600,\n        transition={\"duration\": 0},\n        title={\n            \"text\": \"Average Image with Cells\",\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n            \"y\": 0.97,\n            \"x\": 0.5,\n        },\n        xaxis={\n            \"title\": \"X (px)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": False,\n        },\n        yaxis={\n            \"title\": \"Y (px)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": False,\n        },\n        paper_bgcolor=\"rgba(0,0,0,0)\",\n        plot_bgcolor=\"rgba(0,0,0,0)\",\n        modebar_remove=[\n            \"zoom\",\n            \"resetScale\",\n            \"pan\",\n            \"select\",\n            \"zoomIn\",\n            \"zoomOut\",\n            \"autoScale2d\",\n        ],\n        shapes=[\n            go.layout.Shape(\n                type=\"rect\",\n                xref=\"paper\",\n                yref=\"paper\",\n                x0=0.035,\n                y0=0,\n                x1=0.965,\n                y1=1.0,\n                line={\"width\": 1, \"color\": \"black\"},\n            )\n        ],\n    )\n    fig1 = go.Figure(\n        go.Image(\n            z=None,\n            hovertemplate=\"x: %{x} &lt;br&gt;y: %{y} &lt;br&gt;mask_id: %{customdata} &lt;extra&gt;&lt;/extra&gt;\",\n            customdata=None,\n        ),\n        layout=FIG1_LAYOUT,\n    )\n\n    FIG2_WIDTH = 600\n    FIG2_HEIGHT = 600\n    fig2_layout = cell_plot.get_tracelayout(None, width=FIG2_WIDTH, height=FIG2_HEIGHT)\n\n    fig2 = go.Figure(\n        [\n            go.Scatter(\n                x=None,\n                y=None,\n                name=\"Fluorescence\",\n                yaxis=\"y1\",\n            ),\n            go.Scatter(x=None, y=None, name=\"Calcium Event\", yaxis=\"y2\"),\n        ],\n        layout=fig2_layout,\n    )\n\n    fig1_widget = go.FigureWidget(fig1)\n    fig2_widget = go.FigureWidget(fig2)\n\n    def tooltip_click(trace, points, selector):\n        mask_id = trace.customdata[points.ys[0]][points.xs[0]]\n\n        if mask_id &gt; -1:\n            cell_traces_figobj = from_json(\n                (\n                    TraceReport &amp; motioncorrection_dropdown.value &amp; f\"mask='{mask_id}'\"\n                ).fetch1(\"cell_traces\")\n            )\n\n            with fig2_widget.batch_update():\n                fig2_widget.data[0].x = cell_traces_figobj.data[0].x\n                fig2_widget.data[0].y = cell_traces_figobj.data[0].y\n                fig2_widget.data[0].name = cell_traces_figobj.data[0].name\n                fig2_widget.data[1].x = cell_traces_figobj.data[1].x\n                fig2_widget.data[1].y = cell_traces_figobj.data[1].y\n                fig2_widget.data[1].name = cell_traces_figobj.data[1].name\n                fig2_widget.layout[\"title\"] = {\n                    \"text\": f\"Trace for Cell {mask_id}\",\n                    \"xanchor\": \"center\",\n                    \"yanchor\": \"top\",\n                    \"y\": 0.97,\n                    \"x\": 0.5,\n                }\n\n    def response(change, usedb=False):\n        if usedb:\n            cell_overlayed_image = from_json(\n                (ScanLevelReport &amp; motioncorrection_dropdown.value).fetch1(\n                    \"cell_overlayed_image\"\n                )\n            )\n\n            with fig1_widget.batch_update():\n                fig1_widget.data[0].z = cell_overlayed_image.data[0].z\n                fig1_widget.data[0].customdata = cell_overlayed_image.data[0].customdata\n\n                fig2_widget.data[0].x = None\n                fig2_widget.data[0].y = None\n                fig2_widget.data[1].x = None\n                fig2_widget.data[1].y = None\n        else:\n            background_with_cells, cells_maskid_image = cell_plot.figure_data(\n                imaging, motioncorrection_dropdown.value\n            )\n\n            with fig1_widget.batch_update():\n                fig1_widget.data[0].z = background_with_cells\n                fig1_widget.data[0].customdata = cells_maskid_image\n\n                fig2_widget.layout.title = {\n                    \"text\": \"Trace\",\n                    \"xanchor\": \"center\",\n                    \"yanchor\": \"top\",\n                    \"y\": 0.97,\n                    \"x\": 0.5,\n                }\n                fig2_widget.data[0].x = None\n                fig2_widget.data[0].y = None\n                fig2_widget.data[1].x = None\n                fig2_widget.data[1].y = None\n\n    fig1_widget.data[0].on_click(tooltip_click)\n    load_button.on_click(partial(response, usedb=usedb))\n\n    return wg.VBox(\n        [\n            wg.HBox(\n                [motioncorrection_dropdown, load_button],\n                layout=wg.Layout(width=f\"{FIG1_WIDTH+FIG2_WIDTH}px\"),\n            ),\n            wg.HBox([fig1_widget, fig2_widget]),\n        ]\n    )\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/", "title": "analysis.py", "text": ""}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment", "title": "<code>ActivityAlignment</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Attributes:</p> Name Type Description <code>ActivityAlignmentCondition</code> <code>foreign key</code> <p>Primary key from ActivityAlignmentCondition.</p> <code>aligned_timestamps</code> <code>longblob</code> <p>Aligned timestamps.</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>@schema\nclass ActivityAlignment(dj.Computed):\n\"\"\"\n    Attributes:\n        ActivityAlignmentCondition (foreign key): Primary key from\n            ActivityAlignmentCondition.\n        aligned_timestamps (longblob): Aligned timestamps.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ActivityAlignmentCondition\n    ---\n    aligned_timestamps: longblob\n    \"\"\"\n\n    class AlignedTrialActivity(dj.Part):\n\"\"\"Aligned trial activity.\n\n        Attributes:\n            ActivityAlignment (foriegn key): Primary key from ActivityAlignment.\n            imaging.Activity.Trace (foriegn key): Primary key from\n                imaging.Activity.Trace.\n            ActivityAlignmentCondition.Trial (foreign key): Primary key from\n                ActivityAlignmentCondition.Trial.\n            aligned_trace (longblob): Calcium activity aligned to the event time (s).\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; imaging.Activity.Trace\n        -&gt; ActivityAlignmentCondition.Trial\n        ---\n        aligned_trace: longblob  # (s) Calcium activity aligned to the event time\n        \"\"\"\n\n    def make(self, key):\n        sess_time, scan_time, nframes, frame_rate = (\n            _linking_module.scan.ScanInfo * _linking_module.session.Session &amp; key\n        ).fetch1(\"session_datetime\", \"scan_datetime\", \"nframes\", \"fps\")\n\n        trialized_event_times = (\n            _linking_module.trial.get_trialized_alignment_event_times(\n                key,\n                _linking_module.trial.Trial &amp; (ActivityAlignmentCondition.Trial &amp; key),\n            )\n        )\n\n        min_limit = (trialized_event_times.event - trialized_event_times.start).max()\n        max_limit = (trialized_event_times.end - trialized_event_times.event).max()\n\n        aligned_timestamps = np.arange(-min_limit, max_limit, 1 / frame_rate)\n        nsamples = len(aligned_timestamps)\n\n        trace_keys, activity_traces = (\n            _linking_module.imaging.Activity.Trace &amp; key\n        ).fetch(\"KEY\", \"activity_trace\", order_by=\"mask\")\n        activity_traces = np.vstack(activity_traces)\n\n        aligned_trial_activities = []\n        for _, r in trialized_event_times.iterrows():\n            if r.event is None or np.isnan(r.event):\n                continue\n            alignment_start_idx = int((r.event - min_limit) * frame_rate)\n            roi_aligned_activities = activity_traces[\n                :, alignment_start_idx : (alignment_start_idx + nsamples)\n            ]\n            if roi_aligned_activities.shape[-1] != nsamples:\n                shape_diff = nsamples - roi_aligned_activities.shape[-1]\n                roi_aligned_activities = np.pad(\n                    roi_aligned_activities,\n                    ((0, 0), (0, shape_diff)),\n                    mode=\"constant\",\n                    constant_values=np.nan,\n                )\n\n            aligned_trial_activities.extend(\n                [\n                    {**key, **r.trial_key, **trace_key, \"aligned_trace\": aligned_trace}\n                    for trace_key, aligned_trace in zip(\n                        trace_keys, roi_aligned_activities\n                    )\n                ]\n            )\n\n        self.insert1({**key, \"aligned_timestamps\": aligned_timestamps})\n        self.AlignedTrialActivity.insert(aligned_trial_activities)\n\n    def plot_aligned_activities(self, key, roi, axs=None, title=None):\n\"\"\"Plot event-aligned activities for selected trials, and trial-averaged\n            activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.).\n\n        Args:\n            key (dict): key of ActivityAlignment master table\n            roi (int): imaging segmentation mask\n            axs (matplotlib.ax): optional definition of axes for plot.\n                Default is plt.subplots(2, 1, figsize=(12, 8))\n            title (str): Optional title label\n\n        Returns:\n            fig (matplotlib.pyplot.figure): Figure of the event aligned activities.\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        fig = None\n        if axs is None:\n            fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(12, 8))\n        else:\n            ax0, ax1 = axs\n\n        aligned_timestamps = (self &amp; key).fetch1(\"aligned_timestamps\")\n        trial_ids, aligned_spikes = (\n            self.AlignedTrialActivity &amp; key &amp; {\"mask\": roi}\n        ).fetch(\"trial_id\", \"aligned_trace\", order_by=\"trial_id\")\n\n        aligned_spikes = np.vstack(aligned_spikes)\n\n        ax0.imshow(\n            aligned_spikes,\n            cmap=\"inferno\",\n            interpolation=\"nearest\",\n            aspect=\"auto\",\n            extent=(\n                aligned_timestamps[0],\n                aligned_timestamps[-1],\n                0,\n                aligned_spikes.shape[0],\n            ),\n        )\n        ax0.axvline(x=0, linestyle=\"--\", color=\"white\")\n        ax0.set_axis_off()\n\n        ax1.plot(aligned_timestamps, np.nanmean(aligned_spikes, axis=0))\n        ax1.axvline(x=0, linestyle=\"--\", color=\"black\")\n        ax1.set_xlabel(\"Time (s)\")\n        ax1.set_xlim(aligned_timestamps[0], aligned_timestamps[-1])\n\n        if title:\n            plt.suptitle(title)\n\n        return fig\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment.AlignedTrialActivity", "title": "<code>AlignedTrialActivity</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Aligned trial activity.</p> <p>Attributes:</p> Name Type Description <code>ActivityAlignment</code> <code>foriegn key</code> <p>Primary key from ActivityAlignment.</p> <code>imaging.Activity.Trace</code> <code>foriegn key</code> <p>Primary key from imaging.Activity.Trace.</p> <code>ActivityAlignmentCondition.Trial</code> <code>foreign key</code> <p>Primary key from ActivityAlignmentCondition.Trial.</p> <code>aligned_trace</code> <code>longblob</code> <p>Calcium activity aligned to the event time (s).</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>class AlignedTrialActivity(dj.Part):\n\"\"\"Aligned trial activity.\n\n    Attributes:\n        ActivityAlignment (foriegn key): Primary key from ActivityAlignment.\n        imaging.Activity.Trace (foriegn key): Primary key from\n            imaging.Activity.Trace.\n        ActivityAlignmentCondition.Trial (foreign key): Primary key from\n            ActivityAlignmentCondition.Trial.\n        aligned_trace (longblob): Calcium activity aligned to the event time (s).\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; imaging.Activity.Trace\n    -&gt; ActivityAlignmentCondition.Trial\n    ---\n    aligned_trace: longblob  # (s) Calcium activity aligned to the event time\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment.plot_aligned_activities", "title": "<code>plot_aligned_activities(key, roi, axs=None, title=None)</code>", "text": "<p>Plot event-aligned activities for selected trials, and trial-averaged     activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key of ActivityAlignment master table</p> required <code>roi</code> <code>int</code> <p>imaging segmentation mask</p> required <code>axs</code> <code>matplotlib.ax</code> <p>optional definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8))</p> <code>None</code> <code>title</code> <code>str</code> <p>Optional title label</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.pyplot.figure</code> <p>Figure of the event aligned activities.</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>def plot_aligned_activities(self, key, roi, axs=None, title=None):\n\"\"\"Plot event-aligned activities for selected trials, and trial-averaged\n        activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.).\n\n    Args:\n        key (dict): key of ActivityAlignment master table\n        roi (int): imaging segmentation mask\n        axs (matplotlib.ax): optional definition of axes for plot.\n            Default is plt.subplots(2, 1, figsize=(12, 8))\n        title (str): Optional title label\n\n    Returns:\n        fig (matplotlib.pyplot.figure): Figure of the event aligned activities.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    fig = None\n    if axs is None:\n        fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(12, 8))\n    else:\n        ax0, ax1 = axs\n\n    aligned_timestamps = (self &amp; key).fetch1(\"aligned_timestamps\")\n    trial_ids, aligned_spikes = (\n        self.AlignedTrialActivity &amp; key &amp; {\"mask\": roi}\n    ).fetch(\"trial_id\", \"aligned_trace\", order_by=\"trial_id\")\n\n    aligned_spikes = np.vstack(aligned_spikes)\n\n    ax0.imshow(\n        aligned_spikes,\n        cmap=\"inferno\",\n        interpolation=\"nearest\",\n        aspect=\"auto\",\n        extent=(\n            aligned_timestamps[0],\n            aligned_timestamps[-1],\n            0,\n            aligned_spikes.shape[0],\n        ),\n    )\n    ax0.axvline(x=0, linestyle=\"--\", color=\"white\")\n    ax0.set_axis_off()\n\n    ax1.plot(aligned_timestamps, np.nanmean(aligned_spikes, axis=0))\n    ax1.axvline(x=0, linestyle=\"--\", color=\"black\")\n    ax1.set_xlabel(\"Time (s)\")\n    ax1.set_xlim(aligned_timestamps[0], aligned_timestamps[-1])\n\n    if title:\n        plt.suptitle(title)\n\n    return fig\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignmentCondition", "title": "<code>ActivityAlignmentCondition</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Activity alignment condition.</p> <p>Attributes:</p> Name Type Description <code>imaging.Activity</code> <code>foreign key</code> <p>Primary key from imaging.Activity.</p> <code>event.AlignmentEvent</code> <code>foreign key</code> <p>Primary key from event.AlignmentEvent.</p> <code>trial_condition</code> <code>str</code> <p>User-friendly name of condition.</p> <code>bin_size</code> <code>float</code> <p>bin-size (in second) used to compute the PSTH,</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>@schema\nclass ActivityAlignmentCondition(dj.Manual):\n\"\"\"Activity alignment condition.\n\n    Attributes:\n        imaging.Activity (foreign key): Primary key from imaging.Activity.\n        event.AlignmentEvent (foreign key): Primary key from event.AlignmentEvent.\n        trial_condition (str): User-friendly name of condition.\n        condition_description (str, optional). Description. Default is ''.\n        bin_size (float): bin-size (in second) used to compute the PSTH,\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; imaging.Activity\n    -&gt; event.AlignmentEvent\n    trial_condition: varchar(128) # user-friendly name of condition\n    ---\n    condition_description='': varchar(1000)\n    bin_size=0.04: float # bin-size (in second) used to compute the PSTH\n    \"\"\"\n\n    class Trial(dj.Part):\n\"\"\"Trial\n\n        Attributes:\n            ActivityAlignmentCondition (foreign key): Primary key from\n                ActivityAlignmentCondition.\n            trial.Trial: Primary key from trial.Trial.\n        \"\"\"\n\n        definition = \"\"\"  # Trials (or subset) to compute event-aligned activity\n        -&gt; master\n        -&gt; trial.Trial\n        \"\"\"\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignmentCondition.Trial", "title": "<code>Trial</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trial</p> <p>Attributes:</p> Name Type Description <code>ActivityAlignmentCondition</code> <code>foreign key</code> <p>Primary key from ActivityAlignmentCondition.</p> <code>trial.Trial</code> <code>foreign key</code> <p>Primary key from trial.Trial.</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>class Trial(dj.Part):\n\"\"\"Trial\n\n    Attributes:\n        ActivityAlignmentCondition (foreign key): Primary key from\n            ActivityAlignmentCondition.\n        trial.Trial: Primary key from trial.Trial.\n    \"\"\"\n\n    definition = \"\"\"  # Trials (or subset) to compute event-aligned activity\n    -&gt; master\n    -&gt; trial.Trial\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.activate", "title": "<code>activate(schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>subject</code> element.</p> required <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>subject</code> element: Upstream schema: scan, session, trial.</p> <code>None</code> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>def activate(\n    schema_name, *, create_schema=True, create_tables=True, linking_module=None\n):\n\"\"\"Activate this schema.\n\n    Args:\n        schema_name (str): Schema name on the database server to activate the `subject`\n            element.\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `subject` element: Upstream schema: scan,\n            session, trial.\n    \"\"\"\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(linking_module), (\n        \"The argument 'dependency' must \" + \"be a module's name or a module\"\n    )\n\n    global _linking_module\n    _linking_module = linking_module\n\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/", "title": "ingest.py", "text": ""}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_alignment", "title": "<code>ingest_alignment(alignment_csv_path='./user_data/alignments.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingest event alignment information</p> <p>This is duplicated across wf-array-ephys and wf-calcium-imaging.</p> <p>Parameters:</p> Name Type Description Default <code>alignment_csv_path</code> <code>str</code> <p>relative path of alignments.csv</p> <code>'./user_data/alignments.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Display number of entries inserted when ingesting. Default True.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/ingest.py</code> <pre><code>def ingest_alignment(\n    alignment_csv_path=\"./user_data/alignments.csv\", skip_duplicates=True, verbose=True\n):\n\"\"\"Ingest event alignment information\n\n    This is duplicated across wf-array-ephys and wf-calcium-imaging.\n\n    Args:\n        alignment_csv_path (str): relative path of alignments.csv\n        skip_duplicates (bool, optional): Default True. Passed to DataJoint insert.\n        verbose (bool, optional): Display number of entries inserted when ingesting.\n            Default True.\n    \"\"\"\n\n    csvs = [alignment_csv_path]\n    tables = [event.AlignmentEvent()]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_events", "title": "<code>ingest_events(recording_csv_path='./user_data/behavior_recordings.csv', block_csv_path='./user_data/blocks.csv', trial_csv_path='./user_data/trials.csv', event_csv_path='./user_data/events.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingest session, block, trial, and event data.</p> <p>Ingest each level of experiment hierarchy for element-trial: recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurances within trial).</p> <p>This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging.</p> <p>Parameters:</p> Name Type Description Default <code>recording_csv_path</code> <code>str</code> <p>relative path of behavior_recordings.csv.</p> <code>'./user_data/behavior_recordings.csv'</code> <code>block_csv_path</code> <code>str</code> <p>relative path of blocks.csv.</p> <code>'./user_data/blocks.csv'</code> <code>trial_csv_path</code> <code>str</code> <p>relative path of trials.csv.</p> <code>'./user_data/trials.csv'</code> <code>event_csv_path</code> <code>str</code> <p>relative path of events.csv.</p> <code>'./user_data/events.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Display number of entries inserted when ingesting. Default True.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/ingest.py</code> <pre><code>def ingest_events(\n    recording_csv_path=\"./user_data/behavior_recordings.csv\",\n    block_csv_path=\"./user_data/blocks.csv\",\n    trial_csv_path=\"./user_data/trials.csv\",\n    event_csv_path=\"./user_data/events.csv\",\n    skip_duplicates=True,\n    verbose=True,\n):\n\"\"\"\n    Ingest session, block, trial, and event data.\n\n    Ingest each level of experiment hierarchy for element-trial: recording, block (i.e.,\n    phases of trials), trials (repeated units), events (optionally 0-duration occurances\n    within trial).\n\n    This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging.\n\n    Args:\n        recording_csv_path (str, optional): relative path of behavior_recordings.csv.\n        block_csv_path (str, optional): relative path of blocks.csv.\n        trial_csv_path (str, optional): relative path of trials.csv.\n        event_csv_path (str, optional): relative path of events.csv.\n        skip_duplicates (bool, optional): Default True. Passed to DataJoint insert.\n        verbose (bool, optional): Display number of entries inserted when ingesting.\n            Default True.\n    \"\"\"\n    csvs = [\n        recording_csv_path,\n        recording_csv_path,\n        block_csv_path,\n        block_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        event_csv_path,\n        event_csv_path,\n        event_csv_path,\n    ]\n    tables = [\n        event.BehaviorRecording(),\n        event.BehaviorRecording.File(),\n        trial.Block(),\n        trial.Block.Attribute(),\n        trial.TrialType(),\n        trial.Trial(),\n        trial.Trial.Attribute(),\n        trial.BlockTrial(),\n        event.EventType(),\n        event.Event(),\n        trial.TrialEvent(),\n    ]\n\n    # Allow direct insert required bc element-trial has Imported that should be Manual\n    ingest_csv_to_table(\n        csvs,\n        tables,\n        skip_duplicates=skip_duplicates,\n        verbose=verbose,\n        allow_direct_insert=True,\n    )\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_sessions", "title": "<code>ingest_sessions(session_csv_path='./user_data/sessions.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingests all the manual table starting from session schema from ./user_data/sessions.csv.</p> <p>Parameters:</p> Name Type Description Default <code>session_csv_path</code> <code>str</code> <p>relative path of session csv.</p> <code>'./user_data/sessions.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Default True. Display number of entries inserted when ingesting.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/ingest.py</code> <pre><code>def ingest_sessions(\n    session_csv_path=\"./user_data/sessions.csv\", skip_duplicates=True, verbose=True\n):\n\"\"\"Ingests all the manual table starting from session schema from\n    ./user_data/sessions.csv.\n\n    Args:\n        session_csv_path (str): relative path of session csv.\n        skip_duplicates (bool): Default True. Passed to DataJoint insert.\n        verbose (bool): Default True. Display number of entries inserted when ingesting.\n    \"\"\"\n\n    root_data_dir = get_imaging_root_data_dir()\n\n    # ---------- Insert new \"Session\" and \"Scan\" ---------\n    with open(session_csv_path, newline=\"\") as f:\n        input_sessions = list(csv.DictReader(f, delimiter=\",\"))\n\n    # Folder structure: root / subject / session / .tif (raw)\n    session_list, session_dir_list, scan_list, scanner_list = [], [], [], []\n\n    for sess in input_sessions:\n        sess_dir = find_full_path(root_data_dir, Path(sess[\"session_dir\"]))\n\n        # search for either ScanImage or Scanbox files (in that order)\n        for scan_pattern, scan_type, glob_func in zip(\n            [\"*.tif\", \"*.sbx\"],\n            [\"ScanImage\", \"Scanbox\"],\n            [sess_dir.glob, sess_dir.rglob],\n        ):\n            scan_filepaths = [fp.as_posix() for fp in glob_func(scan_pattern)]\n            if len(scan_filepaths):\n                acq_software = scan_type\n                break\n        else:\n            raise FileNotFoundError(\n                \"Unable to identify scan files from the supported \"\n                + \"acquisition softwares (ScanImage, Scanbox) at: \"\n                + f\"{sess_dir}\"\n            )\n\n        if acq_software == \"ScanImage\":\n            import scanreader\n            from element_interface import scanimage_utils\n\n            try:  # attempt to read .tif as a scanimage file\n                loaded_scan = scanreader.read_scan(scan_filepaths)\n                recording_time = scanimage_utils.get_scanimage_acq_time(loaded_scan)\n                header = scanimage_utils.parse_scanimage_header(loaded_scan)\n                scanner = header[\"SI_imagingSystem\"].strip(\"'\")\n            except Exception as e:\n                print(f\"ScanImage loading error: {scan_filepaths}\\n{str(e)}\")\n                continue\n        elif acq_software == \"Scanbox\":\n            import sbxreader\n\n            try:  # attempt to load Scanbox\n                sbx_fp = pathlib.Path(scan_filepaths[0])\n                sbx_meta = sbxreader.sbx_get_metadata(sbx_fp)\n                # read from file when Scanbox support this\n                recording_time = datetime.fromtimestamp(sbx_fp.stat().st_ctime)\n                scanner = sbx_meta.get(\"imaging_system\", \"Scanbox\")\n            except Exception as e:\n                print(f\"Scanbox loading error: {scan_filepaths}\\n{str(e)}\")\n                continue\n        else:\n            raise NotImplementedError(\n                \"Processing scan from acquisition software of \"\n                + f\"type {acq_software} is not yet implemented\"\n            )\n\n        session_key = {\"subject\": sess[\"subject\"], \"session_datetime\": recording_time}\n        if session_key not in session.Session():\n            scanner_list.append({\"scanner\": scanner})\n            session_list.append(session_key)\n            scan_list.append(\n                {\n                    **session_key,\n                    \"scan_id\": 0,\n                    \"scanner\": scanner,\n                    \"acq_software\": acq_software,\n                }\n            )\n\n            session_dir_list.append(\n                {\n                    **session_key,\n                    \"session_dir\": sess_dir.relative_to(root_data_dir).as_posix(),\n                }\n            )\n    new_equipment = set(val for dic in scanner_list for val in dic.values())\n    if verbose:\n        print(\n            f\"\\n---- Insert {len(new_equipment)} entry(s) into \"\n            + \"experiment.Equipment ----\"\n        )\n    Equipment.insert(scanner_list, skip_duplicates=skip_duplicates)\n\n    if verbose:\n        print(f\"\\n---- Insert {len(session_list)} entry(s) into session.Session ----\")\n    session.Session.insert(session_list, skip_duplicates=skip_duplicates)\n    session.SessionDirectory.insert(session_dir_list, skip_duplicates=skip_duplicates)\n\n    if verbose:\n        print(f\"\\n---- Insert {len(scan_list)} entry(s) into scan.Scan ----\")\n    scan.Scan.insert(scan_list, skip_duplicates=skip_duplicates)\n\n    if verbose:\n        print(\"\\n---- Successfully completed ingest_sessions ----\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_subjects", "title": "<code>ingest_subjects(subject_csv_path='./user_data/subjects.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Inserts ./user_data/subject.csv data into corresponding subject schema tables.</p> <p>Parameters:</p> Name Type Description Default <code>subject_csv_path</code> <code>str</code> <p>relative path of subject csv.</p> <code>'./user_data/subjects.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Display number of entries inserted when ingesting.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/ingest.py</code> <pre><code>def ingest_subjects(\n    subject_csv_path:str=\"./user_data/subjects.csv\", skip_duplicates:bool=True, verbose:bool=True\n):\n\"\"\"Inserts ./user_data/subject.csv data into corresponding subject schema tables.\n\n    Args:\n        subject_csv_path (str): relative path of subject csv.\n        skip_duplicates (bool): Default True. Passed to DataJoint insert.\n        verbose (bool): Display number of entries inserted when ingesting.\n    \"\"\"\n    csvs = [subject_csv_path]\n    tables = [subject.Subject()]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/", "title": "paths.py", "text": ""}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Retrieve imaging root data directory.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_imaging_root_data_dir() -&gt; pathlib.Path:\n\"\"\"Retrieve imaging root data directory.\"\"\"\n\n    data_dir = dj.config.get(\"custom\", {}).get(\"imaging_root_data_dir\", None)\n    return pathlib.Path(data_dir) if data_dir else None\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_nd2_files", "title": "<code>get_nd2_files(scan_key)</code>", "text": "<p>Retrieve the list of Nikon files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or nd2 files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_nd2_files(scan_key):\n\"\"\"Retrieve the list of Nikon files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or nd2 files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .nd2\n    data_dir = get_imaging_root_data_dir()\n\n    from .pipeline import session\n\n    sess_dir = data_dir / (session.SessionDirectory &amp; scan_key).fetch1(\"session_dir\")\n\n    if not sess_dir.exists():\n        raise FileNotFoundError(f\"Session directory not found ({sess_dir})\")\n\n    nd2_filepaths = [fp.as_posix() for fp in sess_dir.glob(\"*.nd2\")]\n    if nd2_filepaths:\n        return nd2_filepaths\n    else:\n        raise FileNotFoundError(f\"No .nd2 file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_prairieview_files", "title": "<code>get_prairieview_files(scan_key)</code>", "text": "<p>Retrieve the list of PrairieView files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or tiff files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_prairieview_files(scan_key):\n\"\"\"Retrieve the list of PrairieView files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or tiff files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .tif\n    data_dir = get_imaging_root_data_dir()\n\n    from .pipeline import session\n\n    sess_dir = data_dir / (session.SessionDirectory &amp; scan_key).fetch1(\"session_dir\")\n\n    if not sess_dir.exists():\n        raise FileNotFoundError(f\"Session directory not found ({sess_dir})\")\n\n    pv_filepaths = [fp.as_posix() for fp in sess_dir.glob(\"*.tif\")]\n    if pv_filepaths:\n        return pv_filepaths\n    else:\n        raise FileNotFoundError(f\"No .tif file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_scan_box_files", "title": "<code>get_scan_box_files(scan_key)</code>", "text": "<p>Retrieve the list of Scanbox files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or scanbox files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_scan_box_files(scan_key):\n\"\"\"Retrieve the list of Scanbox files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or scanbox files are not found.\n    \"\"\"\n\n    # Folder structure: root / subject / session / .sbx\n    data_dir = get_imaging_root_data_dir()\n\n    from .pipeline import session\n\n    sess_dir = data_dir / (session.SessionDirectory &amp; scan_key).fetch1(\"session_dir\")\n\n    if not sess_dir.exists():\n        raise FileNotFoundError(f\"Session directory not found ({sess_dir})\")\n\n    sbx_filepaths = [fp.as_posix() for fp in sess_dir.glob(\"*.sbx\")]\n    if sbx_filepaths:\n        return sbx_filepaths\n    else:\n        raise FileNotFoundError(f\"No .sbx file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_scan_image_files", "title": "<code>get_scan_image_files(scan_key)</code>", "text": "<p>Retrieve the list of ScanImage files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or tiff files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_scan_image_files(scan_key):\n\"\"\"Retrieve the list of ScanImage files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or tiff files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .tif (raw)\n    data_dir = get_imaging_root_data_dir()\n\n    from .pipeline import session\n\n    sess_dir = data_dir / (session.SessionDirectory &amp; scan_key).fetch1(\"session_dir\")\n\n    if not sess_dir.exists():\n        raise FileNotFoundError(f\"Session directory not found ({sess_dir})\")\n\n    tiff_filepaths = [fp.as_posix() for fp in sess_dir.glob(\"*.tif\")]\n    if tiff_filepaths:\n        return tiff_filepaths\n    else:\n        raise FileNotFoundError(f\"No tiff file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/pipeline/", "title": "pipeline.py", "text": ""}, {"location": "api/workflow_calcium_imaging/process/", "title": "process.py", "text": ""}, {"location": "api/workflow_calcium_imaging/process/#workflow_calcium_imaging.process.run", "title": "<code>run(display_progress=True)</code>", "text": "<p>Run all <code>make</code> methods from element-calcium imaging</p> <p>Parameters:</p> Name Type Description Default <code>display_progress</code> <code>bool</code> <p>Whether to display the progress. Default is True.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/process.py</code> <pre><code>def run(display_progress=True):\n\"\"\"Run all `make` methods from element-calcium imaging\n\n    Args:\n        display_progress (bool, optional): Whether to display the progress. Default is\n            True.\n    \"\"\"\n\n    populate_settings = {\n        \"display_progress\": display_progress,\n        \"reserve_jobs\": False,\n        \"suppress_errors\": False,\n    }\n\n    print(\"\\n---- Populate imported and computed tables ----\")\n\n    scan.ScanInfo.populate(**populate_settings)\n\n    imaging.Processing.populate(**populate_settings)\n\n    imaging.MotionCorrection.populate(**populate_settings)\n\n    imaging.Segmentation.populate(**populate_settings)\n\n    imaging.MaskClassification.populate(**populate_settings)\n\n    imaging.Fluorescence.populate(**populate_settings)\n\n    imaging.Activity.populate(**populate_settings)\n\n    print(\"\\n---- Successfully completed workflow_calcium_imaging/process.py ----\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/reference/", "title": "reference.py", "text": ""}, {"location": "api/workflow_calcium_imaging/version/", "title": "version.py", "text": "<p>Package metadata Update the Docker image tag in <code>docker-compose-test.yaml</code> and  <code>docker-compose-dev.yaml</code> to match</p>"}]}