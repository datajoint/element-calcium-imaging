{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Element Calcium Imaging", "text": "<p>DataJoint Element for functional calcium imaging with  ScanImage,  Scanbox, Nikon NIS-Elements,  and <code>Bruker Prairie View</code> acquisition software; and  Suite2p,  CaImAn, and EXTRACT analysis  software. DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline.</p>"}, {"location": "#experiment-flowchart", "title": "Experiment Flowchart", "text": ""}, {"location": "#data-pipeline-diagram", "title": "Data Pipeline Diagram", "text": "<ul> <li>We have designed three variations of the pipeline to handle different use cases. Displayed above is the default <code>imaging</code> schema.  Details on all of the <code>imaging</code> schemas can be found in the Data Pipeline documentation page.</li> </ul>"}, {"location": "#getting-started", "title": "Getting Started", "text": "<ul> <li>Install from PyPI<pre><code>pip install element-calcium-imaging\n</code></pre> </li> </ul> <ul> <li>Data Pipeline - Pipeline and table descriptions</li> </ul> <ul> <li>Tutorials - Start building your data pipeline</li> </ul> <ul> <li>Code Repository</li> </ul>"}, {"location": "#support", "title": "Support", "text": "<ul> <li>If you need help getting started or run into any errors, please contact our team by email at support@datajoint.com.</li> </ul>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>Observes Semantic Versioning standard and Keep a Changelog convention.</p>"}, {"location": "changelog/#062-2023-05-22", "title": "0.6.2 - 2023-05-22", "text": "<ul> <li>Add - CaImAn, Suite2p, and EXTRACT citations</li> </ul>"}, {"location": "changelog/#061-2023-05-15", "title": "0.6.1 - 2023-05-15", "text": "<ul> <li>Update - Docs</li> </ul>"}, {"location": "changelog/#060-2023-05-15", "title": "0.6.0 - 2023-05-15", "text": "<ul> <li>Add - Quality metrics</li> <li>Update - Docs and readme</li> </ul>"}, {"location": "changelog/#057-2023-05-11", "title": "0.5.7 - 2023-05-11", "text": "<ul> <li>Fix - <code>.ipynb</code> dark mode output for all notebooks.</li> <li>Fix - Remove <code>GOOGLE_ANALYTICS_KEY</code> from <code>u24_element_release_call.yml</code>.</li> </ul>"}, {"location": "changelog/#056-2023-04-28", "title": "0.5.6 - 2023-04-28", "text": "<ul> <li>Fix - <code>.ipynb</code> output in tutorials is not visible in dark mode.</li> <li>Fix - typos in docstrings.</li> </ul>"}, {"location": "changelog/#055-2023-04-06", "title": "0.5.5 - 2023-04-06", "text": "<ul> <li>Update - Bump <code>element-interface</code> requirement to <code>0.5.1</code>.</li> </ul>"}, {"location": "changelog/#054-2023-03-08", "title": "0.5.4 - 2023-03-08", "text": "<ul> <li>Add - Requirement for <code>ipywidgets</code></li> <li>Update - Docker Compose file for docs release</li> </ul>"}, {"location": "changelog/#053-2023-02-23", "title": "0.5.3 - 2023-02-23", "text": "<ul> <li>Add - spelling, markdown, and pre-commit config files</li> <li>Add - Notebook rendering to docs</li> </ul>"}, {"location": "changelog/#052-2023-01-11", "title": "0.5.2 - 2023-01-11", "text": "<ul> <li>Bugfix - fix errors in ingesting single-plane PrairieView scans into <code>ScanInfo</code></li> <li>Add - Optional installation of caiman and suite2p through pip</li> </ul>"}, {"location": "changelog/#051-2022-12-15", "title": "0.5.1 - 2022-12-15", "text": "<ul> <li>Add - Imports for prairieview loader</li> </ul>"}, {"location": "changelog/#050-2022-12-14", "title": "0.5.0 - 2022-12-14", "text": "<ul> <li>Add - Cell extraction with EXTRACT package</li> </ul>"}, {"location": "changelog/#042-2022-11-02", "title": "0.4.2 - 2022-11-02", "text": "<ul> <li>Bugfix - Add plotting package to the requirements to generate the figures</li> <li>Add - Scan date parser from nd2 files</li> </ul>"}, {"location": "changelog/#041-2022-10-28", "title": "0.4.1 - 2022-10-28", "text": "<ul> <li>Update - Bump version to trigger PyPI release to revert updates from incorrect tag</li> </ul>"}, {"location": "changelog/#040-2022-10-28", "title": "0.4.0 - 2022-10-28", "text": "<ul> <li>Add - New schema <code>imaging_report</code> to compute and store figures from results</li> <li>Add - Widget to display figures</li> </ul>"}, {"location": "changelog/#030-2022-10-07", "title": "0.3.0 - 2022-10-07", "text": "<ul> <li>Add - Reader for <code>Bruker PrairieView</code> acquisition system</li> </ul>"}, {"location": "changelog/#022-2022-09-28", "title": "0.2.2 - 2022-09-28", "text": "<ul> <li>Update - Minor table explanation edits</li> <li>Update - Query simplifications</li> <li>Update - Minor code refactoring</li> </ul>"}, {"location": "changelog/#021-2022-09-12", "title": "0.2.1 - 2022-09-12", "text": "<ul> <li>Bugfix - fix errors in auto generating new ProcessingTask</li> </ul>"}, {"location": "changelog/#020-2022-07-01", "title": "0.2.0 - 2022-07-01", "text": "<ul> <li>Add - Imaging module (imaging_preprocess.py) for pre-processing steps</li> </ul>"}, {"location": "changelog/#010-2022-06-29", "title": "0.1.0 - 2022-06-29", "text": "<ul> <li>Add - Support for element-interface</li> <li>Add - Trigger Suite2p and CaImAn</li> <li>Add - Imaging module for no curation</li> <li>Add - Support for Nikon acquisition system</li> <li>Add - <code>scan_datetime</code> and <code>scan_duration</code> attributes</li> <li>Add - Estimate for scan duration</li> <li>Add - Citation section to README</li> <li>Update - Move background file to elements.datajoint.org</li> <li>Add - Adopt black formatting into code base</li> </ul>"}, {"location": "changelog/#010b0-2021-05-07", "title": "0.1.0b0 - 2021-05-07", "text": "<ul> <li>Update - First beta release</li> </ul>"}, {"location": "changelog/#010a4-2021-05-07", "title": "0.1.0a4 - 2021-05-07", "text": "<ul> <li>Update - Add workaround to handle DataJoint 0.13.* issue #914</li> </ul>"}, {"location": "changelog/#010a3-2021-05-03", "title": "0.1.0a3 - 2021-05-03", "text": "<ul> <li>Add - GitHub Action release process</li> <li>Add - <code>scan</code> and <code>imaging</code> modules</li> <li>Add - Readers for <code>ScanImage</code>, <code>ScanBox</code>, <code>Suite2p</code>, <code>CaImAn</code></li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses the following resources, please cite the respective manuscript and/or Research Resource Identifier (RRID):</p> <ul> <li>DataJoint Element Calcium Imaging - Version 0.6.2      + Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D,      Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for      Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358<p>+ RRID:SCR_021894</p> </li> </ul> <ul> <li>CaImAn      + Manuscripts</li> </ul> <ul> <li>Suite2p      + Manuscripts</li> </ul> <ul> <li>EXTRACT      + Manuscripts</li> </ul>"}, {"location": "concepts/", "title": "Concepts", "text": ""}, {"location": "concepts/#multiphoton-calcium-imaging", "title": "Multiphoton Calcium Imaging", "text": "<p>Over the past two decades, in vivo two-photon laser-scanning imaging of calcium signals has evolved into a mainstream modality for neurophysiology experiments to record population activity in intact neural circuits. The tools for signal acquisition and analysis continue to evolve but common patterns and elements of standardization have emerged.</p> <p>The preprocessing workflow for two-photon laser-scanning microscopy includes motion correction (rigid or non-rigid), cell segmentation, and calcium event extraction (sometimes described as \"deconvolution\" or \"spike inference\"). Some include raster artifact correction, cropping and stitching operations.</p> <p> </p>      Left to right: Raw scans, Motion corrected scans, Cell segmentation, Calcium events    <p>For a long time, most labs developed custom processing pipelines, sharing them with others as academic open-source projects. This has changed recently with the emerging of a few leaders as the standardization candidates for the initial preprocessing.</p> <ul> <li>CaImAn (Originally developed by Andrea   Giovannucci, current support by FlatIron Institute: Eftychios A. Pnevmatikakis,   Johannes Friedrich)</li> <li>Suite2p (Carsen Stringer and Marius Pachitariu   at Janelia), 200+ users, active support</li> <li>EXTRACT (Hakan Inan et al. 2017,   2021).</li> </ul> <p>Element Calcium Imaging encapsulates these packages to ease the management of data and its analysis.</p>"}, {"location": "concepts/#acquisition-tools", "title": "Acquisition tools", "text": ""}, {"location": "concepts/#hardware", "title": "Hardware", "text": "<p>The primary acquisition systems are:</p> <ul> <li>Sutter</li> <li>Thorlabs</li> <li>Bruker</li> <li>Neurolabware</li> </ul> <p>We do not include Miniscopes in these estimates. In, all there are perhaps on the order of 3000 two-photon setups globally but their processing needs may need to be further segmented.</p>"}, {"location": "concepts/#software", "title": "Software", "text": "<ul> <li>Vidrio ScanImage</li> <li>Thorlabs ThorImageLS</li> <li>Scanbox</li> <li>Nikon NIS-Elements</li> <li>Bruker Prairie View</li> </ul>"}, {"location": "partnerships/", "title": "Key partnerships", "text": "<p>Several labs have developed DataJoint-based data management and processing pipelines for two-photon calcium imaging. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experiment workflow, pipeline design, associated tools, and interfaces.</p> <p>These teams include:</p> <ul> <li>MICrONS (Andreas Tolias Lab, BCM) - https://github.com/cajal</li> <li>BrainCoGS (Princeton) - https://github.com/BrainCOGS</li> <li>Moser Group (Kavli Institute/NTNU) - private repository</li> <li>Anne Churchland Lab (UCLA)</li> </ul>"}, {"location": "pipeline/", "title": "Data Pipeline", "text": "<p>Each node in the following diagram represents the analysis code in the workflow and the corresponding table in the database.  Within the workflow, Element Calcium Imaging connects to upstream Elements including Lab, Animal, Session, and Event. For more  detailed documentation on each table, see the API docs for the respective schemas.</p> <p>The Element is composed of two main schemas, <code>scan</code> and <code>imaging</code>. To handle several use cases of this pipeline, we have designed two alternatives to the <code>imaging</code>  schema, including <code>imaging_no_curation</code> and <code>imaging_preprocess</code>.</p>"}, {"location": "pipeline/#diagrams", "title": "Diagrams", "text": ""}, {"location": "pipeline/#imaging-module", "title": "<code>imaging</code> module", "text": "<ul> <li>Multiple scans are acquired during each session and each scan is processed independently. </li> </ul>"}, {"location": "pipeline/#imaging_no_curation-module", "title": "<code>imaging_no_curation</code> module", "text": "<ul> <li>Same as the <code>imaging</code> module, but without the <code>Curation</code> table. </li> </ul>"}, {"location": "pipeline/#imaging_preprocess-module", "title": "<code>imaging_preprocess</code> module", "text": "<ul> <li>Same as the <code>imaging</code> module, and additional pre-processing steps can be performed on each scan prior to processing with Suite2p or CaImAn. </li> </ul>"}, {"location": "pipeline/#multi-scan-processing-branch", "title": "<code>multi-scan-processing</code> branch", "text": "<ul> <li>The processing workflow is typically performed on a per-scan basis, however, depending on the nature of the research questions, different labs may opt to perform processing/segmentation on a concatenated set of data from multiple scans. To this end, we have extended the Calcium Imaging Element and provided a design version capable of supporting a multi-scan processing scheme.</li> </ul>"}, {"location": "pipeline/#table-descriptions", "title": "Table descriptions", "text": ""}, {"location": "pipeline/#reference-schema", "title": "<code>reference</code> schema", "text": "<ul> <li>For further details see the reference schema API docs</li> </ul> Table Description Equipment Scanner metadata"}, {"location": "pipeline/#subject-schema", "title": "<code>subject</code> schema", "text": "<ul> <li>Although not required, most choose to connect the <code>Session</code> table to a <code>Subject</code> table.</li> </ul> <ul> <li>For further details see the subject schema API docs</li> </ul> Table Description Subject Basic information of the research subject"}, {"location": "pipeline/#session-schema", "title": "<code>session</code> schema", "text": "<ul> <li>For further details see the session schema API docs</li> </ul> Table Description Session Unique experimental session identifier"}, {"location": "pipeline/#scan-schema", "title": "<code>scan</code> schema", "text": "<ul> <li>For further details see the scan schema API docs</li> </ul> Table Description AcquisitionSoftware Software used in the acquisition of the imaging scans Channel Recording Channel Scan A set of imaging scans performed in a single session ScanLocation Anatomical location of the region scanned ScanInfo Metadata of the imaging scan ScanInfo.Field Metadata of the fields imaged ScanInfo.ScanFile Path of the scan file ScanQualityMetrics Metrics to assess the quality of the scan ScanQualityMetrics.Frames Metrics used to evaluate each frame"}, {"location": "pipeline/#imaging-schema", "title": "<code>imaging</code> schema", "text": "<ul> <li>For further details see the imaging schema API docs</li> </ul> Table Description ProcessingMethod Available analysis suites that can be used in processing of the imaging scans ProcessingParamSet All parameters required to process a calcium imaging scan CellCompartment Cell compartments that can be imaged MaskType Available labels for segmented masks ProcessingTask Task defined by a combination of Scan and ProcessingParamSet Processing The core table that executes a ProcessingTask Curation Curated results MotionCorrection Results of the motion correction procedure MotionCorrection.RigidMotionCorrection Details of the rigid motion correction performed on the imaging data MotionCorrection.NonRigidMotionCorrection Details of nonrigid motion correction performed on the imaging data MotionCorrection.NonRigidMotionCorrection.Block Results of non-rigid motion correction for each block MotionCorrection.Summary Summary images for each field and channel after motion corrections Segmentation Results of the segmentation Segmentation.Mask Masks identified in the segmentation procedure MaskClassificationMethod Method used in the mask classification procedure MaskClassification Result of the mask classification procedure MaskClassification.MaskType Type assigned to each mask Fluorescence Fluorescence measurements Fluorescence.Trace Fluorescence traces for each region of interest ActivityExtractionMethod Method used in activity extraction Activity Inferred neural activity Activity.Trace Inferred neural activity from fluorescence traces ProcessingQualityMetrics Quality metrics used to evaluate the results of the calcium imaging analysis pipeline ProcessingQualityMetrics.Mask Quality metrics used to evaluate the masks ProcessingQualityMetrics.Trace Quality metrics used to evaluate the fluorescence traces"}, {"location": "roadmap/", "title": "Roadmap", "text": "<p>Through our interviews and direct collaboration on key projects, we identified the common motifs to create Element Calcium Imaging. Major features include:</p> <ul> <li> Ingestion of scan metadata, also compatible with mesoscale imaging and   multi-ROI scanning mode</li> <li> Tables for all processing steps: motion correction, cell segmentation,    fluorescence trace extraction, spike inference, and cell classification</li> <li> Store different curations of the segmentation results</li> <li> Ingestion of data acquired with ScanImage, Scanbox, Nikon NIS-Elements, and   Bruker Prairie View acquisition systems</li> <li> Ingestion of processing outputs from both Suite2p and CaImAn analysis suites</li> <li> Sample data and complete test suite for quality assurance</li> <li> Cell extraction with the EXTRACT analysis package</li> <li> Quality metrics</li> <li> Data compression</li> <li> Deepinterpolation</li> <li> Data export to NWB</li> <li> Data publishing to DANDI</li> </ul> <p>Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will continue adding features to this  Element.</p>"}, {"location": "api/element_calcium_imaging/imaging/", "title": "imaging.py", "text": ""}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.activate", "title": "<code>activate(imaging_schema_name, scan_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>imaging_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>imaging</code> module.</p> required <code>scan_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>scan</code> module. Omitted, if the <code>scan</code> module is already activated.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>imaging</code> module: + all that are required by the <code>scan</code> module.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <ul> <li>Session: A parent table to Scan, identifying a scanning session.</li> <li>Equipment: A parent table to Scan, identifying a scanning device.</li> </ul> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def activate(\n    imaging_schema_name: str,\n    scan_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        imaging_schema_name (str): Schema name on the database server to activate the\n            `imaging` module.\n        scan_schema_name (str): Schema name on the database server to activate the\n            `scan` module. Omitted, if the `scan` module is already activated.\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `imaging` module: + all that are required by\n            the `scan` module.\n\n    Dependencies:\n    Upstream tables:\n        + Session: A parent table to Scan, identifying a scanning session.\n        + Equipment: A parent table to Scan, identifying a scanning device.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    scan.activate(\n        scan_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        linking_module=linking_module,\n    )\n    schema.activate(\n        imaging_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    imaging_report.activate(f\"{imaging_schema_name}_report\", imaging_schema_name)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Return imaging root data director(y/ies)</p> <p>Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dirs</code> <code>list</code> <p>A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies).</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_imaging_root_data_dir() -&gt; list:\n\"\"\"Return imaging root data director(y/ies)\n\n    Retrieve the root data director(y/ies) containing the imaging data\n    for all subjects/sessions (e.g. acquired ScanImage raw files, output files from\n    processing routines, etc.). All data paths and directories in DataJoint Elements are\n    recommended to be stored as relative paths (posix format), with respect to some\n    user-configured \"root\" directory, which varies from machine to machine\n    (e.g. different mounted drive locations).\n\n    Returns:\n        dirs (list): A list of string(s) or Path(s) for the absolute paths of the imaging root data\n            director(y/ies).\n    \"\"\"\n\n    root_directories = _linking_module.get_imaging_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingMethod", "title": "<code>ProcessingMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Method, package, or analysis suite used for processing of calcium imaging data     (e.g. Suite2p, CaImAn, etc.).</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method.</p> <code>processing_method_desc</code> <code>str</code> <p>Processing method description.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ProcessingMethod(dj.Lookup):\n\"\"\"Method, package, or analysis suite used for processing of calcium imaging data\n        (e.g. Suite2p, CaImAn, etc.).\n\n    Attributes:\n        processing_method (str): Processing method.\n        processing_method_desc (str): Processing method description.\n    \"\"\"\n\n    definition = \"\"\"# Method for calcium imaging processing\n    processing_method: char(8)\n    ---\n    processing_method_desc: varchar(1000)  # Processing method description\n    \"\"\"\n\n    contents = [\n        (\"suite2p\", \"suite2p analysis suite\"),\n        (\"caiman\", \"caiman analysis suite\"),\n        (\"extract\", \"extract analysis suite\"),\n    ]\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Retrieve the root directory for all processed data.</p> <p>All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dir</code> <code>str | pathlib.Path</code> <p>Absolute path of the processed imaging root data directory.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_processed_root_data_dir() -&gt; Union[str, pathlib.Path]:\n\"\"\"Retrieve the root directory for all processed data.\n\n    All data paths and directories in DataJoint Elements are recommended to be stored as\n    relative paths (posix format), with respect to some user-configured \"root\"\n    directory, which varies from machine to machine (e.g. different mounted drive\n    locations).\n\n    Returns:\n        dir (str| pathlib.Path): Absolute path of the processed imaging root data\n            directory.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_imaging_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingParamSet", "title": "<code>ProcessingParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters.</p> <p>A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>ProcessingMethod</code> <code>foreign key</code> <p>A primary key from ProcessingMethod.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>param_set_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameter Set, a dictionary of all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ProcessingParamSet(dj.Lookup):\n\"\"\"Parameter set used for the processing of the calcium imaging scans,\n    including both the analysis suite and its respective input parameters.\n\n    A hash of the parameters of the analysis suite is also stored in order\n    to avoid duplicated entries.\n\n    Attributes:\n        paramset_idx (int): Unique parameter set ID.\n        ProcessingMethod (foreign key): A primary key from ProcessingMethod.\n        paramset_desc (str): Parameter set description.\n        param_set_hash (uuid): A universally unique identifier for the parameter set.\n        params (longblob): Parameter Set, a dictionary of all applicable parameters to\n            the analysis suite.\n    \"\"\"\n\n    definition = \"\"\"# Processing Parameter Set\n    paramset_idx: smallint  # Unique parameter set ID.\n    ---\n    -&gt; ProcessingMethod\n    paramset_desc: varchar(1280)  # Parameter-set description\n    param_set_hash: uuid  # A universally unique identifier for the parameter set\n    unique index (param_set_hash)\n    params: longblob  # Parameter Set, a dictionary of all applicable parameters to the analysis suite.\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        processing_method: str,\n        paramset_idx: int,\n        paramset_desc: str,\n        params: dict,\n    ):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n        This function automates the parameter set hashing and avoids insertion of an\n            existing parameter set.\n\n        Attributes:\n            processing_method (str): Processing method/package used for processing of\n                calcium imaging.\n            paramset_idx (int): Unique parameter set ID.\n            paramset_desc (str): Parameter set description.\n            params (dict): Parameter Set, all applicable parameters to the analysis\n                suite.\n        \"\"\"\n        if processing_method == \"extract\":\n            assert (\n                params.get(\"extract\") is not None and params.get(\"suite2p\") is not None\n            ), ValueError(\n                \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n            )\n\n            # Force Suite2p to only run motion correction.\n            params[\"suite2p\"][\"do_registration\"] = True\n            params[\"suite2p\"][\"roidetect\"] = False\n\n        param_dict = {\n            \"processing_method\": processing_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if q_param:  # If the specified param-set already exists\n            p_name = q_param.fetch1(\"paramset_idx\")\n            if p_name == paramset_idx:  # If the existed set has the same name: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set already exists - name: {}\".format(p_name)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingParamSet.insert_new_params", "title": "<code>insert_new_params(processing_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Insert a parameter set into ProcessingParamSet table.</p> <p>This function automates the parameter set hashing and avoids insertion of an     existing parameter set.</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method/package used for processing of calcium imaging.</p> <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>params</code> <code>dict</code> <p>Parameter Set, all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    processing_method: str,\n    paramset_idx: int,\n    paramset_desc: str,\n    params: dict,\n):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n    This function automates the parameter set hashing and avoids insertion of an\n        existing parameter set.\n\n    Attributes:\n        processing_method (str): Processing method/package used for processing of\n            calcium imaging.\n        paramset_idx (int): Unique parameter set ID.\n        paramset_desc (str): Parameter set description.\n        params (dict): Parameter Set, all applicable parameters to the analysis\n            suite.\n    \"\"\"\n    if processing_method == \"extract\":\n        assert (\n            params.get(\"extract\") is not None and params.get(\"suite2p\") is not None\n        ), ValueError(\n            \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n        )\n\n        # Force Suite2p to only run motion correction.\n        params[\"suite2p\"][\"do_registration\"] = True\n        params[\"suite2p\"][\"roidetect\"] = False\n\n    param_dict = {\n        \"processing_method\": processing_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if q_param:  # If the specified param-set already exists\n        p_name = q_param.fetch1(\"paramset_idx\")\n        if p_name == paramset_idx:  # If the existed set has the same name: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set already exists - name: {}\".format(p_name)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_scan_image_files", "title": "<code>get_scan_image_files(scan_key)</code>", "text": "<p>Retrieve the list of ScanImage files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of ScanImage files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_image_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of ScanImage files associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of ScanImage files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_image_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_scan_box_files", "title": "<code>get_scan_box_files(scan_key)</code>", "text": "<p>Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Scanbox files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_box_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Scanbox files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_box_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_nd2_files", "title": "<code>get_nd2_files(scan_key)</code>", "text": "<p>Retrieve the list of Nikon files (*.nd2) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Nikon files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_nd2_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Nikon files (*.nd2) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Nikon files' full file-paths.\n    \"\"\"\n    return _linking_module.get_nd2_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_prairieview_files", "title": "<code>get_prairieview_files(scan_key)</code>", "text": "<p>Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Bruker PrairieView files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_prairieview_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Bruker PrairieView files' full file-paths.\n    \"\"\"\n    return _linking_module.get_prairieview_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.CellCompartment", "title": "<code>CellCompartment</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)</p> <p>Attributes:</p> Name Type Description <code>cell_compartment</code> <code>str</code> <p>Cell compartment.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass CellCompartment(dj.Lookup):\n\"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)\n\n    Attributes:\n        cell_compartment (str): Cell compartment.\n    \"\"\"\n\n    definition = \"\"\"# Cell compartments\n    cell_compartment: char(16)\n    \"\"\"\n\n    contents = zip([\"axon\", \"soma\", \"bouton\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').</p> <p>Attributes:</p> Name Type Description <code>mask_type</code> <code>str</code> <p>Mask type.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass MaskType(dj.Lookup):\n\"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').\n\n    Attributes:\n        mask_type (str): Mask type.\n    \"\"\"\n\n    definition = \"\"\"# Possible types of a segmented mask\n    mask_type: varchar(16)\n    \"\"\"\n\n    contents = zip([\"soma\", \"axon\", \"dendrite\", \"neuropil\", \"artefact\", \"unknown\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask", "title": "<code>ProcessingTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A pairing of processing params and scans to be loaded or triggered</p> <p>This table defines a calcium imaging processing task for a combination of a <code>Scan</code> and a <code>ProcessingParamSet</code> entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods</p> <p>Attributes:</p> Name Type Description <code>scan.Scan</code> <code>foreign key</code> <p>Primary key from scan.Scan.</p> <code>ProcessingParamSet</code> <code>foreign key</code> <p>Primary key from ProcessingParamSet.</p> <code>processing_output_dir</code> <code>str</code> <p>Output directory of the processed scan relative to the root data directory.</p> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results) or 'trigger' (trigger computation).</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ProcessingTask(dj.Manual):\n\"\"\"A pairing of processing params and scans to be loaded or triggered\n\n    This table defines a calcium imaging processing task for a combination of a\n    `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method,\n    method's parameters). The task defined here is then run in the downstream table\n    Processing. This table supports definitions of both loading of pre-generated results\n    and the triggering of new analysis for all supported analysis methods\n\n    Attributes:\n        scan.Scan (foreign key): Primary key from scan.Scan.\n        ProcessingParamSet (foreign key): Primary key from ProcessingParamSet.\n        processing_output_dir (str): Output directory of the processed scan relative to the root data directory.\n        task_mode (str): One of 'load' (load computed analysis results) or 'trigger'\n            (trigger computation).\n    \"\"\"\n\n    definition = \"\"\"# Manual table for defining a processing task ready to be run\n    -&gt; scan.Scan\n    -&gt; ProcessingParamSet\n    ---\n    processing_output_dir: varchar(255)  #  Output directory of the processed scan relative to root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n        Args:\n            key (dict): Primary key from the ProcessingTask table.\n            relative (bool): If True, processing_output_dir is returned relative to\n                imaging_root_dir. Default False.\n            mkdir (bool): If True, create the processing_output_dir directory.\n                Default True.\n\n        Returns:\n            dir (str): A default output directory for the processed results (processed_output_dir\n                in ProcessingTask) based on the following convention:\n                processed_dir / scan_dir / {processing_method}_{paramset_idx}\n                e.g.: sub4/sess1/scan0/suite2p_0\n        \"\"\"\n        image_locators = {\n            \"NIS\": get_nd2_files,\n            \"ScanImage\": get_scan_image_files,\n            \"Scanbox\": get_scan_box_files,\n            \"PrairieView\": get_prairieview_files,\n        }\n        image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n        scan_dir = find_full_path(\n            get_imaging_root_data_dir(), image_locator(key)[0]\n        ).parent\n        root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n        method = (\n            (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n        )\n\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        output_dir = (\n            processed_dir\n            / scan_dir.relative_to(root_dir)\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n        Generate an entry in the ProcessingTask table for a particular scan using an\n        existing parameter set from the ProcessingParamSet table.\n\n        Args:\n            scan_key (dict): Primary key from Scan table.\n            paramset_idx (int): Unique parameter set ID.\n        \"\"\"\n        key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n        method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n            \"processing_method\"\n        )\n\n        try:\n            if method == \"suite2p\":\n                from element_interface import suite2p_loader\n\n                suite2p_loader.Suite2p(output_dir)\n            elif method == \"caiman\":\n                from element_interface import caiman_loader\n\n                caiman_loader.CaImAn(output_dir)\n            elif method == \"extract\":\n                from element_interface import extract_loader\n\n                extract_loader.EXTRACT(output_dir)\n\n            else:\n                raise NotImplementedError(\n                    \"Unknown/unimplemented method: {}\".format(method)\n                )\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"processing_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n\n    auto_generate_entries = generate\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer an output directory for an entry in ProcessingTask table.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from the ProcessingTask table.</p> required <code>relative</code> <code>bool</code> <p>If True, processing_output_dir is returned relative to imaging_root_dir. Default False.</p> <code>False</code> <code>mkdir</code> <code>bool</code> <p>If True, create the processing_output_dir directory. Default True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dir</code> <code>str</code> <p>A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n    Args:\n        key (dict): Primary key from the ProcessingTask table.\n        relative (bool): If True, processing_output_dir is returned relative to\n            imaging_root_dir. Default False.\n        mkdir (bool): If True, create the processing_output_dir directory.\n            Default True.\n\n    Returns:\n        dir (str): A default output directory for the processed results (processed_output_dir\n            in ProcessingTask) based on the following convention:\n            processed_dir / scan_dir / {processing_method}_{paramset_idx}\n            e.g.: sub4/sess1/scan0/suite2p_0\n    \"\"\"\n    image_locators = {\n        \"NIS\": get_nd2_files,\n        \"ScanImage\": get_scan_image_files,\n        \"Scanbox\": get_scan_box_files,\n        \"PrairieView\": get_prairieview_files,\n    }\n    image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n    scan_dir = find_full_path(\n        get_imaging_root_data_dir(), image_locator(key)[0]\n    ).parent\n    root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n    method = (\n        (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n    )\n\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    output_dir = (\n        processed_dir\n        / scan_dir.relative_to(root_dir)\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingTask.generate", "title": "<code>generate(scan_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet</p> <p>Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan table.</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>0</code> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@classmethod\ndef generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n    Generate an entry in the ProcessingTask table for a particular scan using an\n    existing parameter set from the ProcessingParamSet table.\n\n    Args:\n        scan_key (dict): Primary key from Scan table.\n        paramset_idx (int): Unique parameter set ID.\n    \"\"\"\n    key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n    method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n        \"processing_method\"\n    )\n\n    try:\n        if method == \"suite2p\":\n            from element_interface import suite2p_loader\n\n            suite2p_loader.Suite2p(output_dir)\n        elif method == \"caiman\":\n            from element_interface import caiman_loader\n\n            caiman_loader.CaImAn(output_dir)\n        elif method == \"extract\":\n            from element_interface import extract_loader\n\n            extract_loader.EXTRACT(output_dir)\n\n        else:\n            raise NotImplementedError(\n                \"Unknown/unimplemented method: {}\".format(method)\n            )\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"processing_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing", "title": "<code>Processing</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted.</p> <p>Attributes:</p> Name Type Description <code>ProcessingTask</code> <code>foreign key</code> <p>Primary key from ProcessingTask.</p> <code>processing_time</code> <code>datetime</code> <p>Process completion datetime.</p> <code>package_version</code> <code>str</code> <p>Version of the analysis package used in processing the data.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Processing(dj.Computed):\n\"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table.\n    The computation is performed only on the scans with ScanInfo inserted.\n\n    Attributes:\n        ProcessingTask (foreign key): Primary key from ProcessingTask.\n        processing_time (datetime): Process completion datetime.\n        package_version (str, optional): Version of the analysis package used in\n            processing the data.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ProcessingTask\n    ---\n    processing_time     : datetime  # Time of generation of this set of processed, segmented results\n    package_version=''  : varchar(16)\n    \"\"\"\n\n    # Run processing only on Scan with ScanInfo inserted\n    @property\n    def key_source(self):\n\"\"\"Limit the Processing to Scans that have their metadata ingested to the\n        database.\"\"\"\n\n        return ProcessingTask &amp; scan.ScanInfo\n\n    def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n        task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n            \"task_mode\", \"processing_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update processing_output_dir\n            ProcessingTask.update1(\n                {**key, \"processing_output_dir\": output_dir.as_posix()}\n            )\n        output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n        if task_mode == \"load\":\n            method, imaging_dataset = get_loader_result(key, ProcessingTask)\n            if method == \"suite2p\":\n                if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                    raise NotImplementedError(\n                        \"Suite2p ingestion error - Unable to handle\"\n                        + \" ScanImage multi-ROI scanning mode yet\"\n                    )\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n            elif method == \"caiman\":\n                caiman_dataset = imaging_dataset\n                key = {**key, \"processing_time\": caiman_dataset.creation_time}\n            elif method == \"extract\":\n                raise NotImplementedError(\n                    \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n                )\n            else:\n                raise NotImplementedError(\"Unknown method: {}\".format(method))\n        elif task_mode == \"trigger\":\n\n            method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n                \"processing_method\"\n            )\n\n            image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n            image_files = [\n                find_full_path(get_imaging_root_data_dir(), image_file)\n                for image_file in image_files\n            ]\n\n            if method == \"suite2p\":\n                import suite2p\n\n                suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                suite2p_params[\"save_path0\"] = output_dir\n                (\n                    suite2p_params[\"fs\"],\n                    suite2p_params[\"nplanes\"],\n                    suite2p_params[\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                suite2p_params[\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n            elif method == \"caiman\":\n                from element_interface.caiman_loader import _process_scanimage_tiff\n                from element_interface.run_caiman import run_caiman\n\n                caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                    \"fps\", \"ndepths\", \"nchannels\"\n                )\n\n                is3D = bool(ndepths &gt; 1)\n                if is3D:\n                    raise NotImplementedError(\n                        \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                    )\n\n                # handle multi-channel tiff image before running CaImAn\n                if nchannels &gt; 1:\n                    channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                    tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                    tmp_dir.mkdir(exist_ok=True)\n                    _process_scanimage_tiff(\n                        [f.as_posix() for f in image_files], output_dir=tmp_dir\n                    )\n                    image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n                run_caiman(\n                    file_paths=[f.as_posix() for f in image_files],\n                    parameters=caiman_params,\n                    sampling_rate=sampling_rate,\n                    output_dir=output_dir,\n                    is3D=is3D,\n                )\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                caiman_dataset = imaging_dataset\n                key[\"processing_time\"] = caiman_dataset.creation_time\n\n            elif method == \"extract\":\n                import suite2p\n                from element_interface.extract_trigger import EXTRACT_trigger\n                from scipy.io import savemat\n\n                # Motion Correction with Suite2p\n                params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n                params[\"suite2p\"][\"save_path0\"] = output_dir\n                (\n                    params[\"suite2p\"][\"fs\"],\n                    params[\"suite2p\"][\"nplanes\"],\n                    params[\"suite2p\"][\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n                # Convert data.bin to registered_scans.mat\n                scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n                data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                    \"nframes\", \"px_height\", \"px_width\"\n                )\n                data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n                scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n                # Save the motion corrected movie (data.bin) in a .mat file\n                savemat(\n                    scan_matlab_fullpath,\n                    {\"M\": np.transpose(data, axes=[1, 2, 0])},\n                )\n\n                # Execute EXTRACT\n\n                ex = EXTRACT_trigger(\n                    scan_matlab_fullpath, params[\"extract\"], output_dir\n                )\n                ex.run()\n\n                _, extract_dataset = get_loader_result(key, ProcessingTask)\n                key[\"processing_time\"] = extract_dataset.creation_time\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the Processing to Scans that have their metadata ingested to the database.</p>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Processing.make", "title": "<code>make(key)</code>", "text": "<p>Execute the calcium imaging analysis defined by the ProcessingTask.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n    task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n        \"task_mode\", \"processing_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update processing_output_dir\n        ProcessingTask.update1(\n            {**key, \"processing_output_dir\": output_dir.as_posix()}\n        )\n    output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n    if task_mode == \"load\":\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n        if method == \"suite2p\":\n            if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                raise NotImplementedError(\n                    \"Suite2p ingestion error - Unable to handle\"\n                    + \" ScanImage multi-ROI scanning mode yet\"\n                )\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            key = {**key, \"processing_time\": caiman_dataset.creation_time}\n        elif method == \"extract\":\n            raise NotImplementedError(\n                \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n            )\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n    elif task_mode == \"trigger\":\n\n        method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n            \"processing_method\"\n        )\n\n        image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n        image_files = [\n            find_full_path(get_imaging_root_data_dir(), image_file)\n            for image_file in image_files\n        ]\n\n        if method == \"suite2p\":\n            import suite2p\n\n            suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            suite2p_params[\"save_path0\"] = output_dir\n            (\n                suite2p_params[\"fs\"],\n                suite2p_params[\"nplanes\"],\n                suite2p_params[\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            suite2p_params[\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n        elif method == \"caiman\":\n            from element_interface.caiman_loader import _process_scanimage_tiff\n            from element_interface.run_caiman import run_caiman\n\n            caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                \"fps\", \"ndepths\", \"nchannels\"\n            )\n\n            is3D = bool(ndepths &gt; 1)\n            if is3D:\n                raise NotImplementedError(\n                    \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                )\n\n            # handle multi-channel tiff image before running CaImAn\n            if nchannels &gt; 1:\n                channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                tmp_dir.mkdir(exist_ok=True)\n                _process_scanimage_tiff(\n                    [f.as_posix() for f in image_files], output_dir=tmp_dir\n                )\n                image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n            run_caiman(\n                file_paths=[f.as_posix() for f in image_files],\n                parameters=caiman_params,\n                sampling_rate=sampling_rate,\n                output_dir=output_dir,\n                is3D=is3D,\n            )\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            caiman_dataset = imaging_dataset\n            key[\"processing_time\"] = caiman_dataset.creation_time\n\n        elif method == \"extract\":\n            import suite2p\n            from element_interface.extract_trigger import EXTRACT_trigger\n            from scipy.io import savemat\n\n            # Motion Correction with Suite2p\n            params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n            params[\"suite2p\"][\"save_path0\"] = output_dir\n            (\n                params[\"suite2p\"][\"fs\"],\n                params[\"suite2p\"][\"nplanes\"],\n                params[\"suite2p\"][\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n            # Convert data.bin to registered_scans.mat\n            scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n            data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                \"nframes\", \"px_height\", \"px_width\"\n            )\n            data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n            scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n            # Save the motion corrected movie (data.bin) in a .mat file\n            savemat(\n                scan_matlab_fullpath,\n                {\"M\": np.transpose(data, axes=[1, 2, 0])},\n            )\n\n            # Execute EXTRACT\n\n            ex = EXTRACT_trigger(\n                scan_matlab_fullpath, params[\"extract\"], output_dir\n            )\n            ex.run()\n\n            _, extract_dataset = get_loader_result(key, ProcessingTask)\n            key[\"processing_time\"] = extract_dataset.creation_time\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Curated results</p> <p>If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir.</p> <p>Attributes:</p> Name Type Description <code>Processing</code> <code>foreign key</code> <p>Primary key from Processing.</p> <code>curation_id</code> <code>int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time of generation of this set of curated results.</p> <code>curation_output_dir</code> <code>str</code> <p>Output directory of the curated results, relative to root data directory.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this result.</p> <code>curation_note</code> <code>str</code> <p>Notes about the curation task.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curated results\n\n    If no curation is applied, the curation_output_dir can be set to\n    the value of processing_output_dir.\n\n    Attributes:\n        Processing (foreign key): Primary key from Processing.\n        curation_id (int): Unique curation ID.\n        curation_time (datetime): Time of generation of this set of curated results.\n        curation_output_dir (str): Output directory of the curated results, relative to\n            root data directory.\n        manual_curation (bool): If True, manual curation has been performed on this\n            result.\n        curation_note (str, optional): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"# Curation(s) results\n    -&gt; Processing\n    curation_id: int\n    ---\n    curation_time: datetime  # Time of generation of this set of curated results\n    curation_output_dir: varchar(255)  # Output directory of the curated results, relative to root data directory\n    manual_curation: bool  # Has manual curation been performed on this result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_processing_task(self, key, is_curated=False, curation_note=\"\"):\n\"\"\"Create a Curation entry for a given ProcessingTask key.\n\n        Args:\n            key (dict): Primary key set of an entry in the ProcessingTask table.\n            is_curated (bool): When True, indicates a manual curation.\n            curation_note (str): User's note on the specifics of the curation.\n        \"\"\"\n        if key not in Processing():\n            raise ValueError(\n                f\"No corresponding entry in Processing available for: {key};\"\n                f\"Please run `Processing.populate(key)`\"\n            )\n\n        output_dir = (ProcessingTask &amp; key).fetch1(\"processing_output_dir\")\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n            curation_time = suite2p_dataset.creation_time\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            curation_time = caiman_dataset.creation_time\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            curation_time = extract_dataset.creation_time\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": curation_time,\n                \"curation_output_dir\": output_dir,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Curation.create1_from_processing_task", "title": "<code>create1_from_processing_task(key, is_curated=False, curation_note='')</code>", "text": "<p>Create a Curation entry for a given ProcessingTask key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key set of an entry in the ProcessingTask table.</p> required <code>is_curated</code> <code>bool</code> <p>When True, indicates a manual curation.</p> <code>False</code> <code>curation_note</code> <code>str</code> <p>User's note on the specifics of the curation.</p> <code>''</code> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def create1_from_processing_task(self, key, is_curated=False, curation_note=\"\"):\n\"\"\"Create a Curation entry for a given ProcessingTask key.\n\n    Args:\n        key (dict): Primary key set of an entry in the ProcessingTask table.\n        is_curated (bool): When True, indicates a manual curation.\n        curation_note (str): User's note on the specifics of the curation.\n    \"\"\"\n    if key not in Processing():\n        raise ValueError(\n            f\"No corresponding entry in Processing available for: {key};\"\n            f\"Please run `Processing.populate(key)`\"\n        )\n\n    output_dir = (ProcessingTask &amp; key).fetch1(\"processing_output_dir\")\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n        curation_time = suite2p_dataset.creation_time\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n        curation_time = caiman_dataset.creation_time\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        curation_time = extract_dataset.creation_time\n    else:\n        raise NotImplementedError(\"Unknown method: {}\".format(method))\n\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": curation_time,\n            \"curation_output_dir\": output_dir,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection", "title": "<code>MotionCorrection</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Results of motion correction shifts performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Primary key from Curation.</p> <code>scan.Channel.proj(motion_correct_channel='channel')</code> <code>int</code> <p>Channel used for motion correction in this processing task.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass MotionCorrection(dj.Imported):\n\"\"\"Results of motion correction shifts performed on the imaging data.\n\n    Attributes:\n        Curation (foreign key): Primary key from Curation.\n        scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for\n            motion correction in this processing task.\n    \"\"\"\n\n    definition = \"\"\"# Results of motion correction\n    -&gt; Curation\n    ---\n    -&gt; scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task\n    \"\"\"\n\n    class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob): Mask with true for frames with outlier shifts\n                (already corrected).\n            y_shifts (longblob): y motion correction shifts (pixels).\n            x_shifts (longblob): x motion correction shifts (pixels).\n            z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n            y_std (float): standard deviation of y shifts across all frames (pixels).\n            x_std (float): standard deviation of x shifts across all frames (pixels).\n            z_std (float, optional): standard deviation of z shifts across all frames\n                (pixels).\n        \"\"\"\n\n        definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n        y_shifts            : longblob  # (pixels) y motion correction shifts\n        x_shifts            : longblob  # (pixels) x motion correction shifts\n        z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift)\n        y_std               : float     # (pixels) standard deviation of y shifts across all frames\n        x_std               : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n        blocks/patches.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob, null): Mask with true for frames with outlier\n                shifts (already corrected).\n            block_height (int): Block height in pixels.\n            block_width (int): Block width in pixels.\n            block_depth (int): Block depth in pixels.\n            block_count_y (int): Number of blocks tiled in the y direction.\n            block_count_x (int): Number of blocks tiled in the x direction.\n            block_count_z (int): Number of blocks tiled in the z direction.\n        \"\"\"\n\n        definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n        block_height        : int      # (pixels)\n        block_width         : int      # (pixels)\n        block_depth         : int      # (pixels)\n        block_count_y       : int      # number of blocks tiled in the y direction\n        block_count_x       : int      # number of blocks tiled in the x direction\n        block_count_z       : int      # number of blocks tiled in the z direction\n        \"\"\"\n\n    class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n        Attributes:\n            NonRigidMotionCorrection (foreign key): Primary key from\n                NonRigidMotionCorrection.\n            block_id (int): Unique block ID.\n            block_y (longblob): y_start and y_end in pixels for this block\n            block_x (longblob): x_start and x_end in pixels for this block\n            block_z (longblob): z_start and z_end in pixels for this block\n            y_shifts (longblob): y motion correction shifts for every frame in pixels\n            x_shifts (longblob): x motion correction shifts for every frame in pixels\n            z_shift=null (longblob, optional): x motion correction shifts for every frame\n                in pixels\n            y_std (float): standard deviation of y shifts across all frames in pixels\n            x_std (float): standard deviation of x shifts across all frames in pixels\n            z_std=null (float, optional): standard deviation of z shifts across all frames\n                in pixels\n        \"\"\"\n\n        definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n        -&gt; master.NonRigidMotionCorrection\n        block_id        : int\n        ---\n        block_y         : longblob  # (y_start, y_end) in pixel of this block\n        block_x         : longblob  # (x_start, x_end) in pixel of this block\n        block_z         : longblob  # (z_start, z_end) in pixel of this block\n        y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n        x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n        z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n        y_std           : float     # (pixels) standard deviation of y shifts across all frames\n        x_std           : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n            ref_image (longblob): Image used as alignment template.\n            average_image (longblob): Mean of registered frames.\n            correlation_image (longblob, optional): Correlation map (computed during\n                cell detection).\n            max_proj_image (longblob, optional): Max of registered frames.\n        \"\"\"\n\n        definition = \"\"\"# Summary images for each field and channel after corrections\n        -&gt; master\n        -&gt; scan.ScanInfo.Field\n        ---\n        ref_image               : longblob  # image used as alignment template\n        average_image           : longblob  # mean of registered frames\n        correlation_image=null  : longblob  # correlation map (computed during cell detection)\n        max_proj_image=null     : longblob  # max of registered frames\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n            \"KEY\", \"field_z\", order_by=\"field_z\"\n        )\n\n        if method in [\"suite2p\", \"extract\"]:\n            suite2p_dataset = imaging_dataset\n\n            motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n            # ---- iterate through all s2p plane outputs ----\n            rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n            summary_images = []\n            for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n                # -- rigid motion correction --\n                if idx == 0:\n                    rigid_correction = {\n                        **key,\n                        \"y_shifts\": s2p.ops[\"yoff\"],\n                        \"x_shifts\": s2p.ops[\"xoff\"],\n                        \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                        \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                        \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                        \"z_std\": np.nan,\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    rigid_correction[\"y_shifts\"] = np.vstack(\n                        [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                    )\n                    rigid_correction[\"y_std\"] = np.nanstd(\n                        rigid_correction[\"y_shifts\"].flatten()\n                    )\n                    rigid_correction[\"x_shifts\"] = np.vstack(\n                        [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                    )\n                    rigid_correction[\"x_std\"] = np.nanstd(\n                        rigid_correction[\"x_shifts\"].flatten()\n                    )\n                    rigid_correction[\"outlier_frames\"] = np.logical_or(\n                        rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                    )\n                # -- non-rigid motion correction --\n                if s2p.ops[\"nonrigid\"]:\n                    if idx == 0:\n                        nonrigid_correction = {\n                            **key,\n                            \"block_height\": s2p.ops[\"block_size\"][0],\n                            \"block_width\": s2p.ops[\"block_size\"][1],\n                            \"block_depth\": 1,\n                            \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                            \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                            \"block_count_z\": len(suite2p_dataset.planes),\n                            \"outlier_frames\": s2p.ops[\"badframes\"],\n                        }\n                    else:\n                        nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                            nonrigid_correction[\"outlier_frames\"],\n                            s2p.ops[\"badframes\"],\n                        )\n                    for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                        zip(\n                            s2p.ops[\"xblock\"],\n                            s2p.ops[\"yblock\"],\n                            s2p.ops[\"yoff1\"].T,\n                            s2p.ops[\"xoff1\"].T,\n                        )\n                    ):\n                        if b_id in nonrigid_blocks:\n                            nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                            )\n                            nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                            )\n                            nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                            )\n                            nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                            )\n                        else:\n                            nonrigid_blocks[b_id] = {\n                                **key,\n                                \"block_id\": b_id,\n                                \"block_y\": b_y,\n                                \"block_x\": b_x,\n                                \"block_z\": np.full_like(b_x, plane),\n                                \"y_shifts\": bshift_y,\n                                \"x_shifts\": bshift_x,\n                                \"z_shifts\": np.full(\n                                    (\n                                        len(suite2p_dataset.planes),\n                                        len(bshift_x),\n                                    ),\n                                    0,\n                                ),\n                                \"y_std\": np.nanstd(bshift_y),\n                                \"x_std\": np.nanstd(bshift_x),\n                                \"z_std\": np.nan,\n                            }\n\n                # -- summary images --\n                motion_correction_key = (\n                    scan.ScanInfo.Field * Curation &amp; key &amp; field_keys[plane]\n                ).fetch1(\"KEY\")\n                summary_images.append(\n                    {\n                        **motion_correction_key,\n                        \"ref_image\": s2p.ref_image,\n                        \"average_image\": s2p.mean_image,\n                        \"correlation_image\": s2p.correlation_map,\n                        \"max_proj_image\": s2p.max_proj_image,\n                    }\n                )\n\n            self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n            if rigid_correction:\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            if nonrigid_correction:\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks.values())\n            self.Summary.insert(summary_images)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            self.insert1(\n                {\n                    **key,\n                    \"motion_correct_channel\": caiman_dataset.alignment_channel,\n                }\n            )\n\n            is3D = caiman_dataset.params.motion[\"is3D\"]\n            if not caiman_dataset.params.motion[\"pw_rigid\"]:\n                # -- rigid motion correction --\n                rigid_correction = {\n                    **key,\n                    \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                    \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                    \"z_shifts\": (\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                        if is3D\n                        else np.full_like(\n                            caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                            0,\n                        )\n                    ),\n                    \"x_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                    ),\n                    \"y_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                    ),\n                    \"z_std\": (\n                        np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                        if is3D\n                        else np.nan\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            else:\n                # -- non-rigid motion correction --\n                nonrigid_correction = {\n                    **key,\n                    \"block_height\": (\n                        caiman_dataset.params.motion[\"strides\"][0]\n                        + caiman_dataset.params.motion[\"overlaps\"][0]\n                    ),\n                    \"block_width\": (\n                        caiman_dataset.params.motion[\"strides\"][1]\n                        + caiman_dataset.params.motion[\"overlaps\"][1]\n                    ),\n                    \"block_depth\": (\n                        caiman_dataset.params.motion[\"strides\"][2]\n                        + caiman_dataset.params.motion[\"overlaps\"][2]\n                        if is3D\n                        else 1\n                    ),\n                    \"block_count_x\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                    ),\n                    \"block_count_y\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                    ),\n                    \"block_count_z\": (\n                        len(\n                            set(\n                                caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    :, 4\n                                ]\n                            )\n                        )\n                        if is3D\n                        else 1\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                nonrigid_blocks = []\n                for b_id in range(\n                    len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n                ):\n                    nonrigid_blocks.append(\n                        {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_x\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 0:2\n                                ]\n                            ),\n                            \"block_y\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 2:4\n                                ]\n                            ),\n                            \"block_z\": (\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 4:6]\n                                )\n                                if is3D\n                                else np.full_like(\n                                    np.arange(\n                                        *caiman_dataset.motion_correction[\n                                            \"coord_shifts_els\"\n                                        ][b_id, 0:2]\n                                    ),\n                                    0,\n                                )\n                            ),\n                            \"x_shifts\": caiman_dataset.motion_correction[\n                                \"x_shifts_els\"\n                            ][:, b_id],\n                            \"y_shifts\": caiman_dataset.motion_correction[\n                                \"y_shifts_els\"\n                            ][:, b_id],\n                            \"z_shifts\": (\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                                if is3D\n                                else np.full_like(\n                                    caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                        :, b_id\n                                    ],\n                                    0,\n                                )\n                            ),\n                            \"x_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"y_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"z_std\": (\n                                np.nanstd(\n                                    caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                        :, b_id\n                                    ]\n                                )\n                                if is3D\n                                else np.nan\n                            ),\n                        }\n                    )\n\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks)\n\n            # -- summary images --\n            summary_images = [\n                {\n                    **key,\n                    **fkey,\n                    \"ref_image\": ref_image,\n                    \"average_image\": ave_img,\n                    \"correlation_image\": corr_img,\n                    \"max_proj_image\": max_img,\n                }\n                for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                    field_keys,\n                    caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"reference_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"average_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"max_image\"][...][\n                        np.newaxis, ...\n                    ],\n                )\n            ]\n            self.Summary.insert(summary_images)\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.RigidMotionCorrection", "title": "<code>RigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of rigid motion correction performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts (pixels).</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts (pixels).</p> <code>z_shifts</code> <code>longblob</code> <p>z motion correction shifts (z-drift, pixels).</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames (pixels).</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames (pixels).</p> <code>z_std</code> <code>float</code> <p>standard deviation of z shifts across all frames (pixels).</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob): Mask with true for frames with outlier shifts\n            (already corrected).\n        y_shifts (longblob): y motion correction shifts (pixels).\n        x_shifts (longblob): x motion correction shifts (pixels).\n        z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n        y_std (float): standard deviation of y shifts across all frames (pixels).\n        x_std (float): standard deviation of x shifts across all frames (pixels).\n        z_std (float, optional): standard deviation of z shifts across all frames\n            (pixels).\n    \"\"\"\n\n    definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n    y_shifts            : longblob  # (pixels) y motion correction shifts\n    x_shifts            : longblob  # (pixels) x motion correction shifts\n    z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift)\n    y_std               : float     # (pixels) standard deviation of y shifts across all frames\n    x_std               : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.NonRigidMotionCorrection", "title": "<code>NonRigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob, null</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>block_height</code> <code>int</code> <p>Block height in pixels.</p> <code>block_width</code> <code>int</code> <p>Block width in pixels.</p> <code>block_depth</code> <code>int</code> <p>Block depth in pixels.</p> <code>block_count_y</code> <code>int</code> <p>Number of blocks tiled in the y direction.</p> <code>block_count_x</code> <code>int</code> <p>Number of blocks tiled in the x direction.</p> <code>block_count_z</code> <code>int</code> <p>Number of blocks tiled in the z direction.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n    blocks/patches.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob, null): Mask with true for frames with outlier\n            shifts (already corrected).\n        block_height (int): Block height in pixels.\n        block_width (int): Block width in pixels.\n        block_depth (int): Block depth in pixels.\n        block_count_y (int): Number of blocks tiled in the y direction.\n        block_count_x (int): Number of blocks tiled in the x direction.\n        block_count_z (int): Number of blocks tiled in the z direction.\n    \"\"\"\n\n    definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n    block_height        : int      # (pixels)\n    block_width         : int      # (pixels)\n    block_depth         : int      # (pixels)\n    block_count_y       : int      # number of blocks tiled in the y direction\n    block_count_x       : int      # number of blocks tiled in the x direction\n    block_count_z       : int      # number of blocks tiled in the z direction\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.Block", "title": "<code>Block</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>FOV-tiled blocks used for non-rigid motion correction.</p> <p>Attributes:</p> Name Type Description <code>NonRigidMotionCorrection</code> <code>foreign key</code> <p>Primary key from NonRigidMotionCorrection.</p> <code>block_id</code> <code>int</code> <p>Unique block ID.</p> <code>block_y</code> <code>longblob</code> <p>y_start and y_end in pixels for this block</p> <code>block_x</code> <code>longblob</code> <p>x_start and x_end in pixels for this block</p> <code>block_z</code> <code>longblob</code> <p>z_start and z_end in pixels for this block</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts for every frame in pixels</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>z_shift=null</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames in pixels</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames in pixels</p> <code>z_std=null</code> <code>float</code> <p>standard deviation of z shifts across all frames in pixels</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n    Attributes:\n        NonRigidMotionCorrection (foreign key): Primary key from\n            NonRigidMotionCorrection.\n        block_id (int): Unique block ID.\n        block_y (longblob): y_start and y_end in pixels for this block\n        block_x (longblob): x_start and x_end in pixels for this block\n        block_z (longblob): z_start and z_end in pixels for this block\n        y_shifts (longblob): y motion correction shifts for every frame in pixels\n        x_shifts (longblob): x motion correction shifts for every frame in pixels\n        z_shift=null (longblob, optional): x motion correction shifts for every frame\n            in pixels\n        y_std (float): standard deviation of y shifts across all frames in pixels\n        x_std (float): standard deviation of x shifts across all frames in pixels\n        z_std=null (float, optional): standard deviation of z shifts across all frames\n            in pixels\n    \"\"\"\n\n    definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n    -&gt; master.NonRigidMotionCorrection\n    block_id        : int\n    ---\n    block_y         : longblob  # (y_start, y_end) in pixel of this block\n    block_x         : longblob  # (x_start, x_end) in pixel of this block\n    block_z         : longblob  # (z_start, z_end) in pixel of this block\n    y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n    x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n    z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n    y_std           : float     # (pixels) standard deviation of y shifts across all frames\n    x_std           : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.Summary", "title": "<code>Summary</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Summary images for each field and channel after corrections.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>scan.ScanInfo.Field</code> <code>foreign key</code> <p>Primary key from scan.ScanInfo.Field.</p> <code>ref_image</code> <code>longblob</code> <p>Image used as alignment template.</p> <code>average_image</code> <code>longblob</code> <p>Mean of registered frames.</p> <code>correlation_image</code> <code>longblob</code> <p>Correlation map (computed during cell detection).</p> <code>max_proj_image</code> <code>longblob</code> <p>Max of registered frames.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n        ref_image (longblob): Image used as alignment template.\n        average_image (longblob): Mean of registered frames.\n        correlation_image (longblob, optional): Correlation map (computed during\n            cell detection).\n        max_proj_image (longblob, optional): Max of registered frames.\n    \"\"\"\n\n    definition = \"\"\"# Summary images for each field and channel after corrections\n    -&gt; master\n    -&gt; scan.ScanInfo.Field\n    ---\n    ref_image               : longblob  # image used as alignment template\n    average_image           : longblob  # mean of registered frames\n    correlation_image=null  : longblob  # correlation map (computed during cell detection)\n    max_proj_image=null     : longblob  # max of registered frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MotionCorrection.make", "title": "<code>make(key)</code>", "text": "<p>Populate MotionCorrection with results parsed from analysis outputs</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n        \"KEY\", \"field_z\", order_by=\"field_z\"\n    )\n\n    if method in [\"suite2p\", \"extract\"]:\n        suite2p_dataset = imaging_dataset\n\n        motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n        # ---- iterate through all s2p plane outputs ----\n        rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n        summary_images = []\n        for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n            # -- rigid motion correction --\n            if idx == 0:\n                rigid_correction = {\n                    **key,\n                    \"y_shifts\": s2p.ops[\"yoff\"],\n                    \"x_shifts\": s2p.ops[\"xoff\"],\n                    \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                    \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                    \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                    \"z_std\": np.nan,\n                    \"outlier_frames\": s2p.ops[\"badframes\"],\n                }\n            else:\n                rigid_correction[\"y_shifts\"] = np.vstack(\n                    [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                )\n                rigid_correction[\"y_std\"] = np.nanstd(\n                    rigid_correction[\"y_shifts\"].flatten()\n                )\n                rigid_correction[\"x_shifts\"] = np.vstack(\n                    [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                )\n                rigid_correction[\"x_std\"] = np.nanstd(\n                    rigid_correction[\"x_shifts\"].flatten()\n                )\n                rigid_correction[\"outlier_frames\"] = np.logical_or(\n                    rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                )\n            # -- non-rigid motion correction --\n            if s2p.ops[\"nonrigid\"]:\n                if idx == 0:\n                    nonrigid_correction = {\n                        **key,\n                        \"block_height\": s2p.ops[\"block_size\"][0],\n                        \"block_width\": s2p.ops[\"block_size\"][1],\n                        \"block_depth\": 1,\n                        \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                        \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                        \"block_count_z\": len(suite2p_dataset.planes),\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                        nonrigid_correction[\"outlier_frames\"],\n                        s2p.ops[\"badframes\"],\n                    )\n                for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                    zip(\n                        s2p.ops[\"xblock\"],\n                        s2p.ops[\"yblock\"],\n                        s2p.ops[\"yoff1\"].T,\n                        s2p.ops[\"xoff1\"].T,\n                    )\n                ):\n                    if b_id in nonrigid_blocks:\n                        nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                        )\n                        nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                        )\n                        nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                        )\n                        nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                        )\n                    else:\n                        nonrigid_blocks[b_id] = {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_y\": b_y,\n                            \"block_x\": b_x,\n                            \"block_z\": np.full_like(b_x, plane),\n                            \"y_shifts\": bshift_y,\n                            \"x_shifts\": bshift_x,\n                            \"z_shifts\": np.full(\n                                (\n                                    len(suite2p_dataset.planes),\n                                    len(bshift_x),\n                                ),\n                                0,\n                            ),\n                            \"y_std\": np.nanstd(bshift_y),\n                            \"x_std\": np.nanstd(bshift_x),\n                            \"z_std\": np.nan,\n                        }\n\n            # -- summary images --\n            motion_correction_key = (\n                scan.ScanInfo.Field * Curation &amp; key &amp; field_keys[plane]\n            ).fetch1(\"KEY\")\n            summary_images.append(\n                {\n                    **motion_correction_key,\n                    \"ref_image\": s2p.ref_image,\n                    \"average_image\": s2p.mean_image,\n                    \"correlation_image\": s2p.correlation_map,\n                    \"max_proj_image\": s2p.max_proj_image,\n                }\n            )\n\n        self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n        if rigid_correction:\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        if nonrigid_correction:\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks.values())\n        self.Summary.insert(summary_images)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        self.insert1(\n            {\n                **key,\n                \"motion_correct_channel\": caiman_dataset.alignment_channel,\n            }\n        )\n\n        is3D = caiman_dataset.params.motion[\"is3D\"]\n        if not caiman_dataset.params.motion[\"pw_rigid\"]:\n            # -- rigid motion correction --\n            rigid_correction = {\n                **key,\n                \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                \"z_shifts\": (\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                    if is3D\n                    else np.full_like(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                        0,\n                    )\n                ),\n                \"x_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                ),\n                \"y_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                ),\n                \"z_std\": (\n                    np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                    if is3D\n                    else np.nan\n                ),\n                \"outlier_frames\": None,\n            }\n\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        else:\n            # -- non-rigid motion correction --\n            nonrigid_correction = {\n                **key,\n                \"block_height\": (\n                    caiman_dataset.params.motion[\"strides\"][0]\n                    + caiman_dataset.params.motion[\"overlaps\"][0]\n                ),\n                \"block_width\": (\n                    caiman_dataset.params.motion[\"strides\"][1]\n                    + caiman_dataset.params.motion[\"overlaps\"][1]\n                ),\n                \"block_depth\": (\n                    caiman_dataset.params.motion[\"strides\"][2]\n                    + caiman_dataset.params.motion[\"overlaps\"][2]\n                    if is3D\n                    else 1\n                ),\n                \"block_count_x\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                ),\n                \"block_count_y\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                ),\n                \"block_count_z\": (\n                    len(\n                        set(\n                            caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                :, 4\n                            ]\n                        )\n                    )\n                    if is3D\n                    else 1\n                ),\n                \"outlier_frames\": None,\n            }\n\n            nonrigid_blocks = []\n            for b_id in range(\n                len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n            ):\n                nonrigid_blocks.append(\n                    {\n                        **key,\n                        \"block_id\": b_id,\n                        \"block_x\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 0:2\n                            ]\n                        ),\n                        \"block_y\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 2:4\n                            ]\n                        ),\n                        \"block_z\": (\n                            np.arange(\n                                *caiman_dataset.motion_correction[\n                                    \"coord_shifts_els\"\n                                ][b_id, 4:6]\n                            )\n                            if is3D\n                            else np.full_like(\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 0:2]\n                                ),\n                                0,\n                            )\n                        ),\n                        \"x_shifts\": caiman_dataset.motion_correction[\n                            \"x_shifts_els\"\n                        ][:, b_id],\n                        \"y_shifts\": caiman_dataset.motion_correction[\n                            \"y_shifts_els\"\n                        ][:, b_id],\n                        \"z_shifts\": (\n                            caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                :, b_id\n                            ]\n                            if is3D\n                            else np.full_like(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ],\n                                0,\n                            )\n                        ),\n                        \"x_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"y_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"z_std\": (\n                            np.nanstd(\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                            )\n                            if is3D\n                            else np.nan\n                        ),\n                    }\n                )\n\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks)\n\n        # -- summary images --\n        summary_images = [\n            {\n                **key,\n                **fkey,\n                \"ref_image\": ref_image,\n                \"average_image\": ave_img,\n                \"correlation_image\": corr_img,\n                \"max_proj_image\": max_img,\n            }\n            for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                field_keys,\n                caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"reference_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"average_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"max_image\"][...][\n                    np.newaxis, ...\n                ],\n            )\n        ]\n        self.Summary.insert(summary_images)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation", "title": "<code>Segmentation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Result of the Segmentation process.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Primary key from Curation.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Segmentation(dj.Computed):\n\"\"\"Result of the Segmentation process.\n\n    Attributes:\n        Curation (foreign key): Primary key from Curation.\n    \"\"\"\n\n    definition = \"\"\"# Different mask segmentations.\n    -&gt; Curation\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n        Attributes:\n            Segmentation (foreign key): Primary key from Segmentation.\n            mask (int): Unique mask ID.\n            scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n                used for segmentation.\n            mask_npix (int): Number of pixels in ROIs.\n            mask_center_x (int): Center x coordinate in pixel.\n            mask_center_y (int): Center y coordinate in pixel.\n            mask_center_z (int): Center z coordinate in pixel.\n            mask_xpix (longblob): X coordinates in pixels.\n            mask_ypix (longblob): Y coordinates in pixels.\n            mask_zpix (longblob): Z coordinates in pixels.\n            mask_weights (longblob): Weights of the mask at the indices above.\n        \"\"\"\n\n        definition = \"\"\" # A mask produced by segmentation.\n        -&gt; master\n        mask               : smallint\n        ---\n        -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n        mask_npix          : int       # number of pixels in ROIs\n        mask_center_x      : int       # center x coordinate in pixel\n        mask_center_y      : int       # center y coordinate in pixel\n        mask_center_z=null : int       # center z coordinate in pixel\n        mask_xpix          : longblob  # x coordinates in pixels\n        mask_ypix          : longblob  # y coordinates in pixels\n        mask_zpix=null     : longblob  # z coordinates in pixels\n        mask_weights       : longblob  # weights of the mask at the indices above\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            masks, cells = [], []\n            for plane, s2p in suite2p_dataset.planes.items():\n                mask_count = len(masks)  # increment mask id from all \"plane\"\n                for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                    zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n                ):\n                    masks.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"segmentation_channel\": s2p.segmentation_channel,\n                            \"mask_npix\": mask_stat[\"npix\"],\n                            \"mask_center_x\": mask_stat[\"med\"][1],\n                            \"mask_center_y\": mask_stat[\"med\"][0],\n                            \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                            \"mask_xpix\": mask_stat[\"xpix\"],\n                            \"mask_ypix\": mask_stat[\"ypix\"],\n                            \"mask_zpix\": np.full(\n                                mask_stat[\"npix\"],\n                                mask_stat.get(\"iplane\", plane),\n                            ),\n                            \"mask_weights\": mask_stat[\"lam\"],\n                        }\n                    )\n                    if is_cell:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"suite2p_default_classifier\",\n                                \"mask\": mask_idx + mask_count,\n                                \"mask_type\": \"soma\",\n                                \"confidence\": cell_prob,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"suite2p_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            masks, cells = [], []\n            for mask in caiman_dataset.masks:\n                masks.append(\n                    {\n                        **key,\n                        \"segmentation_channel\": segmentation_channel,\n                        \"mask\": mask[\"mask_id\"],\n                        \"mask_npix\": mask[\"mask_npix\"],\n                        \"mask_center_x\": mask[\"mask_center_x\"],\n                        \"mask_center_y\": mask[\"mask_center_y\"],\n                        \"mask_center_z\": mask[\"mask_center_z\"],\n                        \"mask_xpix\": mask[\"mask_xpix\"],\n                        \"mask_ypix\": mask[\"mask_ypix\"],\n                        \"mask_zpix\": mask[\"mask_zpix\"],\n                        \"mask_weights\": mask[\"mask_weights\"],\n                    }\n                )\n                if caiman_dataset.cnmf.estimates.idx_components is not None:\n                    if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"caiman_default_classifier\",\n                                \"mask\": mask[\"mask_id\"],\n                                \"mask_type\": \"soma\",\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"caiman_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            masks = [\n                dict(\n                    **key,\n                    segmentation_channel=0,\n                    mask=mask[\"mask_id\"],\n                    mask_npix=mask[\"mask_npix\"],\n                    mask_center_x=mask[\"mask_center_x\"],\n                    mask_center_y=mask[\"mask_center_y\"],\n                    mask_center_z=mask[\"mask_center_z\"],\n                    mask_xpix=mask[\"mask_xpix\"],\n                    mask_ypix=mask[\"mask_ypix\"],\n                    mask_zpix=mask[\"mask_zpix\"],\n                    mask_weights=mask[\"mask_weights\"],\n                )\n                for mask in extract_dataset.load_results()\n            ]\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n        else:\n            raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of the masks identified from the Segmentation procedure.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>mask</code> <code>int</code> <p>Unique mask ID.</p> <code>scan.Channel.proj(segmentation_channel='channel')</code> <code>foreign key</code> <p>Channel used for segmentation.</p> <code>mask_npix</code> <code>int</code> <p>Number of pixels in ROIs.</p> <code>mask_center_x</code> <code>int</code> <p>Center x coordinate in pixel.</p> <code>mask_center_y</code> <code>int</code> <p>Center y coordinate in pixel.</p> <code>mask_center_z</code> <code>int</code> <p>Center z coordinate in pixel.</p> <code>mask_xpix</code> <code>longblob</code> <p>X coordinates in pixels.</p> <code>mask_ypix</code> <code>longblob</code> <p>Y coordinates in pixels.</p> <code>mask_zpix</code> <code>longblob</code> <p>Z coordinates in pixels.</p> <code>mask_weights</code> <code>longblob</code> <p>Weights of the mask at the indices above.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        mask (int): Unique mask ID.\n        scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n            used for segmentation.\n        mask_npix (int): Number of pixels in ROIs.\n        mask_center_x (int): Center x coordinate in pixel.\n        mask_center_y (int): Center y coordinate in pixel.\n        mask_center_z (int): Center z coordinate in pixel.\n        mask_xpix (longblob): X coordinates in pixels.\n        mask_ypix (longblob): Y coordinates in pixels.\n        mask_zpix (longblob): Z coordinates in pixels.\n        mask_weights (longblob): Weights of the mask at the indices above.\n    \"\"\"\n\n    definition = \"\"\" # A mask produced by segmentation.\n    -&gt; master\n    mask               : smallint\n    ---\n    -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n    mask_npix          : int       # number of pixels in ROIs\n    mask_center_x      : int       # center x coordinate in pixel\n    mask_center_y      : int       # center y coordinate in pixel\n    mask_center_z=null : int       # center z coordinate in pixel\n    mask_xpix          : longblob  # x coordinates in pixels\n    mask_ypix          : longblob  # y coordinates in pixels\n    mask_zpix=null     : longblob  # z coordinates in pixels\n    mask_weights       : longblob  # weights of the mask at the indices above\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Segmentation.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Segmentation with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        masks, cells = [], []\n        for plane, s2p in suite2p_dataset.planes.items():\n            mask_count = len(masks)  # increment mask id from all \"plane\"\n            for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n            ):\n                masks.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"segmentation_channel\": s2p.segmentation_channel,\n                        \"mask_npix\": mask_stat[\"npix\"],\n                        \"mask_center_x\": mask_stat[\"med\"][1],\n                        \"mask_center_y\": mask_stat[\"med\"][0],\n                        \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                        \"mask_xpix\": mask_stat[\"xpix\"],\n                        \"mask_ypix\": mask_stat[\"ypix\"],\n                        \"mask_zpix\": np.full(\n                            mask_stat[\"npix\"],\n                            mask_stat.get(\"iplane\", plane),\n                        ),\n                        \"mask_weights\": mask_stat[\"lam\"],\n                    }\n                )\n                if is_cell:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"suite2p_default_classifier\",\n                            \"mask\": mask_idx + mask_count,\n                            \"mask_type\": \"soma\",\n                            \"confidence\": cell_prob,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"suite2p_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        masks, cells = [], []\n        for mask in caiman_dataset.masks:\n            masks.append(\n                {\n                    **key,\n                    \"segmentation_channel\": segmentation_channel,\n                    \"mask\": mask[\"mask_id\"],\n                    \"mask_npix\": mask[\"mask_npix\"],\n                    \"mask_center_x\": mask[\"mask_center_x\"],\n                    \"mask_center_y\": mask[\"mask_center_y\"],\n                    \"mask_center_z\": mask[\"mask_center_z\"],\n                    \"mask_xpix\": mask[\"mask_xpix\"],\n                    \"mask_ypix\": mask[\"mask_ypix\"],\n                    \"mask_zpix\": mask[\"mask_zpix\"],\n                    \"mask_weights\": mask[\"mask_weights\"],\n                }\n            )\n            if caiman_dataset.cnmf.estimates.idx_components is not None:\n                if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"caiman_default_classifier\",\n                            \"mask\": mask[\"mask_id\"],\n                            \"mask_type\": \"soma\",\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"caiman_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        masks = [\n            dict(\n                **key,\n                segmentation_channel=0,\n                mask=mask[\"mask_id\"],\n                mask_npix=mask[\"mask_npix\"],\n                mask_center_x=mask[\"mask_center_x\"],\n                mask_center_y=mask[\"mask_center_y\"],\n                mask_center_z=mask[\"mask_center_z\"],\n                mask_xpix=mask[\"mask_xpix\"],\n                mask_ypix=mask[\"mask_ypix\"],\n                mask_zpix=mask[\"mask_zpix\"],\n                mask_weights=mask[\"mask_weights\"],\n            )\n            for mask in extract_dataset.load_results()\n        ]\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n    else:\n        raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassificationMethod", "title": "<code>MaskClassificationMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available mask classification methods.</p> <p>Attributes:</p> Name Type Description <code>mask_classification_method</code> <code>str</code> <p>Mask classification method.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass MaskClassificationMethod(dj.Lookup):\n\"\"\"Available mask classification methods.\n\n    Attributes:\n        mask_classification_method (str): Mask classification method.\n    \"\"\"\n\n    definition = \"\"\"\n    mask_classification_method: varchar(48)\n    \"\"\"\n\n    contents = zip([\"suite2p_default_classifier\", \"caiman_default_classifier\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassification", "title": "<code>MaskClassification</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Classes assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>MaskClassificationMethod</code> <code>foreign key</code> <p>Primary key from MaskClassificationMethod.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass MaskClassification(dj.Computed):\n\"\"\"Classes assigned to each mask.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        MaskClassificationMethod (foreign key): Primary key from\n            MaskClassificationMethod.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Segmentation\n    -&gt; MaskClassificationMethod\n    \"\"\"\n\n    class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n        Attributes:\n            MaskClassification (foreign key): Primary key from MaskClassification.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            MaskType: Primary key from MaskType.\n            confidence (float, optional): Confidence level of the mask classification.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        -&gt; MaskType\n        confidence=null: float\n        \"\"\"\n\n    def make(self, key):\n        pass\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.MaskClassification.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Type assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>MaskClassification</code> <code>foreign key</code> <p>Primary key from MaskClassification.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>MaskType</code> <code>foreign key</code> <p>Primary key from MaskType.</p> <code>confidence</code> <code>float</code> <p>Confidence level of the mask classification.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n    Attributes:\n        MaskClassification (foreign key): Primary key from MaskClassification.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        MaskType: Primary key from MaskType.\n        confidence (float, optional): Confidence level of the mask classification.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    -&gt; MaskType\n    confidence=null: float\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence", "title": "<code>Fluorescence</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Fluorescence(dj.Computed):\n\"\"\"Fluorescence traces.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n    \"\"\"\n\n    definition = \"\"\"# Fluorescence traces before spike extraction or filtering\n    -&gt; Segmentation\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n                comes from.\n            fluorescence (longblob): Fluorescence trace associated with this mask.\n            neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from\n        ---\n        fluorescence                : longblob  # Fluorescence trace associated with this mask\n        neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            fluo_traces, fluo_chn2_traces = [], []\n            for s2p in suite2p_dataset.planes.values():\n                mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n                for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                    fluo_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"fluo_channel\": 0,\n                            \"fluorescence\": f,\n                            \"neuropil_fluorescence\": fneu,\n                        }\n                    )\n                if len(s2p.F_chan2):\n                    mask_chn2_count = len(\n                        fluo_chn2_traces\n                    )  # increment mask id from all planes\n                    for mask_idx, (f2, fneu2) in enumerate(\n                        zip(s2p.F_chan2, s2p.Fneu_chan2)\n                    ):\n                        fluo_chn2_traces.append(\n                            {\n                                **key,\n                                \"mask\": mask_idx + mask_chn2_count,\n                                \"fluo_channel\": 1,\n                                \"fluorescence\": f2,\n                                \"neuropil_fluorescence\": fneu2,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces + fluo_chn2_traces)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            fluo_traces = []\n            for mask in caiman_dataset.masks:\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask[\"mask_id\"],\n                        \"fluo_channel\": segmentation_channel,\n                        \"fluorescence\": mask[\"inferred_trace\"],\n                    }\n                )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n\n            fluo_traces = [\n                {\n                    **key,\n                    \"mask\": mask_id,\n                    \"fluo_channel\": 0,\n                    \"fluorescence\": fluorescence,\n                }\n                for mask_id, fluorescence in enumerate(extract_dataset.T)\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Traces obtained from segmented region of interests.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>scan.Channel.proj(fluo_channel='channel')</code> <code>int</code> <p>The channel that this trace comes from.</p> <code>fluorescence</code> <code>longblob</code> <p>Fluorescence trace associated with this mask.</p> <code>neuropil_fluorescence</code> <code>longblob</code> <p>Neuropil fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n            comes from.\n        fluorescence (longblob): Fluorescence trace associated with this mask.\n        neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from\n    ---\n    fluorescence                : longblob  # Fluorescence trace associated with this mask\n    neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Fluorescence.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Fluorescence with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        fluo_traces, fluo_chn2_traces = [], []\n        for s2p in suite2p_dataset.planes.values():\n            mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n            for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"fluo_channel\": 0,\n                        \"fluorescence\": f,\n                        \"neuropil_fluorescence\": fneu,\n                    }\n                )\n            if len(s2p.F_chan2):\n                mask_chn2_count = len(\n                    fluo_chn2_traces\n                )  # increment mask id from all planes\n                for mask_idx, (f2, fneu2) in enumerate(\n                    zip(s2p.F_chan2, s2p.Fneu_chan2)\n                ):\n                    fluo_chn2_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_chn2_count,\n                            \"fluo_channel\": 1,\n                            \"fluorescence\": f2,\n                            \"neuropil_fluorescence\": fneu2,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces + fluo_chn2_traces)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        fluo_traces = []\n        for mask in caiman_dataset.masks:\n            fluo_traces.append(\n                {\n                    **key,\n                    \"mask\": mask[\"mask_id\"],\n                    \"fluo_channel\": segmentation_channel,\n                    \"fluorescence\": mask[\"inferred_trace\"],\n                }\n            )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n\n        fluo_traces = [\n            {\n                **key,\n                \"mask\": mask_id,\n                \"fluo_channel\": 0,\n                \"fluorescence\": fluorescence,\n            }\n            for mask_id, fluorescence in enumerate(extract_dataset.T)\n        ]\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ActivityExtractionMethod", "title": "<code>ActivityExtractionMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available activity extraction methods.</p> <p>Attributes:</p> Name Type Description <code>extraction_method</code> <code>str</code> <p>Extraction method.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ActivityExtractionMethod(dj.Lookup):\n\"\"\"Available activity extraction methods.\n\n    Attributes:\n        extraction_method (str): Extraction method.\n    \"\"\"\n\n    definition = \"\"\"# Activity extraction method\n    extraction_method: varchar(32)\n    \"\"\"\n\n    contents = zip([\"suite2p_deconvolution\", \"caiman_deconvolution\", \"caiman_dff\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity", "title": "<code>Activity</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>ActivityExtractionMethod</code> <code>foreign key</code> <p>Primary key from ActivityExtractionMethod.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass Activity(dj.Computed):\n\"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        ActivityExtractionMethod (foreign key): Primary key from\n            ActivityExtractionMethod.\n    \"\"\"\n\n    definition = \"\"\"# Neural Activity\n    -&gt; Fluorescence\n    -&gt; ActivityExtractionMethod\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n        Attributes:\n            Activity (foreign key): Primary key from Activity.\n            Fluorescence.Trace (foreign key): Fluorescence.Trace.\n            activity_trace (longblob): Neural activity from fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        activity_trace: longblob\n        \"\"\"\n\n    @property\n    def key_source(self):\n        suite2p_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"suite2p\"'\n            &amp; 'extraction_method LIKE \"suite2p%\"'\n        )\n        caiman_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"caiman\"'\n            &amp; 'extraction_method LIKE \"caiman%\"'\n        )\n        return suite2p_key_source.proj() + caiman_key_source.proj()\n\n    def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n                suite2p_dataset = imaging_dataset\n                # ---- iterate through all s2p plane outputs ----\n                spikes = [\n                    dict(\n                        key,\n                        mask=mask_idx,\n                        fluo_channel=0,\n                        activity_trace=spks,\n                    )\n                    for mask_idx, spks in enumerate(\n                        s\n                        for plane in suite2p_dataset.planes.values()\n                        for s in plane.spks\n                    )\n                ]\n\n                self.insert1(key)\n                self.Trace.insert(spikes)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            if key[\"extraction_method\"] in (\n                \"caiman_deconvolution\",\n                \"caiman_dff\",\n            ):\n                attr_mapper = {\n                    \"caiman_deconvolution\": \"spikes\",\n                    \"caiman_dff\": \"dff\",\n                }\n\n                # infer \"segmentation_channel\" - from params if available, else from caiman loader\n                params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n                segmentation_channel = params.get(\n                    \"segmentation_channel\", caiman_dataset.segmentation_channel\n                )\n\n                self.insert1(key)\n                self.Trace.insert(\n                    dict(\n                        key,\n                        mask=mask[\"mask_id\"],\n                        fluo_channel=segmentation_channel,\n                        activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                    )\n                    for mask in caiman_dataset.masks\n                )\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trace(s) for each mask.</p> <p>Attributes:</p> Name Type Description <code>Activity</code> <code>foreign key</code> <p>Primary key from Activity.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Fluorescence.Trace.</p> <code>activity_trace</code> <code>longblob</code> <p>Neural activity from fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n    Attributes:\n        Activity (foreign key): Primary key from Activity.\n        Fluorescence.Trace (foreign key): Fluorescence.Trace.\n        activity_trace (longblob): Neural activity from fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    activity_trace: longblob\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.Activity.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Activity with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n            suite2p_dataset = imaging_dataset\n            # ---- iterate through all s2p plane outputs ----\n            spikes = [\n                dict(\n                    key,\n                    mask=mask_idx,\n                    fluo_channel=0,\n                    activity_trace=spks,\n                )\n                for mask_idx, spks in enumerate(\n                    s\n                    for plane in suite2p_dataset.planes.values()\n                    for s in plane.spks\n                )\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(spikes)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        if key[\"extraction_method\"] in (\n            \"caiman_deconvolution\",\n            \"caiman_dff\",\n        ):\n            attr_mapper = {\n                \"caiman_deconvolution\": \"spikes\",\n                \"caiman_dff\": \"dff\",\n            }\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            self.insert1(key)\n            self.Trace.insert(\n                dict(\n                    key,\n                    mask=mask[\"mask_id\"],\n                    fluo_channel=segmentation_channel,\n                    activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                )\n                for mask in caiman_dataset.masks\n            )\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingQualityMetrics", "title": "<code>ProcessingQualityMetrics</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Quality metrics used to evaluate the results of the calcium imaging analysis pipeline.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>@schema\nclass ProcessingQualityMetrics(dj.Computed):\n\"\"\"Quality metrics used to evaluate the results of the calcium imaging analysis pipeline.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Fluorescence\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Quality metrics used to evaluate the masks.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            mask_area (float): Mask area in square micrometer.\n            roundness (float): Roundness between 0 and 1. Values closer to 1 are rounder.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        mask_area=null: float  # Mask area in square micrometer.\n        roundness: float       # Roundness between 0 and 1. Values closer to 1 are rounder.\n        \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Quality metrics used to evaluate the fluorescence traces.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Fluorescence.Trace (foreign key): Primary key from Fluorescence.Trace.\n            skewness (float): Skewness of the fluorescence trace.\n            variance (float): Variance of the fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        skewness: float   # Skewness of the fluorescence trace.\n        variance: float   # Variance of the fluorescence trace.\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the ProcessingQualityMetrics table and its part tables.\"\"\"\n        from scipy.stats import skew\n\n        (\n            mask_xpixs,\n            mask_ypixs,\n            mask_weights,\n            fluorescence,\n            fluo_channels,\n            mask_ids,\n            mask_npix,\n            px_height,\n            px_width,\n            um_height,\n            um_width,\n        ) = (Segmentation.Mask * scan.ScanInfo.Field * Fluorescence.Trace &amp; key).fetch(\n            \"mask_xpix\",\n            \"mask_ypix\",\n            \"mask_weights\",\n            \"fluorescence\",\n            \"fluo_channel\",\n            \"mask\",\n            \"mask_npix\",\n            \"px_height\",\n            \"px_width\",\n            \"um_height\",\n            \"um_width\",\n        )\n\n        norm_mean = lambda x: x.mean() / x.max()\n        roundnesses = [\n            norm_mean(np.linalg.eigvals(np.cov(x, y, aweights=w)))\n            for x, y, w in zip(mask_xpixs, mask_ypixs, mask_weights)\n        ]\n\n        fluorescence = np.stack(fluorescence)\n\n        self.insert1(key)\n\n        self.Mask.insert(\n            dict(key, mask=mask_id, mask_area=mask_area, roundness=roundness)\n            for mask_id, mask_area, roundness in zip(\n                mask_ids,\n                mask_npix * (um_height / px_height) * (um_width / px_width),\n                roundnesses,\n            )\n        )\n\n        self.Trace.insert(\n            dict(\n                key,\n                fluo_channel=fluo_channel,\n                mask=mask_id,\n                skewness=skewness,\n                variance=variance,\n            )\n            for fluo_channel, mask_id, skewness, variance in zip(\n                fluo_channels,\n                mask_ids,\n                skew(fluorescence, axis=1),\n                fluorescence.std(axis=1),\n            )\n        )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingQualityMetrics.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Quality metrics used to evaluate the masks.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>mask_area</code> <code>float</code> <p>Mask area in square micrometer.</p> <code>roundness</code> <code>float</code> <p>Roundness between 0 and 1. Values closer to 1 are rounder.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Quality metrics used to evaluate the masks.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        mask_area (float): Mask area in square micrometer.\n        roundness (float): Roundness between 0 and 1. Values closer to 1 are rounder.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    mask_area=null: float  # Mask area in square micrometer.\n    roundness: float       # Roundness between 0 and 1. Values closer to 1 are rounder.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingQualityMetrics.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Quality metrics used to evaluate the fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Primary key from Fluorescence.Trace.</p> <code>skewness</code> <code>float</code> <p>Skewness of the fluorescence trace.</p> <code>variance</code> <code>float</code> <p>Variance of the fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Quality metrics used to evaluate the fluorescence traces.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Fluorescence.Trace (foreign key): Primary key from Fluorescence.Trace.\n        skewness (float): Skewness of the fluorescence trace.\n        variance (float): Variance of the fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    skewness: float   # Skewness of the fluorescence trace.\n    variance: float   # Variance of the fluorescence trace.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.ProcessingQualityMetrics.make", "title": "<code>make(key)</code>", "text": "<p>Populate the ProcessingQualityMetrics table and its part tables.</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the ProcessingQualityMetrics table and its part tables.\"\"\"\n    from scipy.stats import skew\n\n    (\n        mask_xpixs,\n        mask_ypixs,\n        mask_weights,\n        fluorescence,\n        fluo_channels,\n        mask_ids,\n        mask_npix,\n        px_height,\n        px_width,\n        um_height,\n        um_width,\n    ) = (Segmentation.Mask * scan.ScanInfo.Field * Fluorescence.Trace &amp; key).fetch(\n        \"mask_xpix\",\n        \"mask_ypix\",\n        \"mask_weights\",\n        \"fluorescence\",\n        \"fluo_channel\",\n        \"mask\",\n        \"mask_npix\",\n        \"px_height\",\n        \"px_width\",\n        \"um_height\",\n        \"um_width\",\n    )\n\n    norm_mean = lambda x: x.mean() / x.max()\n    roundnesses = [\n        norm_mean(np.linalg.eigvals(np.cov(x, y, aweights=w)))\n        for x, y, w in zip(mask_xpixs, mask_ypixs, mask_weights)\n    ]\n\n    fluorescence = np.stack(fluorescence)\n\n    self.insert1(key)\n\n    self.Mask.insert(\n        dict(key, mask=mask_id, mask_area=mask_area, roundness=roundness)\n        for mask_id, mask_area, roundness in zip(\n            mask_ids,\n            mask_npix * (um_height / px_height) * (um_width / px_width),\n            roundnesses,\n        )\n    )\n\n    self.Trace.insert(\n        dict(\n            key,\n            fluo_channel=fluo_channel,\n            mask=mask_id,\n            skewness=skewness,\n            variance=variance,\n        )\n        for fluo_channel, mask_id, skewness, variance in zip(\n            fluo_channels,\n            mask_ids,\n            skew(fluorescence, axis=1),\n            fluorescence.std(axis=1),\n        )\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging/#element_calcium_imaging.imaging.get_loader_result", "title": "<code>get_loader_result(key, table)</code>", "text": "<p>Retrieve the processed imaging results from a suite2p or caiman loader.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>The <code>key</code> to one entry of ProcessingTask or Curation</p> required <code>table</code> <code>dj.Table</code> <p>A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation)</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the processing_method is different than 'suite2p' or 'caiman'.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,</p> <code>Callable</code> <p>see element-interface for more information on the loaders.)</p> Source code in <code>element_calcium_imaging/imaging.py</code> <pre><code>def get_loader_result(key: dict, table: dj.Table) -&gt; Callable:\n\"\"\"Retrieve the processed imaging results from a suite2p or caiman loader.\n\n    Args:\n        key (dict): The `key` to one entry of ProcessingTask or Curation\n        table (dj.Table): A datajoint table to retrieve the loaded results from (e.g.\n            ProcessingTask, Curation)\n\n    Raises:\n        NotImplementedError: If the processing_method is different than 'suite2p' or\n            'caiman'.\n\n    Returns:\n        A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,\n        see element-interface for more information on the loaders.)\n    \"\"\"\n    method, output_dir = (ProcessingParamSet * table &amp; key).fetch1(\n        \"processing_method\", _table_attribute_mapper[table.__name__]\n    )\n\n    output_path = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n    if method == \"suite2p\" or (\n        method == \"extract\" and table.__name__ == \"MotionCorrection\"\n    ):\n        from element_interface import suite2p_loader\n\n        loaded_dataset = suite2p_loader.Suite2p(output_path)\n    elif method == \"caiman\":\n        from element_interface import caiman_loader\n\n        loaded_dataset = caiman_loader.CaImAn(output_path)\n    elif method == \"extract\":\n        from element_interface import extract_loader\n\n        loaded_dataset = extract_loader.EXTRACT(output_path)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n\n    return method, loaded_dataset\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/", "title": "imaging_no_curation.py", "text": ""}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.activate", "title": "<code>activate(imaging_schema_name, scan_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>imaging_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>imaging</code> module.</p> required <code>scan_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>scan</code> module. Omitted, if the <code>scan</code> module is already activated.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>imaging</code> module: + all that are required by the <code>scan</code> module.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <ul> <li>Session: A parent table to Scan, identifying a scanning session.</li> <li>Equipment: A parent table to Scan, identifying a scanning device.</li> </ul> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def activate(\n    imaging_schema_name: str,\n    scan_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        imaging_schema_name (str): Schema name on the database server to activate the\n            `imaging` module.\n        scan_schema_name (str): Schema name on the database server to activate the\n            `scan` module. Omitted, if the `scan` module is already activated.\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `imaging` module: + all that are required by\n            the `scan` module.\n\n    Dependencies:\n    Upstream tables:\n        + Session: A parent table to Scan, identifying a scanning session.\n        + Equipment: A parent table to Scan, identifying a scanning device.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    scan.activate(\n        scan_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        linking_module=linking_module,\n    )\n    schema.activate(\n        imaging_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    imaging_report.activate(f\"{imaging_schema_name}_report\", imaging_schema_name)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Return imaging root data director(y/ies)</p> <p>Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dirs</code> <code>list</code> <p>A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies).</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_imaging_root_data_dir() -&gt; list:\n\"\"\"Return imaging root data director(y/ies)\n\n    Retrieve the root data director(y/ies) containing the imaging data\n    for all subjects/sessions (e.g. acquired ScanImage raw files, output files from\n    processing routines, etc.). All data paths and directories in DataJoint Elements are\n    recommended to be stored as relative paths (posix format), with respect to some\n    user-configured \"root\" directory, which varies from machine to machine\n    (e.g. different mounted drive locations).\n\n    Returns:\n        dirs (list): A list of string(s) or Path(s) for the absolute paths of the imaging root data\n            director(y/ies).\n    \"\"\"\n\n    root_directories = _linking_module.get_imaging_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingMethod", "title": "<code>ProcessingMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Method, package, or analysis suite used for processing of calcium imaging data     (e.g. Suite2p, CaImAn, etc.).</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method.</p> <code>processing_method_desc</code> <code>str</code> <p>Processing method description.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ProcessingMethod(dj.Lookup):\n\"\"\"Method, package, or analysis suite used for processing of calcium imaging data\n        (e.g. Suite2p, CaImAn, etc.).\n\n    Attributes:\n        processing_method (str): Processing method.\n        processing_method_desc (str): Processing method description.\n    \"\"\"\n\n    definition = \"\"\"# Method for calcium imaging processing\n    processing_method: char(8)\n    ---\n    processing_method_desc: varchar(1000)  # Processing method description\n    \"\"\"\n\n    contents = [\n        (\"suite2p\", \"suite2p analysis suite\"),\n        (\"caiman\", \"caiman analysis suite\"),\n        (\"extract\", \"extract analysis suite\"),\n    ]\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Retrieve the root directory for all processed data.</p> <p>All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dir</code> <code>str | pathlib.Path</code> <p>Absolute path of the processed imaging root data directory.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_processed_root_data_dir() -&gt; Union[str, pathlib.Path]:\n\"\"\"Retrieve the root directory for all processed data.\n\n    All data paths and directories in DataJoint Elements are recommended to be stored as\n    relative paths (posix format), with respect to some user-configured \"root\"\n    directory, which varies from machine to machine (e.g. different mounted drive\n    locations).\n\n    Returns:\n        dir (str| pathlib.Path): Absolute path of the processed imaging root data\n            directory.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_imaging_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingParamSet", "title": "<code>ProcessingParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters.</p> <p>A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>ProcessingMethod</code> <code>foreign key</code> <p>A primary key from ProcessingMethod.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>param_set_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameter Set, a dictionary of all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ProcessingParamSet(dj.Lookup):\n\"\"\"Parameter set used for the processing of the calcium imaging scans,\n    including both the analysis suite and its respective input parameters.\n\n    A hash of the parameters of the analysis suite is also stored in order\n    to avoid duplicated entries.\n\n    Attributes:\n        paramset_idx (int): Unique parameter set ID.\n        ProcessingMethod (foreign key): A primary key from ProcessingMethod.\n        paramset_desc (str): Parameter set description.\n        param_set_hash (uuid): A universally unique identifier for the parameter set.\n        params (longblob): Parameter Set, a dictionary of all applicable parameters to\n            the analysis suite.\n    \"\"\"\n\n    definition = \"\"\"# Processing Parameter Set\n    paramset_idx: smallint  # Unique parameter set ID.\n    ---\n    -&gt; ProcessingMethod\n    paramset_desc: varchar(1280)  # Parameter-set description\n    param_set_hash: uuid  # A universally unique identifier for the parameter set\n    unique index (param_set_hash)\n    params: longblob  # Parameter Set, a dictionary of all applicable parameters to the analysis suite.\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        processing_method: str,\n        paramset_idx: int,\n        paramset_desc: str,\n        params: dict,\n    ):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n        This function automates the parameter set hashing and avoids insertion of an\n            existing parameter set.\n\n        Attributes:\n            processing_method (str): Processing method/package used for processing of\n                calcium imaging.\n            paramset_idx (int): Unique parameter set ID.\n            paramset_desc (str): Parameter set description.\n            params (dict): Parameter Set, all applicable parameters to the analysis\n                suite.\n        \"\"\"\n        if processing_method == \"extract\":\n            assert (\n                params.get(\"extract\") is not None and params.get(\"suite2p\") is not None\n            ), ValueError(\n                \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n            )\n\n            # Force Suite2p to only run motion correction.\n            params[\"suite2p\"][\"do_registration\"] = True\n            params[\"suite2p\"][\"roidetect\"] = False\n\n        param_dict = {\n            \"processing_method\": processing_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if q_param:  # If the specified param-set already exists\n            p_name = q_param.fetch1(\"paramset_idx\")\n            if p_name == paramset_idx:  # If the existed set has the same name: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set already exists - name: {}\".format(p_name)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingParamSet.insert_new_params", "title": "<code>insert_new_params(processing_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Insert a parameter set into ProcessingParamSet table.</p> <p>This function automates the parameter set hashing and avoids insertion of an     existing parameter set.</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method/package used for processing of calcium imaging.</p> <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>params</code> <code>dict</code> <p>Parameter Set, all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    processing_method: str,\n    paramset_idx: int,\n    paramset_desc: str,\n    params: dict,\n):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n    This function automates the parameter set hashing and avoids insertion of an\n        existing parameter set.\n\n    Attributes:\n        processing_method (str): Processing method/package used for processing of\n            calcium imaging.\n        paramset_idx (int): Unique parameter set ID.\n        paramset_desc (str): Parameter set description.\n        params (dict): Parameter Set, all applicable parameters to the analysis\n            suite.\n    \"\"\"\n    if processing_method == \"extract\":\n        assert (\n            params.get(\"extract\") is not None and params.get(\"suite2p\") is not None\n        ), ValueError(\n            \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n        )\n\n        # Force Suite2p to only run motion correction.\n        params[\"suite2p\"][\"do_registration\"] = True\n        params[\"suite2p\"][\"roidetect\"] = False\n\n    param_dict = {\n        \"processing_method\": processing_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if q_param:  # If the specified param-set already exists\n        p_name = q_param.fetch1(\"paramset_idx\")\n        if p_name == paramset_idx:  # If the existed set has the same name: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set already exists - name: {}\".format(p_name)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_scan_image_files", "title": "<code>get_scan_image_files(scan_key)</code>", "text": "<p>Retrieve the list of ScanImage files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of ScanImage files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_image_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of ScanImage files associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of ScanImage files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_image_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_scan_box_files", "title": "<code>get_scan_box_files(scan_key)</code>", "text": "<p>Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Scanbox files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_box_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Scanbox files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_box_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_nd2_files", "title": "<code>get_nd2_files(scan_key)</code>", "text": "<p>Retrieve the list of Nikon files (*.nd2) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Nikon files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_nd2_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Nikon files (*.nd2) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Nikon files' full file-paths.\n    \"\"\"\n    return _linking_module.get_nd2_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_prairieview_files", "title": "<code>get_prairieview_files(scan_key)</code>", "text": "<p>Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Bruker PrairieView files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_prairieview_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Bruker PrairieView files' full file-paths.\n    \"\"\"\n    return _linking_module.get_prairieview_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.CellCompartment", "title": "<code>CellCompartment</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)</p> <p>Attributes:</p> Name Type Description <code>cell_compartment</code> <code>str</code> <p>Cell compartment.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass CellCompartment(dj.Lookup):\n\"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)\n\n    Attributes:\n        cell_compartment (str): Cell compartment.\n    \"\"\"\n\n    definition = \"\"\"# Cell compartments\n    cell_compartment: char(16)\n    \"\"\"\n\n    contents = zip([\"axon\", \"soma\", \"bouton\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').</p> <p>Attributes:</p> Name Type Description <code>mask_type</code> <code>str</code> <p>Mask type.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass MaskType(dj.Lookup):\n\"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').\n\n    Attributes:\n        mask_type (str): Mask type.\n    \"\"\"\n\n    definition = \"\"\"# Possible types of a segmented mask\n    mask_type: varchar(16)\n    \"\"\"\n\n    contents = zip([\"soma\", \"axon\", \"dendrite\", \"neuropil\", \"artefact\", \"unknown\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask", "title": "<code>ProcessingTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A pairing of processing params and scans to be loaded or triggered</p> <p>This table defines a calcium imaging processing task for a combination of a <code>Scan</code> and a <code>ProcessingParamSet</code> entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table <code>Processing</code>. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods.</p> <p>Attributes:</p> Name Type Description <code>scan.Scan</code> <code>foreign key</code> <p>Primary key from scan.Scan.</p> <code>ProcessingParamSet</code> <code>foreign key</code> <p>Primary key from ProcessingParamSet.</p> <code>processing_output_dir</code> <code>str</code> <p>Output directory of the processed scan relative to the root data directory.</p> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results) or 'trigger' (trigger computation).</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ProcessingTask(dj.Manual):\n\"\"\"A pairing of processing params and scans to be loaded or triggered\n\n    This table defines a calcium imaging processing task for a combination of a\n    `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method,\n    method's parameters). The task defined here is then run in the downstream table\n    `Processing`. This table supports definitions of both loading of pre-generated results\n    and the triggering of new analysis for all supported analysis methods.\n\n    Attributes:\n        scan.Scan (foreign key): Primary key from scan.Scan.\n        ProcessingParamSet (foreign key): Primary key from ProcessingParamSet.\n        processing_output_dir (str): Output directory of the processed scan relative to the root data directory.\n        task_mode (str): One of 'load' (load computed analysis results) or 'trigger'\n            (trigger computation).\n    \"\"\"\n\n    definition = \"\"\"# Manual table for defining a processing task ready to be run\n    -&gt; scan.Scan\n    -&gt; ProcessingParamSet\n    ---\n    processing_output_dir: varchar(255)  #  Output directory of the processed scan relative to root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n        Args:\n            key (dict): Primary key from the ProcessingTask table.\n            relative (bool): If True, processing_output_dir is returned relative to\n                imaging_root_dir. Default False.\n            mkdir (bool): If True, create the processing_output_dir directory.\n                Default True.\n\n        Returns:\n            dir (str): A default output directory for the processed results (processed_output_dir\n                in ProcessingTask) based on the following convention:\n                processed_dir / scan_dir / {processing_method}_{paramset_idx}\n                e.g.: sub4/sess1/scan0/suite2p_0\n        \"\"\"\n        image_locators = {\n            \"NIS\": get_nd2_files,\n            \"ScanImage\": get_scan_image_files,\n            \"Scanbox\": get_scan_box_files,\n            \"PrairieView\": get_prairieview_files,\n        }\n        image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n        scan_dir = find_full_path(\n            get_imaging_root_data_dir(), image_locator(key)[0]\n        ).parent\n        root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n        method = (\n            (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n        )\n\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        output_dir = (\n            processed_dir\n            / scan_dir.relative_to(root_dir)\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n        Generate an entry in the ProcessingTask table for a particular scan using an\n        existing parameter set from the ProcessingParamSet table.\n\n        Args:\n            scan_key (dict): Primary key from Scan table.\n            paramset_idx (int): Unique parameter set ID.\n        \"\"\"\n        key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n        method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n            \"processing_method\"\n        )\n\n        try:\n            if method == \"suite2p\":\n                from element_interface import suite2p_loader\n\n                suite2p_loader.Suite2p(output_dir)\n            elif method == \"caiman\":\n                from element_interface import caiman_loader\n\n                caiman_loader.CaImAn(output_dir)\n            elif method == \"extract\":\n                from element_interface import extract_loader\n\n                extract_loader.EXTRACT(output_dir)\n\n            else:\n                raise NotImplementedError(\n                    \"Unknown/unimplemented method: {}\".format(method)\n                )\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"processing_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n\n    auto_generate_entries = generate\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer an output directory for an entry in ProcessingTask table.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from the ProcessingTask table.</p> required <code>relative</code> <code>bool</code> <p>If True, processing_output_dir is returned relative to imaging_root_dir. Default False.</p> <code>False</code> <code>mkdir</code> <code>bool</code> <p>If True, create the processing_output_dir directory. Default True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dir</code> <code>str</code> <p>A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n    Args:\n        key (dict): Primary key from the ProcessingTask table.\n        relative (bool): If True, processing_output_dir is returned relative to\n            imaging_root_dir. Default False.\n        mkdir (bool): If True, create the processing_output_dir directory.\n            Default True.\n\n    Returns:\n        dir (str): A default output directory for the processed results (processed_output_dir\n            in ProcessingTask) based on the following convention:\n            processed_dir / scan_dir / {processing_method}_{paramset_idx}\n            e.g.: sub4/sess1/scan0/suite2p_0\n    \"\"\"\n    image_locators = {\n        \"NIS\": get_nd2_files,\n        \"ScanImage\": get_scan_image_files,\n        \"Scanbox\": get_scan_box_files,\n        \"PrairieView\": get_prairieview_files,\n    }\n    image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n    scan_dir = find_full_path(\n        get_imaging_root_data_dir(), image_locator(key)[0]\n    ).parent\n    root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n    method = (\n        (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n    )\n\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    output_dir = (\n        processed_dir\n        / scan_dir.relative_to(root_dir)\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingTask.generate", "title": "<code>generate(scan_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet</p> <p>Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan table.</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>0</code> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@classmethod\ndef generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n    Generate an entry in the ProcessingTask table for a particular scan using an\n    existing parameter set from the ProcessingParamSet table.\n\n    Args:\n        scan_key (dict): Primary key from Scan table.\n        paramset_idx (int): Unique parameter set ID.\n    \"\"\"\n    key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n    method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n        \"processing_method\"\n    )\n\n    try:\n        if method == \"suite2p\":\n            from element_interface import suite2p_loader\n\n            suite2p_loader.Suite2p(output_dir)\n        elif method == \"caiman\":\n            from element_interface import caiman_loader\n\n            caiman_loader.CaImAn(output_dir)\n        elif method == \"extract\":\n            from element_interface import extract_loader\n\n            extract_loader.EXTRACT(output_dir)\n\n        else:\n            raise NotImplementedError(\n                \"Unknown/unimplemented method: {}\".format(method)\n            )\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"processing_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing", "title": "<code>Processing</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted.</p> <p>Attributes:</p> Name Type Description <code>ProcessingTask</code> <code>foreign key</code> <p>Primary key from ProcessingTask.</p> <code>processing_time</code> <code>datetime</code> <p>Process completion datetime.</p> <code>package_version</code> <code>str</code> <p>Version of the analysis package used in processing the data.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass Processing(dj.Computed):\n\"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table.\n    The computation is performed only on the scans with ScanInfo inserted.\n\n    Attributes:\n        ProcessingTask (foreign key): Primary key from ProcessingTask.\n        processing_time (datetime): Process completion datetime.\n        package_version (str, optional): Version of the analysis package used in\n            processing the data.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ProcessingTask\n    ---\n    processing_time     : datetime  # Time of generation of this set of processed, segmented results\n    package_version=''  : varchar(16)\n    \"\"\"\n\n    # Run processing only on Scan with ScanInfo inserted\n    @property\n    def key_source(self):\n\"\"\"Limit the Processing to Scans that have their metadata ingested to the\n        database.\"\"\"\n\n        return ProcessingTask &amp; scan.ScanInfo\n\n    def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n        task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n            \"task_mode\", \"processing_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update processing_output_dir\n            ProcessingTask.update1(\n                {**key, \"processing_output_dir\": output_dir.as_posix()}\n            )\n        output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n        if task_mode == \"load\":\n            method, imaging_dataset = get_loader_result(key, ProcessingTask)\n            if method == \"suite2p\":\n                if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                    raise NotImplementedError(\n                        \"Suite2p ingestion error - Unable to handle\"\n                        + \" ScanImage multi-ROI scanning mode yet\"\n                    )\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n            elif method == \"caiman\":\n                caiman_dataset = imaging_dataset\n                key = {**key, \"processing_time\": caiman_dataset.creation_time}\n            elif method == \"extract\":\n                raise NotImplementedError(\n                    \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n                )\n            else:\n                raise NotImplementedError(\"Unknown method: {}\".format(method))\n        elif task_mode == \"trigger\":\n\n            method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n                \"processing_method\"\n            )\n\n            image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n            image_files = [\n                find_full_path(get_imaging_root_data_dir(), image_file)\n                for image_file in image_files\n            ]\n\n            if method == \"suite2p\":\n                import suite2p\n\n                suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                suite2p_params[\"save_path0\"] = output_dir\n                (\n                    suite2p_params[\"fs\"],\n                    suite2p_params[\"nplanes\"],\n                    suite2p_params[\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                suite2p_params[\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n            elif method == \"caiman\":\n                from element_interface.caiman_loader import _process_scanimage_tiff\n                from element_interface.run_caiman import run_caiman\n\n                caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                    \"fps\", \"ndepths\", \"nchannels\"\n                )\n\n                is3D = bool(ndepths &gt; 1)\n                if is3D:\n                    raise NotImplementedError(\n                        \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                    )\n\n                # handle multi-channel tiff image before running CaImAn\n                if nchannels &gt; 1:\n                    channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                    tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                    tmp_dir.mkdir(exist_ok=True)\n                    _process_scanimage_tiff(\n                        [f.as_posix() for f in image_files], output_dir=tmp_dir\n                    )\n                    image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n                run_caiman(\n                    file_paths=[f.as_posix() for f in image_files],\n                    parameters=caiman_params,\n                    sampling_rate=sampling_rate,\n                    output_dir=output_dir,\n                    is3D=is3D,\n                )\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                caiman_dataset = imaging_dataset\n                key[\"processing_time\"] = caiman_dataset.creation_time\n\n            elif method == \"extract\":\n                import suite2p\n                from element_interface.extract_trigger import EXTRACT_trigger\n                from scipy.io import savemat\n\n                # Motion Correction with Suite2p\n                params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n                params[\"suite2p\"][\"save_path0\"] = output_dir\n                (\n                    params[\"suite2p\"][\"fs\"],\n                    params[\"suite2p\"][\"nplanes\"],\n                    params[\"suite2p\"][\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n                # Convert data.bin to registered_scans.mat\n                scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n                data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                    \"nframes\", \"px_height\", \"px_width\"\n                )\n                data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n                scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n                # Save the motion corrected movie (data.bin) in a .mat file\n                savemat(\n                    scan_matlab_fullpath,\n                    {\"M\": np.transpose(data, axes=[1, 2, 0])},\n                )\n\n                # Execute EXTRACT\n\n                ex = EXTRACT_trigger(\n                    scan_matlab_fullpath, params[\"extract\"], output_dir\n                )\n                ex.run()\n\n                _, extract_dataset = get_loader_result(key, ProcessingTask)\n                key[\"processing_time\"] = extract_dataset.creation_time\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the Processing to Scans that have their metadata ingested to the database.</p>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Processing.make", "title": "<code>make(key)</code>", "text": "<p>Execute the calcium imaging analysis defined by the ProcessingTask.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n    task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n        \"task_mode\", \"processing_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update processing_output_dir\n        ProcessingTask.update1(\n            {**key, \"processing_output_dir\": output_dir.as_posix()}\n        )\n    output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n    if task_mode == \"load\":\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n        if method == \"suite2p\":\n            if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                raise NotImplementedError(\n                    \"Suite2p ingestion error - Unable to handle\"\n                    + \" ScanImage multi-ROI scanning mode yet\"\n                )\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            key = {**key, \"processing_time\": caiman_dataset.creation_time}\n        elif method == \"extract\":\n            raise NotImplementedError(\n                \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n            )\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n    elif task_mode == \"trigger\":\n\n        method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n            \"processing_method\"\n        )\n\n        image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n        image_files = [\n            find_full_path(get_imaging_root_data_dir(), image_file)\n            for image_file in image_files\n        ]\n\n        if method == \"suite2p\":\n            import suite2p\n\n            suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            suite2p_params[\"save_path0\"] = output_dir\n            (\n                suite2p_params[\"fs\"],\n                suite2p_params[\"nplanes\"],\n                suite2p_params[\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            suite2p_params[\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n        elif method == \"caiman\":\n            from element_interface.caiman_loader import _process_scanimage_tiff\n            from element_interface.run_caiman import run_caiman\n\n            caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                \"fps\", \"ndepths\", \"nchannels\"\n            )\n\n            is3D = bool(ndepths &gt; 1)\n            if is3D:\n                raise NotImplementedError(\n                    \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                )\n\n            # handle multi-channel tiff image before running CaImAn\n            if nchannels &gt; 1:\n                channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                tmp_dir.mkdir(exist_ok=True)\n                _process_scanimage_tiff(\n                    [f.as_posix() for f in image_files], output_dir=tmp_dir\n                )\n                image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n            run_caiman(\n                file_paths=[f.as_posix() for f in image_files],\n                parameters=caiman_params,\n                sampling_rate=sampling_rate,\n                output_dir=output_dir,\n                is3D=is3D,\n            )\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            caiman_dataset = imaging_dataset\n            key[\"processing_time\"] = caiman_dataset.creation_time\n\n        elif method == \"extract\":\n            import suite2p\n            from element_interface.extract_trigger import EXTRACT_trigger\n            from scipy.io import savemat\n\n            # Motion Correction with Suite2p\n            params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n            params[\"suite2p\"][\"save_path0\"] = output_dir\n            (\n                params[\"suite2p\"][\"fs\"],\n                params[\"suite2p\"][\"nplanes\"],\n                params[\"suite2p\"][\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n            # Convert data.bin to registered_scans.mat\n            scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n            data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                \"nframes\", \"px_height\", \"px_width\"\n            )\n            data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n            scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n            # Save the motion corrected movie (data.bin) in a .mat file\n            savemat(\n                scan_matlab_fullpath,\n                {\"M\": np.transpose(data, axes=[1, 2, 0])},\n            )\n\n            # Execute EXTRACT\n\n            ex = EXTRACT_trigger(\n                scan_matlab_fullpath, params[\"extract\"], output_dir\n            )\n            ex.run()\n\n            _, extract_dataset = get_loader_result(key, ProcessingTask)\n            key[\"processing_time\"] = extract_dataset.creation_time\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection", "title": "<code>MotionCorrection</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Results of motion correction shifts performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>Processing</code> <code>foreign key</code> <p>Primary key from Processing.</p> <code>scan.Channel.proj(motion_correct_channel='channel')</code> <code>int</code> <p>Channel used for motion correction in this processing task.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass MotionCorrection(dj.Imported):\n\"\"\"Results of motion correction shifts performed on the imaging data.\n\n    Attributes:\n        Processing (foreign key): Primary key from Processing.\n        scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for\n            motion correction in this processing task.\n    \"\"\"\n\n    definition = \"\"\"# Results of motion correction\n    -&gt; Processing\n    ---\n    -&gt; scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task\n    \"\"\"\n\n    class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob): Mask with true for frames with outlier shifts\n                (already corrected).\n            y_shifts (longblob): y motion correction shifts (pixels).\n            x_shifts (longblob): x motion correction shifts (pixels).\n            z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n            y_std (float): standard deviation of y shifts across all frames (pixels).\n            x_std (float): standard deviation of x shifts across all frames (pixels).\n            z_std (float, optional): standard deviation of z shifts across all frames\n                (pixels).\n        \"\"\"\n\n        definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n        y_shifts            : longblob  # (pixels) y motion correction shifts\n        x_shifts            : longblob  # (pixels) x motion correction shifts\n        z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift)\n        y_std               : float     # (pixels) standard deviation of y shifts across all frames\n        x_std               : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n        blocks/patches.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob, null): Mask with true for frames with outlier\n                shifts (already corrected).\n            block_height (int): Block height in pixels.\n            block_width (int): Block width in pixels.\n            block_depth (int): Block depth in pixels.\n            block_count_y (int): Number of blocks tiled in the y direction.\n            block_count_x (int): Number of blocks tiled in the x direction.\n            block_count_z (int): Number of blocks tiled in the z direction.\n        \"\"\"\n\n        definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n        block_height        : int      # (pixels)\n        block_width         : int      # (pixels)\n        block_depth         : int      # (pixels)\n        block_count_y       : int      # number of blocks tiled in the y direction\n        block_count_x       : int      # number of blocks tiled in the x direction\n        block_count_z       : int      # number of blocks tiled in the z direction\n        \"\"\"\n\n    class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n        Attributes:\n            NonRigidMotionCorrection (foreign key): Primary key from\n                NonRigidMotionCorrection.\n            block_id (int): Unique block ID.\n            block_y (longblob): y_start and y_end in pixels for this block\n            block_x (longblob): x_start and x_end in pixels for this block\n            block_z (longblob): z_start and z_end in pixels for this block\n            y_shifts (longblob): y motion correction shifts for every frame in pixels\n            x_shifts (longblob): x motion correction shifts for every frame in pixels\n            z_shift=null (longblob, optional): x motion correction shifts for every frame\n                in pixels\n            y_std (float): standard deviation of y shifts across all frames in pixels\n            x_std (float): standard deviation of x shifts across all frames in pixels\n            z_std=null (float, optional): standard deviation of z shifts across all frames\n                in pixels\n        \"\"\"\n\n        definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n        -&gt; master.NonRigidMotionCorrection\n        block_id        : int\n        ---\n        block_y         : longblob  # (y_start, y_end) in pixel of this block\n        block_x         : longblob  # (x_start, x_end) in pixel of this block\n        block_z         : longblob  # (z_start, z_end) in pixel of this block\n        y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n        x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n        z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n        y_std           : float     # (pixels) standard deviation of y shifts across all frames\n        x_std           : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n            ref_image (longblob): Image used as alignment template.\n            average_image (longblob): Mean of registered frames.\n            correlation_image (longblob, optional): Correlation map (computed during\n                cell detection).\n            max_proj_image (longblob, optional): Max of registered frames.\n        \"\"\"\n\n        definition = \"\"\"# Summary images for each field and channel after corrections\n        -&gt; master\n        -&gt; scan.ScanInfo.Field\n        ---\n        ref_image               : longblob  # image used as alignment template\n        average_image           : longblob  # mean of registered frames\n        correlation_image=null  : longblob  # correlation map (computed during cell detection)\n        max_proj_image=null     : longblob  # max of registered frames\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n            \"KEY\", \"field_z\", order_by=\"field_z\"\n        )\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n            # ---- iterate through all s2p plane outputs ----\n            rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n            summary_images = []\n            for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n                # -- rigid motion correction --\n                if idx == 0:\n                    rigid_correction = {\n                        **key,\n                        \"y_shifts\": s2p.ops[\"yoff\"],\n                        \"x_shifts\": s2p.ops[\"xoff\"],\n                        \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                        \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                        \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                        \"z_std\": np.nan,\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    rigid_correction[\"y_shifts\"] = np.vstack(\n                        [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                    )\n                    rigid_correction[\"y_std\"] = np.nanstd(\n                        rigid_correction[\"y_shifts\"].flatten()\n                    )\n                    rigid_correction[\"x_shifts\"] = np.vstack(\n                        [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                    )\n                    rigid_correction[\"x_std\"] = np.nanstd(\n                        rigid_correction[\"x_shifts\"].flatten()\n                    )\n                    rigid_correction[\"outlier_frames\"] = np.logical_or(\n                        rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                    )\n                # -- non-rigid motion correction --\n                if s2p.ops[\"nonrigid\"]:\n                    if idx == 0:\n                        nonrigid_correction = {\n                            **key,\n                            \"block_height\": s2p.ops[\"block_size\"][0],\n                            \"block_width\": s2p.ops[\"block_size\"][1],\n                            \"block_depth\": 1,\n                            \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                            \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                            \"block_count_z\": len(suite2p_dataset.planes),\n                            \"outlier_frames\": s2p.ops[\"badframes\"],\n                        }\n                    else:\n                        nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                            nonrigid_correction[\"outlier_frames\"],\n                            s2p.ops[\"badframes\"],\n                        )\n                    for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                        zip(\n                            s2p.ops[\"xblock\"],\n                            s2p.ops[\"yblock\"],\n                            s2p.ops[\"yoff1\"].T,\n                            s2p.ops[\"xoff1\"].T,\n                        )\n                    ):\n                        if b_id in nonrigid_blocks:\n                            nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                            )\n                            nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                            )\n                            nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                            )\n                            nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                            )\n                        else:\n                            nonrigid_blocks[b_id] = {\n                                **key,\n                                \"block_id\": b_id,\n                                \"block_y\": b_y,\n                                \"block_x\": b_x,\n                                \"block_z\": np.full_like(b_x, plane),\n                                \"y_shifts\": bshift_y,\n                                \"x_shifts\": bshift_x,\n                                \"z_shifts\": np.full(\n                                    (\n                                        len(suite2p_dataset.planes),\n                                        len(bshift_x),\n                                    ),\n                                    0,\n                                ),\n                                \"y_std\": np.nanstd(bshift_y),\n                                \"x_std\": np.nanstd(bshift_x),\n                                \"z_std\": np.nan,\n                            }\n\n                # -- summary images --\n                motion_correction_key = (\n                    scan.ScanInfo.Field * Processing &amp; key &amp; field_keys[plane]\n                ).fetch1(\"KEY\")\n                summary_images.append(\n                    {\n                        **motion_correction_key,\n                        \"ref_image\": s2p.ref_image,\n                        \"average_image\": s2p.mean_image,\n                        \"correlation_image\": s2p.correlation_map,\n                        \"max_proj_image\": s2p.max_proj_image,\n                    }\n                )\n\n            self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n            if rigid_correction:\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            if nonrigid_correction:\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks.values())\n            self.Summary.insert(summary_images)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            self.insert1(\n                {\n                    **key,\n                    \"motion_correct_channel\": caiman_dataset.alignment_channel,\n                }\n            )\n\n            is3D = caiman_dataset.params.motion[\"is3D\"]\n            if not caiman_dataset.params.motion[\"pw_rigid\"]:\n                # -- rigid motion correction --\n                rigid_correction = {\n                    **key,\n                    \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                    \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                    \"z_shifts\": (\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                        if is3D\n                        else np.full_like(\n                            caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                            0,\n                        )\n                    ),\n                    \"x_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                    ),\n                    \"y_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                    ),\n                    \"z_std\": (\n                        np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                        if is3D\n                        else np.nan\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            else:\n                # -- non-rigid motion correction --\n                nonrigid_correction = {\n                    **key,\n                    \"block_height\": (\n                        caiman_dataset.params.motion[\"strides\"][0]\n                        + caiman_dataset.params.motion[\"overlaps\"][0]\n                    ),\n                    \"block_width\": (\n                        caiman_dataset.params.motion[\"strides\"][1]\n                        + caiman_dataset.params.motion[\"overlaps\"][1]\n                    ),\n                    \"block_depth\": (\n                        caiman_dataset.params.motion[\"strides\"][2]\n                        + caiman_dataset.params.motion[\"overlaps\"][2]\n                        if is3D\n                        else 1\n                    ),\n                    \"block_count_x\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                    ),\n                    \"block_count_y\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                    ),\n                    \"block_count_z\": (\n                        len(\n                            set(\n                                caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    :, 4\n                                ]\n                            )\n                        )\n                        if is3D\n                        else 1\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                nonrigid_blocks = []\n                for b_id in range(\n                    len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n                ):\n                    nonrigid_blocks.append(\n                        {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_x\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 0:2\n                                ]\n                            ),\n                            \"block_y\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 2:4\n                                ]\n                            ),\n                            \"block_z\": (\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 4:6]\n                                )\n                                if is3D\n                                else np.full_like(\n                                    np.arange(\n                                        *caiman_dataset.motion_correction[\n                                            \"coord_shifts_els\"\n                                        ][b_id, 0:2]\n                                    ),\n                                    0,\n                                )\n                            ),\n                            \"x_shifts\": caiman_dataset.motion_correction[\n                                \"x_shifts_els\"\n                            ][:, b_id],\n                            \"y_shifts\": caiman_dataset.motion_correction[\n                                \"y_shifts_els\"\n                            ][:, b_id],\n                            \"z_shifts\": (\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                                if is3D\n                                else np.full_like(\n                                    caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                        :, b_id\n                                    ],\n                                    0,\n                                )\n                            ),\n                            \"x_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"y_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"z_std\": (\n                                np.nanstd(\n                                    caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                        :, b_id\n                                    ]\n                                )\n                                if is3D\n                                else np.nan\n                            ),\n                        }\n                    )\n\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks)\n\n            # -- summary images --\n            summary_images = [\n                {\n                    **key,\n                    **fkey,\n                    \"ref_image\": ref_image,\n                    \"average_image\": ave_img,\n                    \"correlation_image\": corr_img,\n                    \"max_proj_image\": max_img,\n                }\n                for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                    field_keys,\n                    caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"reference_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"average_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"max_image\"][...][\n                        np.newaxis, ...\n                    ],\n                )\n            ]\n            self.Summary.insert(summary_images)\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.RigidMotionCorrection", "title": "<code>RigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of rigid motion correction performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts (pixels).</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts (pixels).</p> <code>z_shifts</code> <code>longblob</code> <p>z motion correction shifts (z-drift, pixels).</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames (pixels).</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames (pixels).</p> <code>z_std</code> <code>float</code> <p>standard deviation of z shifts across all frames (pixels).</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob): Mask with true for frames with outlier shifts\n            (already corrected).\n        y_shifts (longblob): y motion correction shifts (pixels).\n        x_shifts (longblob): x motion correction shifts (pixels).\n        z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n        y_std (float): standard deviation of y shifts across all frames (pixels).\n        x_std (float): standard deviation of x shifts across all frames (pixels).\n        z_std (float, optional): standard deviation of z shifts across all frames\n            (pixels).\n    \"\"\"\n\n    definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n    y_shifts            : longblob  # (pixels) y motion correction shifts\n    x_shifts            : longblob  # (pixels) x motion correction shifts\n    z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift)\n    y_std               : float     # (pixels) standard deviation of y shifts across all frames\n    x_std               : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.NonRigidMotionCorrection", "title": "<code>NonRigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob, null</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>block_height</code> <code>int</code> <p>Block height in pixels.</p> <code>block_width</code> <code>int</code> <p>Block width in pixels.</p> <code>block_depth</code> <code>int</code> <p>Block depth in pixels.</p> <code>block_count_y</code> <code>int</code> <p>Number of blocks tiled in the y direction.</p> <code>block_count_x</code> <code>int</code> <p>Number of blocks tiled in the x direction.</p> <code>block_count_z</code> <code>int</code> <p>Number of blocks tiled in the z direction.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n    blocks/patches.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob, null): Mask with true for frames with outlier\n            shifts (already corrected).\n        block_height (int): Block height in pixels.\n        block_width (int): Block width in pixels.\n        block_depth (int): Block depth in pixels.\n        block_count_y (int): Number of blocks tiled in the y direction.\n        block_count_x (int): Number of blocks tiled in the x direction.\n        block_count_z (int): Number of blocks tiled in the z direction.\n    \"\"\"\n\n    definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n    block_height        : int      # (pixels)\n    block_width         : int      # (pixels)\n    block_depth         : int      # (pixels)\n    block_count_y       : int      # number of blocks tiled in the y direction\n    block_count_x       : int      # number of blocks tiled in the x direction\n    block_count_z       : int      # number of blocks tiled in the z direction\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.Block", "title": "<code>Block</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>FOV-tiled blocks used for non-rigid motion correction.</p> <p>Attributes:</p> Name Type Description <code>NonRigidMotionCorrection</code> <code>foreign key</code> <p>Primary key from NonRigidMotionCorrection.</p> <code>block_id</code> <code>int</code> <p>Unique block ID.</p> <code>block_y</code> <code>longblob</code> <p>y_start and y_end in pixels for this block</p> <code>block_x</code> <code>longblob</code> <p>x_start and x_end in pixels for this block</p> <code>block_z</code> <code>longblob</code> <p>z_start and z_end in pixels for this block</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts for every frame in pixels</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>z_shift=null</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames in pixels</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames in pixels</p> <code>z_std=null</code> <code>float</code> <p>standard deviation of z shifts across all frames in pixels</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n    Attributes:\n        NonRigidMotionCorrection (foreign key): Primary key from\n            NonRigidMotionCorrection.\n        block_id (int): Unique block ID.\n        block_y (longblob): y_start and y_end in pixels for this block\n        block_x (longblob): x_start and x_end in pixels for this block\n        block_z (longblob): z_start and z_end in pixels for this block\n        y_shifts (longblob): y motion correction shifts for every frame in pixels\n        x_shifts (longblob): x motion correction shifts for every frame in pixels\n        z_shift=null (longblob, optional): x motion correction shifts for every frame\n            in pixels\n        y_std (float): standard deviation of y shifts across all frames in pixels\n        x_std (float): standard deviation of x shifts across all frames in pixels\n        z_std=null (float, optional): standard deviation of z shifts across all frames\n            in pixels\n    \"\"\"\n\n    definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n    -&gt; master.NonRigidMotionCorrection\n    block_id        : int\n    ---\n    block_y         : longblob  # (y_start, y_end) in pixel of this block\n    block_x         : longblob  # (x_start, x_end) in pixel of this block\n    block_z         : longblob  # (z_start, z_end) in pixel of this block\n    y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n    x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n    z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n    y_std           : float     # (pixels) standard deviation of y shifts across all frames\n    x_std           : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.Summary", "title": "<code>Summary</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Summary images for each field and channel after corrections.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>scan.ScanInfo.Field</code> <code>foreign key</code> <p>Primary key from scan.ScanInfo.Field.</p> <code>ref_image</code> <code>longblob</code> <p>Image used as alignment template.</p> <code>average_image</code> <code>longblob</code> <p>Mean of registered frames.</p> <code>correlation_image</code> <code>longblob</code> <p>Correlation map (computed during cell detection).</p> <code>max_proj_image</code> <code>longblob</code> <p>Max of registered frames.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n        ref_image (longblob): Image used as alignment template.\n        average_image (longblob): Mean of registered frames.\n        correlation_image (longblob, optional): Correlation map (computed during\n            cell detection).\n        max_proj_image (longblob, optional): Max of registered frames.\n    \"\"\"\n\n    definition = \"\"\"# Summary images for each field and channel after corrections\n    -&gt; master\n    -&gt; scan.ScanInfo.Field\n    ---\n    ref_image               : longblob  # image used as alignment template\n    average_image           : longblob  # mean of registered frames\n    correlation_image=null  : longblob  # correlation map (computed during cell detection)\n    max_proj_image=null     : longblob  # max of registered frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MotionCorrection.make", "title": "<code>make(key)</code>", "text": "<p>Populate MotionCorrection with results parsed from analysis outputs</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n        \"KEY\", \"field_z\", order_by=\"field_z\"\n    )\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n        # ---- iterate through all s2p plane outputs ----\n        rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n        summary_images = []\n        for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n            # -- rigid motion correction --\n            if idx == 0:\n                rigid_correction = {\n                    **key,\n                    \"y_shifts\": s2p.ops[\"yoff\"],\n                    \"x_shifts\": s2p.ops[\"xoff\"],\n                    \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                    \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                    \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                    \"z_std\": np.nan,\n                    \"outlier_frames\": s2p.ops[\"badframes\"],\n                }\n            else:\n                rigid_correction[\"y_shifts\"] = np.vstack(\n                    [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                )\n                rigid_correction[\"y_std\"] = np.nanstd(\n                    rigid_correction[\"y_shifts\"].flatten()\n                )\n                rigid_correction[\"x_shifts\"] = np.vstack(\n                    [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                )\n                rigid_correction[\"x_std\"] = np.nanstd(\n                    rigid_correction[\"x_shifts\"].flatten()\n                )\n                rigid_correction[\"outlier_frames\"] = np.logical_or(\n                    rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                )\n            # -- non-rigid motion correction --\n            if s2p.ops[\"nonrigid\"]:\n                if idx == 0:\n                    nonrigid_correction = {\n                        **key,\n                        \"block_height\": s2p.ops[\"block_size\"][0],\n                        \"block_width\": s2p.ops[\"block_size\"][1],\n                        \"block_depth\": 1,\n                        \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                        \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                        \"block_count_z\": len(suite2p_dataset.planes),\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                        nonrigid_correction[\"outlier_frames\"],\n                        s2p.ops[\"badframes\"],\n                    )\n                for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                    zip(\n                        s2p.ops[\"xblock\"],\n                        s2p.ops[\"yblock\"],\n                        s2p.ops[\"yoff1\"].T,\n                        s2p.ops[\"xoff1\"].T,\n                    )\n                ):\n                    if b_id in nonrigid_blocks:\n                        nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                        )\n                        nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                        )\n                        nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                        )\n                        nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                        )\n                    else:\n                        nonrigid_blocks[b_id] = {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_y\": b_y,\n                            \"block_x\": b_x,\n                            \"block_z\": np.full_like(b_x, plane),\n                            \"y_shifts\": bshift_y,\n                            \"x_shifts\": bshift_x,\n                            \"z_shifts\": np.full(\n                                (\n                                    len(suite2p_dataset.planes),\n                                    len(bshift_x),\n                                ),\n                                0,\n                            ),\n                            \"y_std\": np.nanstd(bshift_y),\n                            \"x_std\": np.nanstd(bshift_x),\n                            \"z_std\": np.nan,\n                        }\n\n            # -- summary images --\n            motion_correction_key = (\n                scan.ScanInfo.Field * Processing &amp; key &amp; field_keys[plane]\n            ).fetch1(\"KEY\")\n            summary_images.append(\n                {\n                    **motion_correction_key,\n                    \"ref_image\": s2p.ref_image,\n                    \"average_image\": s2p.mean_image,\n                    \"correlation_image\": s2p.correlation_map,\n                    \"max_proj_image\": s2p.max_proj_image,\n                }\n            )\n\n        self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n        if rigid_correction:\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        if nonrigid_correction:\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks.values())\n        self.Summary.insert(summary_images)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        self.insert1(\n            {\n                **key,\n                \"motion_correct_channel\": caiman_dataset.alignment_channel,\n            }\n        )\n\n        is3D = caiman_dataset.params.motion[\"is3D\"]\n        if not caiman_dataset.params.motion[\"pw_rigid\"]:\n            # -- rigid motion correction --\n            rigid_correction = {\n                **key,\n                \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                \"z_shifts\": (\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                    if is3D\n                    else np.full_like(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                        0,\n                    )\n                ),\n                \"x_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                ),\n                \"y_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                ),\n                \"z_std\": (\n                    np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                    if is3D\n                    else np.nan\n                ),\n                \"outlier_frames\": None,\n            }\n\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        else:\n            # -- non-rigid motion correction --\n            nonrigid_correction = {\n                **key,\n                \"block_height\": (\n                    caiman_dataset.params.motion[\"strides\"][0]\n                    + caiman_dataset.params.motion[\"overlaps\"][0]\n                ),\n                \"block_width\": (\n                    caiman_dataset.params.motion[\"strides\"][1]\n                    + caiman_dataset.params.motion[\"overlaps\"][1]\n                ),\n                \"block_depth\": (\n                    caiman_dataset.params.motion[\"strides\"][2]\n                    + caiman_dataset.params.motion[\"overlaps\"][2]\n                    if is3D\n                    else 1\n                ),\n                \"block_count_x\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                ),\n                \"block_count_y\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                ),\n                \"block_count_z\": (\n                    len(\n                        set(\n                            caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                :, 4\n                            ]\n                        )\n                    )\n                    if is3D\n                    else 1\n                ),\n                \"outlier_frames\": None,\n            }\n\n            nonrigid_blocks = []\n            for b_id in range(\n                len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n            ):\n                nonrigid_blocks.append(\n                    {\n                        **key,\n                        \"block_id\": b_id,\n                        \"block_x\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 0:2\n                            ]\n                        ),\n                        \"block_y\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 2:4\n                            ]\n                        ),\n                        \"block_z\": (\n                            np.arange(\n                                *caiman_dataset.motion_correction[\n                                    \"coord_shifts_els\"\n                                ][b_id, 4:6]\n                            )\n                            if is3D\n                            else np.full_like(\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 0:2]\n                                ),\n                                0,\n                            )\n                        ),\n                        \"x_shifts\": caiman_dataset.motion_correction[\n                            \"x_shifts_els\"\n                        ][:, b_id],\n                        \"y_shifts\": caiman_dataset.motion_correction[\n                            \"y_shifts_els\"\n                        ][:, b_id],\n                        \"z_shifts\": (\n                            caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                :, b_id\n                            ]\n                            if is3D\n                            else np.full_like(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ],\n                                0,\n                            )\n                        ),\n                        \"x_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"y_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"z_std\": (\n                            np.nanstd(\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                            )\n                            if is3D\n                            else np.nan\n                        ),\n                    }\n                )\n\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks)\n\n        # -- summary images --\n        summary_images = [\n            {\n                **key,\n                **fkey,\n                \"ref_image\": ref_image,\n                \"average_image\": ave_img,\n                \"correlation_image\": corr_img,\n                \"max_proj_image\": max_img,\n            }\n            for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                field_keys,\n                caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"reference_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"average_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"max_image\"][...][\n                    np.newaxis, ...\n                ],\n            )\n        ]\n        self.Summary.insert(summary_images)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation", "title": "<code>Segmentation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Result of the Segmentation process.</p> <p>Attributes:</p> Name Type Description <code>Processing</code> <code>foreign key</code> <p>Primary key from Processing.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass Segmentation(dj.Computed):\n\"\"\"Result of the Segmentation process.\n\n    Attributes:\n        Processing (foreign key): Primary key from Processing.\n    \"\"\"\n\n    definition = \"\"\"# Different mask segmentations.\n    -&gt; Processing\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n        Attributes:\n            Segmentation (foreign key): Primary key from Segmentation.\n            mask (int): Unique mask ID.\n            scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n                used for segmentation.\n            mask_npix (int): Number of pixels in ROIs.\n            mask_center_x (int): Center x coordinate in pixel.\n            mask_center_y (int): Center y coordinate in pixel.\n            mask_center_z (int): Center z coordinate in pixel.\n            mask_xpix (longblob): X coordinates in pixels.\n            mask_ypix (longblob): Y coordinates in pixels.\n            mask_zpix (longblob): Z coordinates in pixels.\n            mask_weights (longblob): Weights of the mask at the indices above.\n        \"\"\"\n\n        definition = \"\"\" # A mask produced by segmentation.\n        -&gt; master\n        mask               : smallint\n        ---\n        -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n        mask_npix          : int       # number of pixels in ROIs\n        mask_center_x      : int       # center x coordinate in pixel\n        mask_center_y      : int       # center y coordinate in pixel\n        mask_center_z=null : int       # center z coordinate in pixel\n        mask_xpix          : longblob  # x coordinates in pixels\n        mask_ypix          : longblob  # y coordinates in pixels\n        mask_zpix=null     : longblob  # z coordinates in pixels\n        mask_weights       : longblob  # weights of the mask at the indices above\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            masks, cells = [], []\n            for plane, s2p in suite2p_dataset.planes.items():\n                mask_count = len(masks)  # increment mask id from all \"plane\"\n                for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                    zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n                ):\n                    masks.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"segmentation_channel\": s2p.segmentation_channel,\n                            \"mask_npix\": mask_stat[\"npix\"],\n                            \"mask_center_x\": mask_stat[\"med\"][1],\n                            \"mask_center_y\": mask_stat[\"med\"][0],\n                            \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                            \"mask_xpix\": mask_stat[\"xpix\"],\n                            \"mask_ypix\": mask_stat[\"ypix\"],\n                            \"mask_zpix\": np.full(\n                                mask_stat[\"npix\"],\n                                mask_stat.get(\"iplane\", plane),\n                            ),\n                            \"mask_weights\": mask_stat[\"lam\"],\n                        }\n                    )\n                    if is_cell:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"suite2p_default_classifier\",\n                                \"mask\": mask_idx + mask_count,\n                                \"mask_type\": \"soma\",\n                                \"confidence\": cell_prob,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"suite2p_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            masks, cells = [], []\n            for mask in caiman_dataset.masks:\n                masks.append(\n                    {\n                        **key,\n                        \"segmentation_channel\": segmentation_channel,\n                        \"mask\": mask[\"mask_id\"],\n                        \"mask_npix\": mask[\"mask_npix\"],\n                        \"mask_center_x\": mask[\"mask_center_x\"],\n                        \"mask_center_y\": mask[\"mask_center_y\"],\n                        \"mask_center_z\": mask[\"mask_center_z\"],\n                        \"mask_xpix\": mask[\"mask_xpix\"],\n                        \"mask_ypix\": mask[\"mask_ypix\"],\n                        \"mask_zpix\": mask[\"mask_zpix\"],\n                        \"mask_weights\": mask[\"mask_weights\"],\n                    }\n                )\n                if caiman_dataset.cnmf.estimates.idx_components is not None:\n                    if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"caiman_default_classifier\",\n                                \"mask\": mask[\"mask_id\"],\n                                \"mask_type\": \"soma\",\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"caiman_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            masks = [\n                dict(\n                    **key,\n                    segmentation_channel=0,\n                    mask=mask[\"mask_id\"],\n                    mask_npix=mask[\"mask_npix\"],\n                    mask_center_x=mask[\"mask_center_x\"],\n                    mask_center_y=mask[\"mask_center_y\"],\n                    mask_center_z=mask[\"mask_center_z\"],\n                    mask_xpix=mask[\"mask_xpix\"],\n                    mask_ypix=mask[\"mask_ypix\"],\n                    mask_zpix=mask[\"mask_zpix\"],\n                    mask_weights=mask[\"mask_weights\"],\n                )\n                for mask in extract_dataset.load_results()\n            ]\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n        else:\n            raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of the masks identified from the Segmentation procedure.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>mask</code> <code>int</code> <p>Unique mask ID.</p> <code>scan.Channel.proj(segmentation_channel='channel')</code> <code>foreign key</code> <p>Channel used for segmentation.</p> <code>mask_npix</code> <code>int</code> <p>Number of pixels in ROIs.</p> <code>mask_center_x</code> <code>int</code> <p>Center x coordinate in pixel.</p> <code>mask_center_y</code> <code>int</code> <p>Center y coordinate in pixel.</p> <code>mask_center_z</code> <code>int</code> <p>Center z coordinate in pixel.</p> <code>mask_xpix</code> <code>longblob</code> <p>X coordinates in pixels.</p> <code>mask_ypix</code> <code>longblob</code> <p>Y coordinates in pixels.</p> <code>mask_zpix</code> <code>longblob</code> <p>Z coordinates in pixels.</p> <code>mask_weights</code> <code>longblob</code> <p>Weights of the mask at the indices above.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        mask (int): Unique mask ID.\n        scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n            used for segmentation.\n        mask_npix (int): Number of pixels in ROIs.\n        mask_center_x (int): Center x coordinate in pixel.\n        mask_center_y (int): Center y coordinate in pixel.\n        mask_center_z (int): Center z coordinate in pixel.\n        mask_xpix (longblob): X coordinates in pixels.\n        mask_ypix (longblob): Y coordinates in pixels.\n        mask_zpix (longblob): Z coordinates in pixels.\n        mask_weights (longblob): Weights of the mask at the indices above.\n    \"\"\"\n\n    definition = \"\"\" # A mask produced by segmentation.\n    -&gt; master\n    mask               : smallint\n    ---\n    -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n    mask_npix          : int       # number of pixels in ROIs\n    mask_center_x      : int       # center x coordinate in pixel\n    mask_center_y      : int       # center y coordinate in pixel\n    mask_center_z=null : int       # center z coordinate in pixel\n    mask_xpix          : longblob  # x coordinates in pixels\n    mask_ypix          : longblob  # y coordinates in pixels\n    mask_zpix=null     : longblob  # z coordinates in pixels\n    mask_weights       : longblob  # weights of the mask at the indices above\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Segmentation.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Segmentation with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        masks, cells = [], []\n        for plane, s2p in suite2p_dataset.planes.items():\n            mask_count = len(masks)  # increment mask id from all \"plane\"\n            for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n            ):\n                masks.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"segmentation_channel\": s2p.segmentation_channel,\n                        \"mask_npix\": mask_stat[\"npix\"],\n                        \"mask_center_x\": mask_stat[\"med\"][1],\n                        \"mask_center_y\": mask_stat[\"med\"][0],\n                        \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                        \"mask_xpix\": mask_stat[\"xpix\"],\n                        \"mask_ypix\": mask_stat[\"ypix\"],\n                        \"mask_zpix\": np.full(\n                            mask_stat[\"npix\"],\n                            mask_stat.get(\"iplane\", plane),\n                        ),\n                        \"mask_weights\": mask_stat[\"lam\"],\n                    }\n                )\n                if is_cell:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"suite2p_default_classifier\",\n                            \"mask\": mask_idx + mask_count,\n                            \"mask_type\": \"soma\",\n                            \"confidence\": cell_prob,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"suite2p_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        masks, cells = [], []\n        for mask in caiman_dataset.masks:\n            masks.append(\n                {\n                    **key,\n                    \"segmentation_channel\": segmentation_channel,\n                    \"mask\": mask[\"mask_id\"],\n                    \"mask_npix\": mask[\"mask_npix\"],\n                    \"mask_center_x\": mask[\"mask_center_x\"],\n                    \"mask_center_y\": mask[\"mask_center_y\"],\n                    \"mask_center_z\": mask[\"mask_center_z\"],\n                    \"mask_xpix\": mask[\"mask_xpix\"],\n                    \"mask_ypix\": mask[\"mask_ypix\"],\n                    \"mask_zpix\": mask[\"mask_zpix\"],\n                    \"mask_weights\": mask[\"mask_weights\"],\n                }\n            )\n            if caiman_dataset.cnmf.estimates.idx_components is not None:\n                if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"caiman_default_classifier\",\n                            \"mask\": mask[\"mask_id\"],\n                            \"mask_type\": \"soma\",\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"caiman_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        masks = [\n            dict(\n                **key,\n                segmentation_channel=0,\n                mask=mask[\"mask_id\"],\n                mask_npix=mask[\"mask_npix\"],\n                mask_center_x=mask[\"mask_center_x\"],\n                mask_center_y=mask[\"mask_center_y\"],\n                mask_center_z=mask[\"mask_center_z\"],\n                mask_xpix=mask[\"mask_xpix\"],\n                mask_ypix=mask[\"mask_ypix\"],\n                mask_zpix=mask[\"mask_zpix\"],\n                mask_weights=mask[\"mask_weights\"],\n            )\n            for mask in extract_dataset.load_results()\n        ]\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n    else:\n        raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassificationMethod", "title": "<code>MaskClassificationMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available mask classification methods.</p> <p>Attributes:</p> Name Type Description <code>mask_classification_method</code> <code>str</code> <p>Mask classification method.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass MaskClassificationMethod(dj.Lookup):\n\"\"\"Available mask classification methods.\n\n    Attributes:\n        mask_classification_method (str): Mask classification method.\n    \"\"\"\n\n    definition = \"\"\"\n    mask_classification_method: varchar(48)\n    \"\"\"\n\n    contents = zip([\"suite2p_default_classifier\", \"caiman_default_classifier\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassification", "title": "<code>MaskClassification</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Classes assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>MaskClassificationMethod</code> <code>foreign key</code> <p>Primary key from MaskClassificationMethod.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass MaskClassification(dj.Computed):\n\"\"\"Classes assigned to each mask.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        MaskClassificationMethod (foreign key): Primary key from\n            MaskClassificationMethod.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Segmentation\n    -&gt; MaskClassificationMethod\n    \"\"\"\n\n    class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n        Attributes:\n            MaskClassification (foreign key): Primary key from MaskClassification.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            MaskType: Primary key from MaskType.\n            confidence (float, optional): Confidence level of the mask classification.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        -&gt; MaskType\n        confidence=null: float\n        \"\"\"\n\n    def make(self, key):\n        pass\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.MaskClassification.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Type assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>MaskClassification</code> <code>foreign key</code> <p>Primary key from MaskClassification.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>MaskType</code> <code>foreign key</code> <p>Primary key from MaskType.</p> <code>confidence</code> <code>float</code> <p>Confidence level of the mask classification.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n    Attributes:\n        MaskClassification (foreign key): Primary key from MaskClassification.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        MaskType: Primary key from MaskType.\n        confidence (float, optional): Confidence level of the mask classification.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    -&gt; MaskType\n    confidence=null: float\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence", "title": "<code>Fluorescence</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass Fluorescence(dj.Computed):\n\"\"\"Fluorescence traces.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n    \"\"\"\n\n    definition = \"\"\"# Fluorescence traces before spike extraction or filtering\n    -&gt; Segmentation\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n                comes from.\n            fluorescence (longblob): Fluorescence trace associated with this mask.\n            neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from\n        ---\n        fluorescence                : longblob  # Fluorescence trace associated with this mask\n        neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            fluo_traces, fluo_chn2_traces = [], []\n            for s2p in suite2p_dataset.planes.values():\n                mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n                for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                    fluo_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"fluo_channel\": 0,\n                            \"fluorescence\": f,\n                            \"neuropil_fluorescence\": fneu,\n                        }\n                    )\n                if len(s2p.F_chan2):\n                    mask_chn2_count = len(\n                        fluo_chn2_traces\n                    )  # increment mask id from all planes\n                    for mask_idx, (f2, fneu2) in enumerate(\n                        zip(s2p.F_chan2, s2p.Fneu_chan2)\n                    ):\n                        fluo_chn2_traces.append(\n                            {\n                                **key,\n                                \"mask\": mask_idx + mask_chn2_count,\n                                \"fluo_channel\": 1,\n                                \"fluorescence\": f2,\n                                \"neuropil_fluorescence\": fneu2,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces + fluo_chn2_traces)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            fluo_traces = []\n            for mask in caiman_dataset.masks:\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask[\"mask_id\"],\n                        \"fluo_channel\": segmentation_channel,\n                        \"fluorescence\": mask[\"inferred_trace\"],\n                    }\n                )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n\n            fluo_traces = [\n                {\n                    **key,\n                    \"mask\": mask_id,\n                    \"fluo_channel\": 0,\n                    \"fluorescence\": fluorescence,\n                }\n                for mask_id, fluorescence in enumerate(extract_dataset.T)\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Traces obtained from segmented region of interests.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>scan.Channel.proj(fluo_channel='channel')</code> <code>int</code> <p>The channel that this trace comes from.</p> <code>fluorescence</code> <code>longblob</code> <p>Fluorescence trace associated with this mask.</p> <code>neuropil_fluorescence</code> <code>longblob</code> <p>Neuropil fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n            comes from.\n        fluorescence (longblob): Fluorescence trace associated with this mask.\n        neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from\n    ---\n    fluorescence                : longblob  # Fluorescence trace associated with this mask\n    neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Fluorescence.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Fluorescence with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        fluo_traces, fluo_chn2_traces = [], []\n        for s2p in suite2p_dataset.planes.values():\n            mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n            for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"fluo_channel\": 0,\n                        \"fluorescence\": f,\n                        \"neuropil_fluorescence\": fneu,\n                    }\n                )\n            if len(s2p.F_chan2):\n                mask_chn2_count = len(\n                    fluo_chn2_traces\n                )  # increment mask id from all planes\n                for mask_idx, (f2, fneu2) in enumerate(\n                    zip(s2p.F_chan2, s2p.Fneu_chan2)\n                ):\n                    fluo_chn2_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_chn2_count,\n                            \"fluo_channel\": 1,\n                            \"fluorescence\": f2,\n                            \"neuropil_fluorescence\": fneu2,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces + fluo_chn2_traces)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        fluo_traces = []\n        for mask in caiman_dataset.masks:\n            fluo_traces.append(\n                {\n                    **key,\n                    \"mask\": mask[\"mask_id\"],\n                    \"fluo_channel\": segmentation_channel,\n                    \"fluorescence\": mask[\"inferred_trace\"],\n                }\n            )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n\n        fluo_traces = [\n            {\n                **key,\n                \"mask\": mask_id,\n                \"fluo_channel\": 0,\n                \"fluorescence\": fluorescence,\n            }\n            for mask_id, fluorescence in enumerate(extract_dataset.T)\n        ]\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ActivityExtractionMethod", "title": "<code>ActivityExtractionMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available activity extraction methods.</p> <p>Attributes:</p> Name Type Description <code>extraction_method</code> <code>str</code> <p>Extraction method.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ActivityExtractionMethod(dj.Lookup):\n\"\"\"Available activity extraction methods.\n\n    Attributes:\n        extraction_method (str): Extraction method.\n    \"\"\"\n\n    definition = \"\"\"# Activity extraction method\n    extraction_method: varchar(32)\n    \"\"\"\n\n    contents = zip([\"suite2p_deconvolution\", \"caiman_deconvolution\", \"caiman_dff\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity", "title": "<code>Activity</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>ActivityExtractionMethod</code> <code>foreign key</code> <p>Primary key from ActivityExtractionMethod.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass Activity(dj.Computed):\n\"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        ActivityExtractionMethod (foreign key): Primary key from\n            ActivityExtractionMethod.\n    \"\"\"\n\n    definition = \"\"\"# Neural Activity\n    -&gt; Fluorescence\n    -&gt; ActivityExtractionMethod\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n        Attributes:\n            Activity (foreign key): Primary key from Activity.\n            Fluorescence.Trace (foreign key): Fluorescence.Trace.\n            activity_trace (longblob): Neural activity from fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        activity_trace: longblob\n        \"\"\"\n\n    @property\n    def key_source(self):\n        suite2p_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"suite2p\"'\n            &amp; 'extraction_method LIKE \"suite2p%\"'\n        )\n        caiman_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"caiman\"'\n            &amp; 'extraction_method LIKE \"caiman%\"'\n        )\n        return suite2p_key_source.proj() + caiman_key_source.proj()\n\n    def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n                suite2p_dataset = imaging_dataset\n                # ---- iterate through all s2p plane outputs ----\n                spikes = [\n                    dict(\n                        key,\n                        mask=mask_idx,\n                        fluo_channel=0,\n                        activity_trace=spks,\n                    )\n                    for mask_idx, spks in enumerate(\n                        s\n                        for plane in suite2p_dataset.planes.values()\n                        for s in plane.spks\n                    )\n                ]\n\n                self.insert1(key)\n                self.Trace.insert(spikes)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            if key[\"extraction_method\"] in (\n                \"caiman_deconvolution\",\n                \"caiman_dff\",\n            ):\n                attr_mapper = {\n                    \"caiman_deconvolution\": \"spikes\",\n                    \"caiman_dff\": \"dff\",\n                }\n\n                # infer \"segmentation_channel\" - from params if available, else from caiman loader\n                params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n                segmentation_channel = params.get(\n                    \"segmentation_channel\", caiman_dataset.segmentation_channel\n                )\n\n                self.insert1(key)\n                self.Trace.insert(\n                    dict(\n                        key,\n                        mask=mask[\"mask_id\"],\n                        fluo_channel=segmentation_channel,\n                        activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                    )\n                    for mask in caiman_dataset.masks\n                )\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trace(s) for each mask.</p> <p>Attributes:</p> Name Type Description <code>Activity</code> <code>foreign key</code> <p>Primary key from Activity.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Fluorescence.Trace.</p> <code>activity_trace</code> <code>longblob</code> <p>Neural activity from fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n    Attributes:\n        Activity (foreign key): Primary key from Activity.\n        Fluorescence.Trace (foreign key): Fluorescence.Trace.\n        activity_trace (longblob): Neural activity from fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    activity_trace: longblob\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.Activity.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Activity with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n            suite2p_dataset = imaging_dataset\n            # ---- iterate through all s2p plane outputs ----\n            spikes = [\n                dict(\n                    key,\n                    mask=mask_idx,\n                    fluo_channel=0,\n                    activity_trace=spks,\n                )\n                for mask_idx, spks in enumerate(\n                    s\n                    for plane in suite2p_dataset.planes.values()\n                    for s in plane.spks\n                )\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(spikes)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        if key[\"extraction_method\"] in (\n            \"caiman_deconvolution\",\n            \"caiman_dff\",\n        ):\n            attr_mapper = {\n                \"caiman_deconvolution\": \"spikes\",\n                \"caiman_dff\": \"dff\",\n            }\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            self.insert1(key)\n            self.Trace.insert(\n                dict(\n                    key,\n                    mask=mask[\"mask_id\"],\n                    fluo_channel=segmentation_channel,\n                    activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                )\n                for mask in caiman_dataset.masks\n            )\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingQualityMetrics", "title": "<code>ProcessingQualityMetrics</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Quality metrics used to evaluate the results of the calcium imaging analysis pipeline.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>@schema\nclass ProcessingQualityMetrics(dj.Computed):\n\"\"\"Quality metrics used to evaluate the results of the calcium imaging analysis pipeline.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Fluorescence\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Quality metrics used to evaluate the masks.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            mask_area (float): Mask area in square micrometer.\n            roundness (float): Roundness between 0 and 1. Values closer to 1 are rounder.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        mask_area=null: float  # Mask area in square micrometer.\n        roundness: float       # Roundness between 0 and 1. Values closer to 1 are rounder.\n        \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Quality metrics used to evaluate the fluorescence traces.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Fluorescence.Trace (foreign key): Primary key from Fluorescence.Trace.\n            skewness (float): Skewness of the fluorescence trace.\n            variance (float): Variance of the fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        skewness: float   # Skewness of the fluorescence trace.\n        variance: float   # Variance of the fluorescence trace.\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the ProcessingQualityMetrics table and its part tables.\"\"\"\n        from scipy.stats import skew\n\n        (\n            mask_xpixs,\n            mask_ypixs,\n            mask_weights,\n            fluorescence,\n            fluo_channels,\n            mask_ids,\n            mask_npix,\n            px_height,\n            px_width,\n            um_height,\n            um_width,\n        ) = (Segmentation.Mask * scan.ScanInfo.Field * Fluorescence.Trace &amp; key).fetch(\n            \"mask_xpix\",\n            \"mask_ypix\",\n            \"mask_weights\",\n            \"fluorescence\",\n            \"fluo_channel\",\n            \"mask\",\n            \"mask_npix\",\n            \"px_height\",\n            \"px_width\",\n            \"um_height\",\n            \"um_width\",\n        )\n\n        norm_mean = lambda x: x.mean() / x.max()\n        roundnesses = [\n            norm_mean(np.linalg.eigvals(np.cov(x, y, aweights=w)))\n            for x, y, w in zip(mask_xpixs, mask_ypixs, mask_weights)\n        ]\n\n        fluorescence = np.stack(fluorescence)\n\n        self.insert1(key)\n\n        self.Mask.insert(\n            dict(key, mask=mask_id, mask_area=mask_area, roundness=roundness)\n            for mask_id, mask_area, roundness in zip(\n                mask_ids,\n                mask_npix * (um_height / px_height) * (um_width / px_width),\n                roundnesses,\n            )\n        )\n\n        self.Trace.insert(\n            dict(\n                key,\n                fluo_channel=fluo_channel,\n                mask=mask_id,\n                skewness=skewness,\n                variance=variance,\n            )\n            for fluo_channel, mask_id, skewness, variance in zip(\n                fluo_channels,\n                mask_ids,\n                skew(fluorescence, axis=1),\n                fluorescence.std(axis=1),\n            )\n        )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingQualityMetrics.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Quality metrics used to evaluate the masks.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>mask_area</code> <code>float</code> <p>Mask area in square micrometer.</p> <code>roundness</code> <code>float</code> <p>Roundness between 0 and 1. Values closer to 1 are rounder.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Quality metrics used to evaluate the masks.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        mask_area (float): Mask area in square micrometer.\n        roundness (float): Roundness between 0 and 1. Values closer to 1 are rounder.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    mask_area=null: float  # Mask area in square micrometer.\n    roundness: float       # Roundness between 0 and 1. Values closer to 1 are rounder.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingQualityMetrics.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Quality metrics used to evaluate the fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Primary key from Fluorescence.Trace.</p> <code>skewness</code> <code>float</code> <p>Skewness of the fluorescence trace.</p> <code>variance</code> <code>float</code> <p>Variance of the fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Quality metrics used to evaluate the fluorescence traces.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Fluorescence.Trace (foreign key): Primary key from Fluorescence.Trace.\n        skewness (float): Skewness of the fluorescence trace.\n        variance (float): Variance of the fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    skewness: float   # Skewness of the fluorescence trace.\n    variance: float   # Variance of the fluorescence trace.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.ProcessingQualityMetrics.make", "title": "<code>make(key)</code>", "text": "<p>Populate the ProcessingQualityMetrics table and its part tables.</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the ProcessingQualityMetrics table and its part tables.\"\"\"\n    from scipy.stats import skew\n\n    (\n        mask_xpixs,\n        mask_ypixs,\n        mask_weights,\n        fluorescence,\n        fluo_channels,\n        mask_ids,\n        mask_npix,\n        px_height,\n        px_width,\n        um_height,\n        um_width,\n    ) = (Segmentation.Mask * scan.ScanInfo.Field * Fluorescence.Trace &amp; key).fetch(\n        \"mask_xpix\",\n        \"mask_ypix\",\n        \"mask_weights\",\n        \"fluorescence\",\n        \"fluo_channel\",\n        \"mask\",\n        \"mask_npix\",\n        \"px_height\",\n        \"px_width\",\n        \"um_height\",\n        \"um_width\",\n    )\n\n    norm_mean = lambda x: x.mean() / x.max()\n    roundnesses = [\n        norm_mean(np.linalg.eigvals(np.cov(x, y, aweights=w)))\n        for x, y, w in zip(mask_xpixs, mask_ypixs, mask_weights)\n    ]\n\n    fluorescence = np.stack(fluorescence)\n\n    self.insert1(key)\n\n    self.Mask.insert(\n        dict(key, mask=mask_id, mask_area=mask_area, roundness=roundness)\n        for mask_id, mask_area, roundness in zip(\n            mask_ids,\n            mask_npix * (um_height / px_height) * (um_width / px_width),\n            roundnesses,\n        )\n    )\n\n    self.Trace.insert(\n        dict(\n            key,\n            fluo_channel=fluo_channel,\n            mask=mask_id,\n            skewness=skewness,\n            variance=variance,\n        )\n        for fluo_channel, mask_id, skewness, variance in zip(\n            fluo_channels,\n            mask_ids,\n            skew(fluorescence, axis=1),\n            fluorescence.std(axis=1),\n        )\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_no_curation/#element_calcium_imaging.imaging_no_curation.get_loader_result", "title": "<code>get_loader_result(key, table)</code>", "text": "<p>Retrieve the processed imaging results from a suite2p or caiman loader.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>The <code>key</code> to one entry of ProcessingTask or Curation</p> required <code>table</code> <code>dj.Table</code> <p>A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation)</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the processing_method is different than 'suite2p' or 'caiman'.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,</p> <code>Callable</code> <p>see element-interface for more information on the loaders.)</p> Source code in <code>element_calcium_imaging/imaging_no_curation.py</code> <pre><code>def get_loader_result(key: dict, table: dj.Table) -&gt; Callable:\n\"\"\"Retrieve the processed imaging results from a suite2p or caiman loader.\n\n    Args:\n        key (dict): The `key` to one entry of ProcessingTask or Curation\n        table (dj.Table): A datajoint table to retrieve the loaded results from (e.g.\n            ProcessingTask, Curation)\n\n    Raises:\n        NotImplementedError: If the processing_method is different than 'suite2p' or\n            'caiman'.\n\n    Returns:\n        A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,\n        see element-interface for more information on the loaders.)\n    \"\"\"\n    method, output_dir = (ProcessingParamSet * table &amp; key).fetch1(\n        \"processing_method\", _table_attribute_mapper[table.__name__]\n    )\n\n    output_path = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n    if method == \"suite2p\" or (\n        method == \"extract\" and table.__name__ == \"MotionCorrection\"\n    ):\n        from element_interface import suite2p_loader\n\n        loaded_dataset = suite2p_loader.Suite2p(output_path)\n    elif method == \"caiman\":\n        from element_interface import caiman_loader\n\n        loaded_dataset = caiman_loader.CaImAn(output_path)\n    elif method == \"extract\":\n        from element_interface import extract_loader\n\n        loaded_dataset = extract_loader.EXTRACT(output_path)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n\n    return method, loaded_dataset\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/", "title": "imaging_preprocess.py", "text": ""}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.activate", "title": "<code>activate(imaging_schema_name, scan_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>imaging_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>imaging</code> module.</p> required <code>scan_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>scan</code> module. Omitted, if the <code>scan</code> module is already activated.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>imaging</code> module: + all that are required by the <code>scan</code> module.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <ul> <li>Session: A parent table to Scan, identifying a scanning session.</li> <li>Equipment: A parent table to Scan, identifying a scanning device.</li> </ul> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def activate(\n    imaging_schema_name: str,\n    scan_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        imaging_schema_name (str): Schema name on the database server to activate the\n            `imaging` module.\n        scan_schema_name (str): Schema name on the database server to activate the\n            `scan` module. Omitted, if the `scan` module is already activated.\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `imaging` module: + all that are required by\n            the `scan` module.\n\n    Dependencies:\n    Upstream tables:\n        + Session: A parent table to Scan, identifying a scanning session.\n        + Equipment: A parent table to Scan, identifying a scanning device.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    scan.activate(\n        scan_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        linking_module=linking_module,\n    )\n    schema.activate(\n        imaging_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    imaging_report.activate(f\"{imaging_schema_name}_report\", imaging_schema_name)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Return imaging root data director(y/ies)</p> <p>Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dirs</code> <code>list</code> <p>A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies).</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_imaging_root_data_dir() -&gt; list:\n\"\"\"Return imaging root data director(y/ies)\n\n    Retrieve the root data director(y/ies) containing the imaging data\n    for all subjects/sessions (e.g. acquired ScanImage raw files, output files from\n    processing routines, etc.). All data paths and directories in DataJoint Elements are\n    recommended to be stored as relative paths (posix format), with respect to some\n    user-configured \"root\" directory, which varies from machine to machine\n    (e.g. different mounted drive locations).\n\n    Returns:\n        dirs (list): A list of string(s) or Path(s) for the absolute paths of the imaging root data\n            director(y/ies).\n    \"\"\"\n\n    root_directories = _linking_module.get_imaging_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessMethod", "title": "<code>PreprocessMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Method(s) used for preprocessing of calcium imaging data.</p> <p>Attributes:</p> Name Type Description <code>preprocess_method</code> <code>str</code> <p>Preprocessing method.</p> <code>preprocess_method_desc</code> <code>str</code> <p>Processing method description.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass PreprocessMethod(dj.Lookup):\n\"\"\"Method(s) used for preprocessing of calcium imaging data.\n\n    Attributes:\n        preprocess_method (str): Preprocessing method.\n        preprocess_method_desc (str): Processing method description.\n    \"\"\"\n\n    definition = \"\"\"  #  Method/package used for pre-processing\n    preprocess_method: varchar(16)\n    ---\n    preprocess_method_desc: varchar(1000)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Retrieve the root directory for all processed data.</p> <p>All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dir</code> <code>str | pathlib.Path</code> <p>Absolute path of the processed imaging root data directory.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_processed_root_data_dir() -&gt; Union[str, pathlib.Path]:\n\"\"\"Retrieve the root directory for all processed data.\n\n    All data paths and directories in DataJoint Elements are recommended to be stored as\n    relative paths (posix format), with respect to some user-configured \"root\"\n    directory, which varies from machine to machine (e.g. different mounted drive\n    locations).\n\n    Returns:\n        dir (str| pathlib.Path): Absolute path of the processed imaging root data\n            directory.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_imaging_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSet", "title": "<code>PreprocessParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for the preprocessing of the calcium imaging scans.</p> <p>A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>PreprocessMethod</code> <code>foreign key</code> <p>A primary key from PreprocessMethod.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>param_set_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameter Set, a dictionary of all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass PreprocessParamSet(dj.Lookup):\n\"\"\"Parameter set used for the preprocessing of the calcium imaging scans.\n\n    A hash of the parameters of the analysis suite is also stored in order\n    to avoid duplicated entries.\n\n    Attributes:\n        paramset_idx (int): Unique parameter set ID.\n        PreprocessMethod (foreign key): A primary key from PreprocessMethod.\n        paramset_desc (str): Parameter set description.\n        param_set_hash (uuid): A universally unique identifier for the parameter set.\n        params (longblob): Parameter Set, a dictionary of all applicable parameters to\n            the analysis suite.\n    \"\"\"\n\n    definition = \"\"\"  #  Parameter set used for pre-processing of calcium imaging data\n    paramset_idx:  smallint\n    ---\n    -&gt; PreprocessMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        preprocess_method: str,\n        paramset_idx: int,\n        paramset_desc: str,\n        params: dict,\n    ):\n\"\"\"Insert a parameter set into PreprocessParamSet table.\n        This function automates the parameter set hashing and avoids insertion of an\n            existing parameter set.\n\n        Attributes:\n            preprocess_method (str): Method used for processing of calcium imaging scans.\n            paramset_idx (int): Unique parameter set ID.\n            paramset_desc (str): Parameter set description.\n            params (dict): Parameter Set, all applicable parameters.\n        \"\"\"\n        param_dict = {\n            \"preprocess_method\": preprocess_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if q_param:  # If the specified param-set already exists\n            p_name = q_param.fetch1(\"paramset_idx\")\n            if p_name == paramset_idx:  # If the existed set has the same name: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set already exists - name: {}\".format(p_name)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSet.insert_new_params", "title": "<code>insert_new_params(preprocess_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Insert a parameter set into PreprocessParamSet table. This function automates the parameter set hashing and avoids insertion of an     existing parameter set.</p> <p>Attributes:</p> Name Type Description <code>preprocess_method</code> <code>str</code> <p>Method used for processing of calcium imaging scans.</p> <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>params</code> <code>dict</code> <p>Parameter Set, all applicable parameters.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    preprocess_method: str,\n    paramset_idx: int,\n    paramset_desc: str,\n    params: dict,\n):\n\"\"\"Insert a parameter set into PreprocessParamSet table.\n    This function automates the parameter set hashing and avoids insertion of an\n        existing parameter set.\n\n    Attributes:\n        preprocess_method (str): Method used for processing of calcium imaging scans.\n        paramset_idx (int): Unique parameter set ID.\n        paramset_desc (str): Parameter set description.\n        params (dict): Parameter Set, all applicable parameters.\n    \"\"\"\n    param_dict = {\n        \"preprocess_method\": preprocess_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if q_param:  # If the specified param-set already exists\n        p_name = q_param.fetch1(\"paramset_idx\")\n        if p_name == paramset_idx:  # If the existed set has the same name: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set already exists - name: {}\".format(p_name)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_scan_image_files", "title": "<code>get_scan_image_files(scan_key)</code>", "text": "<p>Retrieve the list of ScanImage files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of ScanImage files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_image_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of ScanImage files associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of ScanImage files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_image_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_scan_box_files", "title": "<code>get_scan_box_files(scan_key)</code>", "text": "<p>Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Scanbox files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_box_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Scanbox files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_box_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_nd2_files", "title": "<code>get_nd2_files(scan_key)</code>", "text": "<p>Retrieve the list of Nikon files (*.nd2) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Nikon files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_nd2_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Nikon files (*.nd2) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Nikon files' full file-paths.\n    \"\"\"\n    return _linking_module.get_nd2_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_prairieview_files", "title": "<code>get_prairieview_files(scan_key)</code>", "text": "<p>Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Bruker PrairieView files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_prairieview_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Bruker PrairieView files' full file-paths.\n    \"\"\"\n    return _linking_module.get_prairieview_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSteps", "title": "<code>PreprocessParamSteps</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Ordered list of paramset_idx that will be run.</p> <p>When pre-processing is not performed, do not create an entry in <code>Step</code> Part table</p> <p>Attributes:</p> Name Type Description <code>preprocess_param_steps_id</code> <code>int</code> <code>preprocess_param_steps_name</code> <code>str</code> <code>preprocess_param_steps_desc</code> <code>str</code> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass PreprocessParamSteps(dj.Manual):\n\"\"\"Ordered list of paramset_idx that will be run.\n\n    When pre-processing is not performed, do not create an entry in `Step` Part table\n\n    Attributes:\n        preprocess_param_steps_id (int):\n        preprocess_param_steps_name (str):\n        preprocess_param_steps_desc (str):\n    \"\"\"\n\n    definition = \"\"\"\n    preprocess_param_steps_id: smallint\n    ---\n    preprocess_param_steps_name: varchar(32)\n    preprocess_param_steps_desc: varchar(128)\n    \"\"\"\n\n    class Step(dj.Part):\n\"\"\"ADD DEFINITION\n\n        Attributes:\n            PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps.\n            step_number (int):\n            PreprocessParamSet (foreign key): A primary key from PreprocessParamSet.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        step_number: smallint                  # Order of operations\n        ---\n        -&gt; PreprocessParamSet\n        \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessParamSteps.Step", "title": "<code>Step</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>ADD DEFINITION</p> <p>Attributes:</p> Name Type Description <code>PreprocessParamSteps</code> <code>foreign key</code> <p>A primary key from PreprocessParamSteps.</p> <code>step_number</code> <code>int</code> <code>PreprocessParamSet</code> <code>foreign key</code> <p>A primary key from PreprocessParamSet.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Step(dj.Part):\n\"\"\"ADD DEFINITION\n\n    Attributes:\n        PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps.\n        step_number (int):\n        PreprocessParamSet (foreign key): A primary key from PreprocessParamSet.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    step_number: smallint                  # Order of operations\n    ---\n    -&gt; PreprocessParamSet\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.PreprocessTask", "title": "<code>PreprocessTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>This table defines a calcium imaging preprocessing task for a combination of a <code>Scan</code> and a <code>PreprocessParamSteps</code> entries, including all the inputs (scan, method, steps). The task defined here is then run in the downstream table Preprocess. This table supports definitions of both loading of pre-generated, results, triggering of new analysis, or skipping of preprocessing step.</p> <p>Attributes:</p> Name Type Description <code>Scan</code> <code>foreign key</code> <p>A primary key from Scan.</p> <code>PreprocessParamSteps</code> <code>foreign key</code> <p>A primary key from PreprocessParamSteps.</p> <code>preprocess_output_dir</code> <code>str</code> <p>Output directory for the results of preprocessing.</p> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results), 'trigger' (trigger computation), 'none' (no pre-processing). Default none.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass PreprocessTask(dj.Manual):\n\"\"\"This table defines a calcium imaging preprocessing task for a combination of a\n    `Scan` and a `PreprocessParamSteps` entries, including all the inputs (scan, method,\n    steps). The task defined here is then run in the downstream table\n    Preprocess. This table supports definitions of both loading of pre-generated,\n    results, triggering of new analysis, or skipping of preprocessing step.\n\n    Attributes:\n        Scan (foreign key): A primary key from Scan.\n        PreprocessParamSteps (foreign key): A primary key from PreprocessParamSteps.\n        preprocess_output_dir (str): Output directory for the results of preprocessing.\n        task_mode (str, optional): One of 'load' (load computed analysis results), 'trigger'\n            (trigger computation), 'none' (no pre-processing). Default none.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a pre-processing task ready to be run\n    -&gt; scan.Scan\n    -&gt; PreprocessParamSteps\n    ---\n    preprocess_output_dir: varchar(255)  # Pre-processing output directory relative\n                                         # to the root data directory\n    task_mode='none': enum('none','load', 'trigger') # 'none': no pre-processing\n                                                     # 'load': load analysis results\n                                                     # 'trigger': trigger computation\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Preprocess", "title": "<code>Preprocess</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Perform the computation of an entry (task) defined in the PreprocessTask table.</p> <ul> <li>If <code>task_mode == \"none\"</code>: no pre-processing performed</li> <li>If <code>task_mode == \"trigger\"</code>: Not implemented</li> <li>If <code>task_mode == \"load\"</code>: Not implemented</li> </ul> <p>Attributes:</p> Name Type Description <code>PreprocessTask</code> <code>foreign key</code> <code>preprocess_time</code> <code>datetime</code> <code>package_version</code> <code>str</code> <p>Version of the analysis package used in processing the data.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Preprocess(dj.Imported):\n\"\"\"Perform the computation of an entry (task) defined in the PreprocessTask table.\n\n    + If `task_mode == \"none\"`: no pre-processing performed\n    + If `task_mode == \"trigger\"`: Not implemented\n    + If `task_mode == \"load\"`: Not implemented\n\n    Attributes:\n        PreprocessTask (foreign key):\n        preprocess_time (datetime, optional):\n        package_version (str, optional): Version of the analysis package used in\n            processing the data.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PreprocessTask\n    ---\n    preprocess_time=null: datetime  # Time of generation of pre-processing results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Execute the preprocessing analysis steps defined in PreprocessTask.\"\"\"\n\n        task_mode, output_dir = (PreprocessTask &amp; key).fetch1(\n            \"task_mode\", \"preprocess_output_dir\"\n        )\n        _ = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n        if task_mode == \"none\":\n            print(f\"No pre-processing run on entry: {key}\")\n        elif task_mode in [\"load\", \"trigger\"]:\n            raise NotImplementedError(\n                \"Pre-processing steps are not implemented.\"\n                \"Please overwrite this `make` function with\"\n                \"desired pre-processing steps.\"\n            )\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Preprocess.make", "title": "<code>make(key)</code>", "text": "<p>Execute the preprocessing analysis steps defined in PreprocessTask.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Execute the preprocessing analysis steps defined in PreprocessTask.\"\"\"\n\n    task_mode, output_dir = (PreprocessTask &amp; key).fetch1(\n        \"task_mode\", \"preprocess_output_dir\"\n    )\n    _ = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n    if task_mode == \"none\":\n        print(f\"No pre-processing run on entry: {key}\")\n    elif task_mode in [\"load\", \"trigger\"]:\n        raise NotImplementedError(\n            \"Pre-processing steps are not implemented.\"\n            \"Please overwrite this `make` function with\"\n            \"desired pre-processing steps.\"\n        )\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingParamSet", "title": "<code>ProcessingParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for the processing of the calcium imaging scans, including both the analysis suite and its respective input parameters.</p> <p>A hash of the parameters of the analysis suite is also stored in order to avoid duplicated entries.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>ProcessingMethod</code> <code>foreign key</code> <p>A primary key from ProcessingMethod.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>param_set_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameter Set, a dictionary of all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass ProcessingParamSet(dj.Lookup):\n\"\"\"Parameter set used for the processing of the calcium imaging scans,\n    including both the analysis suite and its respective input parameters.\n\n    A hash of the parameters of the analysis suite is also stored in order\n    to avoid duplicated entries.\n\n    Attributes:\n        paramset_idx (int): Unique parameter set ID.\n        ProcessingMethod (foreign key): A primary key from ProcessingMethod.\n        paramset_desc (str): Parameter set description.\n        param_set_hash (uuid): A universally unique identifier for the parameter set.\n        params (longblob): Parameter Set, a dictionary of all applicable parameters to\n            the analysis suite.\n    \"\"\"\n\n    definition = \"\"\"# Processing Parameter Set\n    paramset_idx: smallint  # Unique parameter set ID.\n    ---\n    -&gt; ProcessingMethod\n    paramset_desc: varchar(1280)  # Parameter-set description\n    param_set_hash: uuid  # A universally unique identifier for the parameter set\n    unique index (param_set_hash)\n    params: longblob  # Parameter Set, a dictionary of all applicable parameters to the analysis suite.\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        processing_method: str,\n        paramset_idx: int,\n        paramset_desc: str,\n        params: dict,\n    ):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n        This function automates the parameter set hashing and avoids insertion of an\n            existing parameter set.\n\n        Attributes:\n            processing_method (str): Processing method/package used for processing of\n                calcium imaging.\n            paramset_idx (int): Unique parameter set ID.\n            paramset_desc (str): Parameter set description.\n            params (dict): Parameter Set, all applicable parameters to the analysis\n                suite.\n        \"\"\"\n        if processing_method == \"extract\":\n            assert (\n                params.get(\"extract\") is not None and params.get(\"suite2p\") is not None\n            ), ValueError(\n                \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n            )\n\n            # Force Suite2p to only run motion correction.\n            params[\"suite2p\"][\"do_registration\"] = True\n            params[\"suite2p\"][\"roidetect\"] = False\n\n        param_dict = {\n            \"processing_method\": processing_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if q_param:  # If the specified param-set already exists\n            p_name = q_param.fetch1(\"paramset_idx\")\n            if p_name == paramset_idx:  # If the existed set has the same name: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set already exists - name: {}\".format(p_name)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingParamSet.insert_new_params", "title": "<code>insert_new_params(processing_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Insert a parameter set into ProcessingParamSet table.</p> <p>This function automates the parameter set hashing and avoids insertion of an     existing parameter set.</p> <p>Attributes:</p> Name Type Description <code>processing_method</code> <code>str</code> <p>Processing method/package used for processing of calcium imaging.</p> <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>paramset_desc</code> <code>str</code> <p>Parameter set description.</p> <code>params</code> <code>dict</code> <p>Parameter Set, all applicable parameters to the analysis suite.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    processing_method: str,\n    paramset_idx: int,\n    paramset_desc: str,\n    params: dict,\n):\n\"\"\"Insert a parameter set into ProcessingParamSet table.\n\n    This function automates the parameter set hashing and avoids insertion of an\n        existing parameter set.\n\n    Attributes:\n        processing_method (str): Processing method/package used for processing of\n            calcium imaging.\n        paramset_idx (int): Unique parameter set ID.\n        paramset_desc (str): Parameter set description.\n        params (dict): Parameter Set, all applicable parameters to the analysis\n            suite.\n    \"\"\"\n    if processing_method == \"extract\":\n        assert (\n            params.get(\"extract\") is not None and params.get(\"suite2p\") is not None\n        ), ValueError(\n            \"Please provide the processing parameters in the {'suite2p': {...}, 'extract': {...}} dictionary format.\"\n        )\n\n        # Force Suite2p to only run motion correction.\n        params[\"suite2p\"][\"do_registration\"] = True\n        params[\"suite2p\"][\"roidetect\"] = False\n\n    param_dict = {\n        \"processing_method\": processing_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    q_param = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if q_param:  # If the specified param-set already exists\n        p_name = q_param.fetch1(\"paramset_idx\")\n        if p_name == paramset_idx:  # If the existed set has the same name: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set already exists - name: {}\".format(p_name)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.CellCompartment", "title": "<code>CellCompartment</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)</p> <p>Attributes:</p> Name Type Description <code>cell_compartment</code> <code>str</code> <p>Cell compartment.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass CellCompartment(dj.Lookup):\n\"\"\"Cell compartments that can be imaged (e.g. 'axon', 'soma', etc.)\n\n    Attributes:\n        cell_compartment (str): Cell compartment.\n    \"\"\"\n\n    definition = \"\"\"# Cell compartments\n    cell_compartment: char(16)\n    \"\"\"\n\n    contents = zip([\"axon\", \"soma\", \"bouton\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').</p> <p>Attributes:</p> Name Type Description <code>mask_type</code> <code>str</code> <p>Mask type.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass MaskType(dj.Lookup):\n\"\"\"Available labels for segmented masks (e.g. 'soma', 'axon', 'dendrite', 'neuropil').\n\n    Attributes:\n        mask_type (str): Mask type.\n    \"\"\"\n\n    definition = \"\"\"# Possible types of a segmented mask\n    mask_type: varchar(16)\n    \"\"\"\n\n    contents = zip([\"soma\", \"axon\", \"dendrite\", \"neuropil\", \"artefact\", \"unknown\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask", "title": "<code>ProcessingTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A pairing of processing params and scans to be loaded or triggered</p> <p>This table defines a calcium imaging processing task for a combination of a <code>Scan</code> and a <code>ProcessingParamSet</code> entries, including all the inputs (scan, method, method's parameters). The task defined here is then run in the downstream table Processing. This table supports definitions of both loading of pre-generated results and the triggering of new analysis for all supported analysis methods</p> <p>Attributes:</p> Name Type Description <code>Preprocess</code> <code>foreign key</code> <p>Primary key from Preprocess.</p> <code>ProcessingParamSet</code> <code>foreign key</code> <p>Primary key from ProcessingParamSet.</p> <code>processing_output_dir</code> <code>str</code> <p>Output directory of the processed scan relative to the root data directory.</p> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results) or 'trigger' (trigger computation).</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass ProcessingTask(dj.Manual):\n\"\"\"A pairing of processing params and scans to be loaded or triggered\n\n    This table defines a calcium imaging processing task for a combination of a\n    `Scan` and a `ProcessingParamSet` entries, including all the inputs (scan, method,\n    method's parameters). The task defined here is then run in the downstream table\n    Processing. This table supports definitions of both loading of pre-generated results\n    and the triggering of new analysis for all supported analysis methods\n\n    Attributes:\n        Preprocess (foreign key): Primary key from Preprocess.\n        ProcessingParamSet (foreign key): Primary key from ProcessingParamSet.\n        processing_output_dir (str): Output directory of the processed scan relative to the root data directory.\n        task_mode (str): One of 'load' (load computed analysis results) or 'trigger'\n            (trigger computation).\n    \"\"\"\n\n    definition = \"\"\"# Manual table for defining a processing task ready to be run\n    -&gt; Preprocess\n    -&gt; ProcessingParamSet\n    ---\n    processing_output_dir: varchar(255)  #  Output directory of the processed scan relative to root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n        Args:\n            key (dict): Primary key from the ProcessingTask table.\n            relative (bool): If True, processing_output_dir is returned relative to\n                imaging_root_dir. Default False.\n            mkdir (bool): If True, create the processing_output_dir directory.\n                Default True.\n\n        Returns:\n            dir (str): A default output directory for the processed results (processed_output_dir\n                in ProcessingTask) based on the following convention:\n                processed_dir / scan_dir / {processing_method}_{paramset_idx}\n                e.g.: sub4/sess1/scan0/suite2p_0\n        \"\"\"\n        image_locators = {\n            \"NIS\": get_nd2_files,\n            \"ScanImage\": get_scan_image_files,\n            \"Scanbox\": get_scan_box_files,\n            \"PrairieView\": get_prairieview_files,\n        }\n        image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n        scan_dir = find_full_path(\n            get_imaging_root_data_dir(), image_locator(key)[0]\n        ).parent\n        root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n        method = (\n            (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n        )\n\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        output_dir = (\n            processed_dir\n            / scan_dir.relative_to(root_dir)\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n        Generate an entry in the ProcessingTask table for a particular scan using an\n        existing parameter set from the ProcessingParamSet table.\n\n        Args:\n            scan_key (dict): Primary key from Scan table.\n            paramset_idx (int): Unique parameter set ID.\n        \"\"\"\n        key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n        method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n            \"processing_method\"\n        )\n\n        try:\n            if method == \"suite2p\":\n                from element_interface import suite2p_loader\n\n                suite2p_loader.Suite2p(output_dir)\n            elif method == \"caiman\":\n                from element_interface import caiman_loader\n\n                caiman_loader.CaImAn(output_dir)\n            elif method == \"extract\":\n                from element_interface import extract_loader\n\n                extract_loader.EXTRACT(output_dir)\n\n            else:\n                raise NotImplementedError(\n                    \"Unknown/unimplemented method: {}\".format(method)\n                )\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"processing_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n\n    auto_generate_entries = generate\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer an output directory for an entry in ProcessingTask table.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from the ProcessingTask table.</p> required <code>relative</code> <code>bool</code> <p>If True, processing_output_dir is returned relative to imaging_root_dir. Default False.</p> <code>False</code> <code>mkdir</code> <code>bool</code> <p>If True, create the processing_output_dir directory. Default True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dir</code> <code>str</code> <p>A default output directory for the processed results (processed_output_dir in ProcessingTask) based on the following convention: processed_dir / scan_dir / {processing_method}_{paramset_idx} e.g.: sub4/sess1/scan0/suite2p_0</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer an output directory for an entry in ProcessingTask table.\n\n    Args:\n        key (dict): Primary key from the ProcessingTask table.\n        relative (bool): If True, processing_output_dir is returned relative to\n            imaging_root_dir. Default False.\n        mkdir (bool): If True, create the processing_output_dir directory.\n            Default True.\n\n    Returns:\n        dir (str): A default output directory for the processed results (processed_output_dir\n            in ProcessingTask) based on the following convention:\n            processed_dir / scan_dir / {processing_method}_{paramset_idx}\n            e.g.: sub4/sess1/scan0/suite2p_0\n    \"\"\"\n    image_locators = {\n        \"NIS\": get_nd2_files,\n        \"ScanImage\": get_scan_image_files,\n        \"Scanbox\": get_scan_box_files,\n        \"PrairieView\": get_prairieview_files,\n    }\n    image_locator = image_locators[(scan.Scan &amp; key).fetch1(\"acq_software\")]\n\n    scan_dir = find_full_path(\n        get_imaging_root_data_dir(), image_locator(key)[0]\n    ).parent\n    root_dir = find_root_directory(get_imaging_root_data_dir(), scan_dir)\n\n    method = (\n        (ProcessingParamSet &amp; key).fetch1(\"processing_method\").replace(\".\", \"-\")\n    )\n\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    output_dir = (\n        processed_dir\n        / scan_dir.relative_to(root_dir)\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingTask.generate", "title": "<code>generate(scan_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet</p> <p>Generate an entry in the ProcessingTask table for a particular scan using an existing parameter set from the ProcessingParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan table.</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID.</p> <code>0</code> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@classmethod\ndef generate(cls, scan_key, paramset_idx=0):\n\"\"\"Generate a ProcessingTask for a Scan using an parameter ProcessingParamSet\n\n    Generate an entry in the ProcessingTask table for a particular scan using an\n    existing parameter set from the ProcessingParamSet table.\n\n    Args:\n        scan_key (dict): Primary key from Scan table.\n        paramset_idx (int): Unique parameter set ID.\n    \"\"\"\n    key = {**scan_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = cls.infer_output_dir(key, relative=False, mkdir=True)\n\n    method = (ProcessingParamSet &amp; {\"paramset_idx\": paramset_idx}).fetch1(\n        \"processing_method\"\n    )\n\n    try:\n        if method == \"suite2p\":\n            from element_interface import suite2p_loader\n\n            suite2p_loader.Suite2p(output_dir)\n        elif method == \"caiman\":\n            from element_interface import caiman_loader\n\n            caiman_loader.CaImAn(output_dir)\n        elif method == \"extract\":\n            from element_interface import extract_loader\n\n            extract_loader.EXTRACT(output_dir)\n\n        else:\n            raise NotImplementedError(\n                \"Unknown/unimplemented method: {}\".format(method)\n            )\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"processing_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing", "title": "<code>Processing</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Perform the computation of an entry (task) defined in the ProcessingTask table. The computation is performed only on the scans with ScanInfo inserted.</p> <p>Attributes:</p> Name Type Description <code>ProcessingTask</code> <code>foreign key</code> <p>Primary key from ProcessingTask.</p> <code>processing_time</code> <code>datetime</code> <p>Process completion datetime.</p> <code>package_version</code> <code>str</code> <p>Version of the analysis package used in processing the data.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Processing(dj.Computed):\n\"\"\"Perform the computation of an entry (task) defined in the ProcessingTask table.\n    The computation is performed only on the scans with ScanInfo inserted.\n\n    Attributes:\n        ProcessingTask (foreign key): Primary key from ProcessingTask.\n        processing_time (datetime): Process completion datetime.\n        package_version (str, optional): Version of the analysis package used in\n            processing the data.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ProcessingTask\n    ---\n    processing_time     : datetime  # Time of generation of this set of processed, segmented results\n    package_version=''  : varchar(16)\n    \"\"\"\n\n    # Run processing only on Scan with ScanInfo inserted\n    @property\n    def key_source(self):\n\"\"\"Limit the Processing to Scans that have their metadata ingested to the\n        database.\"\"\"\n\n        return ProcessingTask &amp; scan.ScanInfo\n\n    def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n        task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n            \"task_mode\", \"processing_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update processing_output_dir\n            ProcessingTask.update1(\n                {**key, \"processing_output_dir\": output_dir.as_posix()}\n            )\n        output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n        if task_mode == \"load\":\n            method, imaging_dataset = get_loader_result(key, ProcessingTask)\n            if method == \"suite2p\":\n                if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                    raise NotImplementedError(\n                        \"Suite2p ingestion error - Unable to handle\"\n                        + \" ScanImage multi-ROI scanning mode yet\"\n                    )\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n            elif method == \"caiman\":\n                caiman_dataset = imaging_dataset\n                key = {**key, \"processing_time\": caiman_dataset.creation_time}\n            elif method == \"extract\":\n                raise NotImplementedError(\n                    \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n                )\n            else:\n                raise NotImplementedError(\"Unknown method: {}\".format(method))\n        elif task_mode == \"trigger\":\n\n            method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n                \"processing_method\"\n            )\n\n            preprocess_paramsets = (\n                PreprocessParamSteps.Step()\n                &amp; dict(preprocess_param_steps_id=key[\"preprocess_param_steps_id\"])\n            ).fetch(\"paramset_idx\")\n\n            if len(preprocess_paramsets) == 0:\n                # No pre-processing steps were performed on the acquired dataset, so process the raw/acquired files.\n                image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n                image_files = [\n                    find_full_path(get_imaging_root_data_dir(), image_file)\n                    for image_file in image_files\n                ]\n\n            else:\n                preprocess_output_dir = (PreprocessTask &amp; key).fetch1(\n                    \"preprocess_output_dir\"\n                )\n\n                preprocess_output_dir = find_full_path(\n                    get_imaging_root_data_dir(), preprocess_output_dir\n                )\n\n                if not preprocess_output_dir.exists():\n                    raise FileNotFoundError(\n                        f\"Pre-processed output directory not found ({preprocess_output_dir})\"\n                    )\n\n                image_files = list(preprocess_output_dir.glob(\"*.tif\"))\n\n            if method == \"suite2p\":\n                import suite2p\n\n                suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                suite2p_params[\"save_path0\"] = output_dir\n                (\n                    suite2p_params[\"fs\"],\n                    suite2p_params[\"nplanes\"],\n                    suite2p_params[\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                suite2p_params[\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                suite2p_dataset = imaging_dataset\n                key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n            elif method == \"caiman\":\n                from element_interface.caiman_loader import _process_scanimage_tiff\n                from element_interface.run_caiman import run_caiman\n\n                caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                    \"params\"\n                )\n                sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                    \"fps\", \"ndepths\", \"nchannels\"\n                )\n\n                is3D = bool(ndepths &gt; 1)\n                if is3D:\n                    raise NotImplementedError(\n                        \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                    )\n\n                # handle multi-channel tiff image before running CaImAn\n                if nchannels &gt; 1:\n                    channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                    tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                    tmp_dir.mkdir(exist_ok=True)\n                    _process_scanimage_tiff(\n                        [f.as_posix() for f in image_files], output_dir=tmp_dir\n                    )\n                    image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n                run_caiman(\n                    file_paths=[f.as_posix() for f in image_files],\n                    parameters=caiman_params,\n                    sampling_rate=sampling_rate,\n                    output_dir=output_dir,\n                    is3D=is3D,\n                )\n\n                _, imaging_dataset = get_loader_result(key, ProcessingTask)\n                caiman_dataset = imaging_dataset\n                key[\"processing_time\"] = caiman_dataset.creation_time\n\n            elif method == \"extract\":\n                import suite2p\n                from element_interface.extract_trigger import EXTRACT_trigger\n                from scipy.io import savemat\n\n                # Motion Correction with Suite2p\n                params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n                params[\"suite2p\"][\"save_path0\"] = output_dir\n                (\n                    params[\"suite2p\"][\"fs\"],\n                    params[\"suite2p\"][\"nplanes\"],\n                    params[\"suite2p\"][\"nchannels\"],\n                ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n                input_format = pathlib.Path(image_files[0]).suffix\n                params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n                suite2p_paths = {\n                    \"data_path\": [image_files[0].parent.as_posix()],\n                    \"tiff_list\": [f.as_posix() for f in image_files],\n                }\n\n                suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n                # Convert data.bin to registered_scans.mat\n                scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n                data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                    \"nframes\", \"px_height\", \"px_width\"\n                )\n                data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n                scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n                # Save the motion corrected movie (data.bin) in a .mat file\n                savemat(\n                    scan_matlab_fullpath,\n                    {\"M\": np.transpose(data, axes=[1, 2, 0])},\n                )\n\n                # Execute EXTRACT\n\n                ex = EXTRACT_trigger(\n                    scan_matlab_fullpath, params[\"extract\"], output_dir\n                )\n                ex.run()\n\n                _, extract_dataset = get_loader_result(key, ProcessingTask)\n                key[\"processing_time\"] = extract_dataset.creation_time\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the Processing to Scans that have their metadata ingested to the database.</p>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Processing.make", "title": "<code>make(key)</code>", "text": "<p>Execute the calcium imaging analysis defined by the ProcessingTask.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Execute the calcium imaging analysis defined by the ProcessingTask.\"\"\"\n\n    task_mode, output_dir = (ProcessingTask &amp; key).fetch1(\n        \"task_mode\", \"processing_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ProcessingTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update processing_output_dir\n        ProcessingTask.update1(\n            {**key, \"processing_output_dir\": output_dir.as_posix()}\n        )\n    output_dir = find_full_path(get_imaging_root_data_dir(), output_dir).as_posix()\n\n    if task_mode == \"load\":\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n        if method == \"suite2p\":\n            if (scan.ScanInfo &amp; key).fetch1(\"nrois\") &gt; 0:\n                raise NotImplementedError(\n                    \"Suite2p ingestion error - Unable to handle\"\n                    + \" ScanImage multi-ROI scanning mode yet\"\n                )\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            key = {**key, \"processing_time\": caiman_dataset.creation_time}\n        elif method == \"extract\":\n            raise NotImplementedError(\n                \"To use EXTRACT with this DataJoint Element please set `task_mode=trigger`\"\n            )\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n    elif task_mode == \"trigger\":\n\n        method = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\n            \"processing_method\"\n        )\n\n        preprocess_paramsets = (\n            PreprocessParamSteps.Step()\n            &amp; dict(preprocess_param_steps_id=key[\"preprocess_param_steps_id\"])\n        ).fetch(\"paramset_idx\")\n\n        if len(preprocess_paramsets) == 0:\n            # No pre-processing steps were performed on the acquired dataset, so process the raw/acquired files.\n            image_files = (scan.ScanInfo.ScanFile &amp; key).fetch(\"file_path\")\n            image_files = [\n                find_full_path(get_imaging_root_data_dir(), image_file)\n                for image_file in image_files\n            ]\n\n        else:\n            preprocess_output_dir = (PreprocessTask &amp; key).fetch1(\n                \"preprocess_output_dir\"\n            )\n\n            preprocess_output_dir = find_full_path(\n                get_imaging_root_data_dir(), preprocess_output_dir\n            )\n\n            if not preprocess_output_dir.exists():\n                raise FileNotFoundError(\n                    f\"Pre-processed output directory not found ({preprocess_output_dir})\"\n                )\n\n            image_files = list(preprocess_output_dir.glob(\"*.tif\"))\n\n        if method == \"suite2p\":\n            import suite2p\n\n            suite2p_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            suite2p_params[\"save_path0\"] = output_dir\n            (\n                suite2p_params[\"fs\"],\n                suite2p_params[\"nplanes\"],\n                suite2p_params[\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            suite2p_params[\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=suite2p_params, db=suite2p_paths)  # Run suite2p\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            suite2p_dataset = imaging_dataset\n            key = {**key, \"processing_time\": suite2p_dataset.creation_time}\n\n        elif method == \"caiman\":\n            from element_interface.caiman_loader import _process_scanimage_tiff\n            from element_interface.run_caiman import run_caiman\n\n            caiman_params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\n                \"params\"\n            )\n            sampling_rate, ndepths, nchannels = (scan.ScanInfo &amp; key).fetch1(\n                \"fps\", \"ndepths\", \"nchannels\"\n            )\n\n            is3D = bool(ndepths &gt; 1)\n            if is3D:\n                raise NotImplementedError(\n                    \"Caiman pipeline is not yet capable of analyzing 3D scans.\"\n                )\n\n            # handle multi-channel tiff image before running CaImAn\n            if nchannels &gt; 1:\n                channel_idx = caiman_params.get(\"channel_to_process\", 0)\n                tmp_dir = pathlib.Path(output_dir) / \"channel_separated_tif\"\n                tmp_dir.mkdir(exist_ok=True)\n                _process_scanimage_tiff(\n                    [f.as_posix() for f in image_files], output_dir=tmp_dir\n                )\n                image_files = tmp_dir.glob(f\"*_chn{channel_idx}.tif\")\n\n            run_caiman(\n                file_paths=[f.as_posix() for f in image_files],\n                parameters=caiman_params,\n                sampling_rate=sampling_rate,\n                output_dir=output_dir,\n                is3D=is3D,\n            )\n\n            _, imaging_dataset = get_loader_result(key, ProcessingTask)\n            caiman_dataset = imaging_dataset\n            key[\"processing_time\"] = caiman_dataset.creation_time\n\n        elif method == \"extract\":\n            import suite2p\n            from element_interface.extract_trigger import EXTRACT_trigger\n            from scipy.io import savemat\n\n            # Motion Correction with Suite2p\n            params = (ProcessingTask * ProcessingParamSet &amp; key).fetch1(\"params\")\n\n            params[\"suite2p\"][\"save_path0\"] = output_dir\n            (\n                params[\"suite2p\"][\"fs\"],\n                params[\"suite2p\"][\"nplanes\"],\n                params[\"suite2p\"][\"nchannels\"],\n            ) = (scan.ScanInfo &amp; key).fetch1(\"fps\", \"ndepths\", \"nchannels\")\n\n            input_format = pathlib.Path(image_files[0]).suffix\n            params[\"suite2p\"][\"input_format\"] = input_format[1:]\n\n            suite2p_paths = {\n                \"data_path\": [image_files[0].parent.as_posix()],\n                \"tiff_list\": [f.as_posix() for f in image_files],\n            }\n\n            suite2p.run_s2p(ops=params[\"suite2p\"], db=suite2p_paths)\n\n            # Convert data.bin to registered_scans.mat\n            scanfile_fullpath = pathlib.Path(output_dir) / \"suite2p/plane0/data.bin\"\n\n            data_shape = (scan.ScanInfo * scan.ScanInfo.Field &amp; key).fetch1(\n                \"nframes\", \"px_height\", \"px_width\"\n            )\n            data = np.memmap(scanfile_fullpath, shape=data_shape, dtype=np.int16)\n\n            scan_matlab_fullpath = scanfile_fullpath.parent / \"registered_scan.mat\"\n\n            # Save the motion corrected movie (data.bin) in a .mat file\n            savemat(\n                scan_matlab_fullpath,\n                {\"M\": np.transpose(data, axes=[1, 2, 0])},\n            )\n\n            # Execute EXTRACT\n\n            ex = EXTRACT_trigger(\n                scan_matlab_fullpath, params[\"extract\"], output_dir\n            )\n            ex.run()\n\n            _, extract_dataset = get_loader_result(key, ProcessingTask)\n            key[\"processing_time\"] = extract_dataset.creation_time\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Curated results</p> <p>If no curation is applied, the curation_output_dir can be set to the value of processing_output_dir.</p> <p>Attributes:</p> Name Type Description <code>Processing</code> <code>foreign key</code> <p>Primary key from Processing.</p> <code>curation_id</code> <code>int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time of generation of this set of curated results.</p> <code>curation_output_dir</code> <code>str</code> <p>Output directory of the curated results, relative to root data directory.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this result.</p> <code>curation_note</code> <code>str</code> <p>Notes about the curation task.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curated results\n\n    If no curation is applied, the curation_output_dir can be set to\n    the value of processing_output_dir.\n\n    Attributes:\n        Processing (foreign key): Primary key from Processing.\n        curation_id (int): Unique curation ID.\n        curation_time (datetime): Time of generation of this set of curated results.\n        curation_output_dir (str): Output directory of the curated results, relative to\n            root data directory.\n        manual_curation (bool): If True, manual curation has been performed on this\n            result.\n        curation_note (str, optional): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"# Curation(s) results\n    -&gt; Processing\n    curation_id: int\n    ---\n    curation_time: datetime  # Time of generation of this set of curated results\n    curation_output_dir: varchar(255)  # Output directory of the curated results, relative to root data directory\n    manual_curation: bool  # Has manual curation been performed on this result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_processing_task(self, key, is_curated=False, curation_note=\"\"):\n\"\"\"Create a Curation entry for a given ProcessingTask key.\n\n        Args:\n            key (dict): Primary key set of an entry in the ProcessingTask table.\n            is_curated (bool): When True, indicates a manual curation.\n            curation_note (str): User's note on the specifics of the curation.\n        \"\"\"\n        if key not in Processing():\n            raise ValueError(\n                f\"No corresponding entry in Processing available for: {key};\"\n                f\"Please run `Processing.populate(key)`\"\n            )\n\n        output_dir = (ProcessingTask &amp; key).fetch1(\"processing_output_dir\")\n        method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n            curation_time = suite2p_dataset.creation_time\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n            curation_time = caiman_dataset.creation_time\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            curation_time = extract_dataset.creation_time\n        else:\n            raise NotImplementedError(\"Unknown method: {}\".format(method))\n\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": curation_time,\n                \"curation_output_dir\": output_dir,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Curation.create1_from_processing_task", "title": "<code>create1_from_processing_task(key, is_curated=False, curation_note='')</code>", "text": "<p>Create a Curation entry for a given ProcessingTask key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key set of an entry in the ProcessingTask table.</p> required <code>is_curated</code> <code>bool</code> <p>When True, indicates a manual curation.</p> <code>False</code> <code>curation_note</code> <code>str</code> <p>User's note on the specifics of the curation.</p> <code>''</code> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def create1_from_processing_task(self, key, is_curated=False, curation_note=\"\"):\n\"\"\"Create a Curation entry for a given ProcessingTask key.\n\n    Args:\n        key (dict): Primary key set of an entry in the ProcessingTask table.\n        is_curated (bool): When True, indicates a manual curation.\n        curation_note (str): User's note on the specifics of the curation.\n    \"\"\"\n    if key not in Processing():\n        raise ValueError(\n            f\"No corresponding entry in Processing available for: {key};\"\n            f\"Please run `Processing.populate(key)`\"\n        )\n\n    output_dir = (ProcessingTask &amp; key).fetch1(\"processing_output_dir\")\n    method, imaging_dataset = get_loader_result(key, ProcessingTask)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n        curation_time = suite2p_dataset.creation_time\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n        curation_time = caiman_dataset.creation_time\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        curation_time = extract_dataset.creation_time\n    else:\n        raise NotImplementedError(\"Unknown method: {}\".format(method))\n\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": curation_time,\n            \"curation_output_dir\": output_dir,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection", "title": "<code>MotionCorrection</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Results of motion correction shifts performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Primary key from Curation.</p> <code>scan.Channel.proj(motion_correct_channel='channel')</code> <code>int</code> <p>Channel used for motion correction in this processing task.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass MotionCorrection(dj.Imported):\n\"\"\"Results of motion correction shifts performed on the imaging data.\n\n    Attributes:\n        Curation (foreign key): Primary key from Curation.\n        scan.Channel.proj(motion_correct_channel='channel') (int): Channel used for\n            motion correction in this processing task.\n    \"\"\"\n\n    definition = \"\"\"# Results of motion correction\n    -&gt; Curation\n    ---\n    -&gt; scan.Channel.proj(motion_correct_channel='channel') # channel used for motion correction in this processing task\n    \"\"\"\n\n    class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob): Mask with true for frames with outlier shifts\n                (already corrected).\n            y_shifts (longblob): y motion correction shifts (pixels).\n            x_shifts (longblob): x motion correction shifts (pixels).\n            z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n            y_std (float): standard deviation of y shifts across all frames (pixels).\n            x_std (float): standard deviation of x shifts across all frames (pixels).\n            z_std (float, optional): standard deviation of z shifts across all frames\n                (pixels).\n        \"\"\"\n\n        definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n        y_shifts            : longblob  # (pixels) y motion correction shifts\n        x_shifts            : longblob  # (pixels) x motion correction shifts\n        z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift)\n        y_std               : float     # (pixels) standard deviation of y shifts across all frames\n        x_std               : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n        blocks/patches.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            outlier_frames (longblob, null): Mask with true for frames with outlier\n                shifts (already corrected).\n            block_height (int): Block height in pixels.\n            block_width (int): Block width in pixels.\n            block_depth (int): Block depth in pixels.\n            block_count_y (int): Number of blocks tiled in the y direction.\n            block_count_x (int): Number of blocks tiled in the x direction.\n            block_count_z (int): Number of blocks tiled in the z direction.\n        \"\"\"\n\n        definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n        -&gt; master\n        ---\n        outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n        block_height        : int      # (pixels)\n        block_width         : int      # (pixels)\n        block_depth         : int      # (pixels)\n        block_count_y       : int      # number of blocks tiled in the y direction\n        block_count_x       : int      # number of blocks tiled in the x direction\n        block_count_z       : int      # number of blocks tiled in the z direction\n        \"\"\"\n\n    class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n        Attributes:\n            NonRigidMotionCorrection (foreign key): Primary key from\n                NonRigidMotionCorrection.\n            block_id (int): Unique block ID.\n            block_y (longblob): y_start and y_end in pixels for this block\n            block_x (longblob): x_start and x_end in pixels for this block\n            block_z (longblob): z_start and z_end in pixels for this block\n            y_shifts (longblob): y motion correction shifts for every frame in pixels\n            x_shifts (longblob): x motion correction shifts for every frame in pixels\n            z_shift=null (longblob, optional): x motion correction shifts for every frame\n                in pixels\n            y_std (float): standard deviation of y shifts across all frames in pixels\n            x_std (float): standard deviation of x shifts across all frames in pixels\n            z_std=null (float, optional): standard deviation of z shifts across all frames\n                in pixels\n        \"\"\"\n\n        definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n        -&gt; master.NonRigidMotionCorrection\n        block_id        : int\n        ---\n        block_y         : longblob  # (y_start, y_end) in pixel of this block\n        block_x         : longblob  # (x_start, x_end) in pixel of this block\n        block_z         : longblob  # (z_start, z_end) in pixel of this block\n        y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n        x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n        z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n        y_std           : float     # (pixels) standard deviation of y shifts across all frames\n        x_std           : float     # (pixels) standard deviation of x shifts across all frames\n        z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n        \"\"\"\n\n    class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n        Attributes:\n            MotionCorrection (foreign key): Primary key from MotionCorrection.\n            scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n            ref_image (longblob): Image used as alignment template.\n            average_image (longblob): Mean of registered frames.\n            correlation_image (longblob, optional): Correlation map (computed during\n                cell detection).\n            max_proj_image (longblob, optional): Max of registered frames.\n        \"\"\"\n\n        definition = \"\"\"# Summary images for each field and channel after corrections\n        -&gt; master\n        -&gt; scan.ScanInfo.Field\n        ---\n        ref_image               : longblob  # image used as alignment template\n        average_image           : longblob  # mean of registered frames\n        correlation_image=null  : longblob  # correlation map (computed during cell detection)\n        max_proj_image=null     : longblob  # max of registered frames\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n            \"KEY\", \"field_z\", order_by=\"field_z\"\n        )\n\n        if method in [\"suite2p\", \"extract\"]:\n            suite2p_dataset = imaging_dataset\n\n            motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n            # ---- iterate through all s2p plane outputs ----\n            rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n            summary_images = []\n            for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n                # -- rigid motion correction --\n                if idx == 0:\n                    rigid_correction = {\n                        **key,\n                        \"y_shifts\": s2p.ops[\"yoff\"],\n                        \"x_shifts\": s2p.ops[\"xoff\"],\n                        \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                        \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                        \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                        \"z_std\": np.nan,\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    rigid_correction[\"y_shifts\"] = np.vstack(\n                        [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                    )\n                    rigid_correction[\"y_std\"] = np.nanstd(\n                        rigid_correction[\"y_shifts\"].flatten()\n                    )\n                    rigid_correction[\"x_shifts\"] = np.vstack(\n                        [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                    )\n                    rigid_correction[\"x_std\"] = np.nanstd(\n                        rigid_correction[\"x_shifts\"].flatten()\n                    )\n                    rigid_correction[\"outlier_frames\"] = np.logical_or(\n                        rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                    )\n                # -- non-rigid motion correction --\n                if s2p.ops[\"nonrigid\"]:\n                    if idx == 0:\n                        nonrigid_correction = {\n                            **key,\n                            \"block_height\": s2p.ops[\"block_size\"][0],\n                            \"block_width\": s2p.ops[\"block_size\"][1],\n                            \"block_depth\": 1,\n                            \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                            \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                            \"block_count_z\": len(suite2p_dataset.planes),\n                            \"outlier_frames\": s2p.ops[\"badframes\"],\n                        }\n                    else:\n                        nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                            nonrigid_correction[\"outlier_frames\"],\n                            s2p.ops[\"badframes\"],\n                        )\n                    for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                        zip(\n                            s2p.ops[\"xblock\"],\n                            s2p.ops[\"yblock\"],\n                            s2p.ops[\"yoff1\"].T,\n                            s2p.ops[\"xoff1\"].T,\n                        )\n                    ):\n                        if b_id in nonrigid_blocks:\n                            nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                            )\n                            nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                            )\n                            nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                                [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                            )\n                            nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                                nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                            )\n                        else:\n                            nonrigid_blocks[b_id] = {\n                                **key,\n                                \"block_id\": b_id,\n                                \"block_y\": b_y,\n                                \"block_x\": b_x,\n                                \"block_z\": np.full_like(b_x, plane),\n                                \"y_shifts\": bshift_y,\n                                \"x_shifts\": bshift_x,\n                                \"z_shifts\": np.full(\n                                    (\n                                        len(suite2p_dataset.planes),\n                                        len(bshift_x),\n                                    ),\n                                    0,\n                                ),\n                                \"y_std\": np.nanstd(bshift_y),\n                                \"x_std\": np.nanstd(bshift_x),\n                                \"z_std\": np.nan,\n                            }\n\n                # -- summary images --\n                motion_correction_key = (\n                    scan.ScanInfo.Field * Curation &amp; key &amp; field_keys[plane]\n                ).fetch1(\"KEY\")\n                summary_images.append(\n                    {\n                        **motion_correction_key,\n                        \"ref_image\": s2p.ref_image,\n                        \"average_image\": s2p.mean_image,\n                        \"correlation_image\": s2p.correlation_map,\n                        \"max_proj_image\": s2p.max_proj_image,\n                    }\n                )\n\n            self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n            if rigid_correction:\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            if nonrigid_correction:\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks.values())\n            self.Summary.insert(summary_images)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            self.insert1(\n                {\n                    **key,\n                    \"motion_correct_channel\": caiman_dataset.alignment_channel,\n                }\n            )\n\n            is3D = caiman_dataset.params.motion[\"is3D\"]\n            if not caiman_dataset.params.motion[\"pw_rigid\"]:\n                # -- rigid motion correction --\n                rigid_correction = {\n                    **key,\n                    \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                    \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                    \"z_shifts\": (\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                        if is3D\n                        else np.full_like(\n                            caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                            0,\n                        )\n                    ),\n                    \"x_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                    ),\n                    \"y_std\": np.nanstd(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                    ),\n                    \"z_std\": (\n                        np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                        if is3D\n                        else np.nan\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                self.RigidMotionCorrection.insert1(rigid_correction)\n            else:\n                # -- non-rigid motion correction --\n                nonrigid_correction = {\n                    **key,\n                    \"block_height\": (\n                        caiman_dataset.params.motion[\"strides\"][0]\n                        + caiman_dataset.params.motion[\"overlaps\"][0]\n                    ),\n                    \"block_width\": (\n                        caiman_dataset.params.motion[\"strides\"][1]\n                        + caiman_dataset.params.motion[\"overlaps\"][1]\n                    ),\n                    \"block_depth\": (\n                        caiman_dataset.params.motion[\"strides\"][2]\n                        + caiman_dataset.params.motion[\"overlaps\"][2]\n                        if is3D\n                        else 1\n                    ),\n                    \"block_count_x\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                    ),\n                    \"block_count_y\": len(\n                        set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                    ),\n                    \"block_count_z\": (\n                        len(\n                            set(\n                                caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    :, 4\n                                ]\n                            )\n                        )\n                        if is3D\n                        else 1\n                    ),\n                    \"outlier_frames\": None,\n                }\n\n                nonrigid_blocks = []\n                for b_id in range(\n                    len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n                ):\n                    nonrigid_blocks.append(\n                        {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_x\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 0:2\n                                ]\n                            ),\n                            \"block_y\": np.arange(\n                                *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                    b_id, 2:4\n                                ]\n                            ),\n                            \"block_z\": (\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 4:6]\n                                )\n                                if is3D\n                                else np.full_like(\n                                    np.arange(\n                                        *caiman_dataset.motion_correction[\n                                            \"coord_shifts_els\"\n                                        ][b_id, 0:2]\n                                    ),\n                                    0,\n                                )\n                            ),\n                            \"x_shifts\": caiman_dataset.motion_correction[\n                                \"x_shifts_els\"\n                            ][:, b_id],\n                            \"y_shifts\": caiman_dataset.motion_correction[\n                                \"y_shifts_els\"\n                            ][:, b_id],\n                            \"z_shifts\": (\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                                if is3D\n                                else np.full_like(\n                                    caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                        :, b_id\n                                    ],\n                                    0,\n                                )\n                            ),\n                            \"x_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"y_std\": np.nanstd(\n                                caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                    :, b_id\n                                ]\n                            ),\n                            \"z_std\": (\n                                np.nanstd(\n                                    caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                        :, b_id\n                                    ]\n                                )\n                                if is3D\n                                else np.nan\n                            ),\n                        }\n                    )\n\n                self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n                self.Block.insert(nonrigid_blocks)\n\n            # -- summary images --\n            summary_images = [\n                {\n                    **key,\n                    **fkey,\n                    \"ref_image\": ref_image,\n                    \"average_image\": ave_img,\n                    \"correlation_image\": corr_img,\n                    \"max_proj_image\": max_img,\n                }\n                for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                    field_keys,\n                    caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"reference_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"average_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                        2, 0, 1\n                    )\n                    if is3D\n                    else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                        np.newaxis, ...\n                    ],\n                    caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                    if is3D\n                    else caiman_dataset.motion_correction[\"max_image\"][...][\n                        np.newaxis, ...\n                    ],\n                )\n            ]\n            self.Summary.insert(summary_images)\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.RigidMotionCorrection", "title": "<code>RigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of rigid motion correction performed on the imaging data.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts (pixels).</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts (pixels).</p> <code>z_shifts</code> <code>longblob</code> <p>z motion correction shifts (z-drift, pixels).</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames (pixels).</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames (pixels).</p> <code>z_std</code> <code>float</code> <p>standard deviation of z shifts across all frames (pixels).</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class RigidMotionCorrection(dj.Part):\n\"\"\"Details of rigid motion correction performed on the imaging data.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob): Mask with true for frames with outlier shifts\n            (already corrected).\n        y_shifts (longblob): y motion correction shifts (pixels).\n        x_shifts (longblob): x motion correction shifts (pixels).\n        z_shifts (longblob, optional): z motion correction shifts (z-drift, pixels).\n        y_std (float): standard deviation of y shifts across all frames (pixels).\n        x_std (float): standard deviation of x shifts across all frames (pixels).\n        z_std (float, optional): standard deviation of z shifts across all frames\n            (pixels).\n    \"\"\"\n\n    definition = \"\"\"# Details of rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob  # mask with true for frames with outlier shifts (already corrected)\n    y_shifts            : longblob  # (pixels) y motion correction shifts\n    x_shifts            : longblob  # (pixels) x motion correction shifts\n    z_shifts=null       : longblob  # (pixels) z motion correction shifts (z-drift)\n    y_std               : float     # (pixels) standard deviation of y shifts across all frames\n    x_std               : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null          : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.NonRigidMotionCorrection", "title": "<code>NonRigidMotionCorrection</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Piece-wise rigid motion correction - tile the FOV into multiple 3D blocks/patches.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>outlier_frames</code> <code>longblob, null</code> <p>Mask with true for frames with outlier shifts (already corrected).</p> <code>block_height</code> <code>int</code> <p>Block height in pixels.</p> <code>block_width</code> <code>int</code> <p>Block width in pixels.</p> <code>block_depth</code> <code>int</code> <p>Block depth in pixels.</p> <code>block_count_y</code> <code>int</code> <p>Number of blocks tiled in the y direction.</p> <code>block_count_x</code> <code>int</code> <p>Number of blocks tiled in the x direction.</p> <code>block_count_z</code> <code>int</code> <p>Number of blocks tiled in the z direction.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class NonRigidMotionCorrection(dj.Part):\n\"\"\"Piece-wise rigid motion correction - tile the FOV into multiple 3D\n    blocks/patches.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        outlier_frames (longblob, null): Mask with true for frames with outlier\n            shifts (already corrected).\n        block_height (int): Block height in pixels.\n        block_width (int): Block width in pixels.\n        block_depth (int): Block depth in pixels.\n        block_count_y (int): Number of blocks tiled in the y direction.\n        block_count_x (int): Number of blocks tiled in the x direction.\n        block_count_z (int): Number of blocks tiled in the z direction.\n    \"\"\"\n\n    definition = \"\"\"# Details of non-rigid motion correction performed on the imaging data\n    -&gt; master\n    ---\n    outlier_frames=null : longblob # mask with true for frames with outlier shifts (already corrected)\n    block_height        : int      # (pixels)\n    block_width         : int      # (pixels)\n    block_depth         : int      # (pixels)\n    block_count_y       : int      # number of blocks tiled in the y direction\n    block_count_x       : int      # number of blocks tiled in the x direction\n    block_count_z       : int      # number of blocks tiled in the z direction\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.Block", "title": "<code>Block</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>FOV-tiled blocks used for non-rigid motion correction.</p> <p>Attributes:</p> Name Type Description <code>NonRigidMotionCorrection</code> <code>foreign key</code> <p>Primary key from NonRigidMotionCorrection.</p> <code>block_id</code> <code>int</code> <p>Unique block ID.</p> <code>block_y</code> <code>longblob</code> <p>y_start and y_end in pixels for this block</p> <code>block_x</code> <code>longblob</code> <p>x_start and x_end in pixels for this block</p> <code>block_z</code> <code>longblob</code> <p>z_start and z_end in pixels for this block</p> <code>y_shifts</code> <code>longblob</code> <p>y motion correction shifts for every frame in pixels</p> <code>x_shifts</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>z_shift=null</code> <code>longblob</code> <p>x motion correction shifts for every frame in pixels</p> <code>y_std</code> <code>float</code> <p>standard deviation of y shifts across all frames in pixels</p> <code>x_std</code> <code>float</code> <p>standard deviation of x shifts across all frames in pixels</p> <code>z_std=null</code> <code>float</code> <p>standard deviation of z shifts across all frames in pixels</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Block(dj.Part):\n\"\"\"FOV-tiled blocks used for non-rigid motion correction.\n\n    Attributes:\n        NonRigidMotionCorrection (foreign key): Primary key from\n            NonRigidMotionCorrection.\n        block_id (int): Unique block ID.\n        block_y (longblob): y_start and y_end in pixels for this block\n        block_x (longblob): x_start and x_end in pixels for this block\n        block_z (longblob): z_start and z_end in pixels for this block\n        y_shifts (longblob): y motion correction shifts for every frame in pixels\n        x_shifts (longblob): x motion correction shifts for every frame in pixels\n        z_shift=null (longblob, optional): x motion correction shifts for every frame\n            in pixels\n        y_std (float): standard deviation of y shifts across all frames in pixels\n        x_std (float): standard deviation of x shifts across all frames in pixels\n        z_std=null (float, optional): standard deviation of z shifts across all frames\n            in pixels\n    \"\"\"\n\n    definition = \"\"\"# FOV-tiled blocks used for non-rigid motion correction\n    -&gt; master.NonRigidMotionCorrection\n    block_id        : int\n    ---\n    block_y         : longblob  # (y_start, y_end) in pixel of this block\n    block_x         : longblob  # (x_start, x_end) in pixel of this block\n    block_z         : longblob  # (z_start, z_end) in pixel of this block\n    y_shifts        : longblob  # (pixels) y motion correction shifts for every frame\n    x_shifts        : longblob  # (pixels) x motion correction shifts for every frame\n    z_shifts=null   : longblob  # (pixels) x motion correction shifts for every frame\n    y_std           : float     # (pixels) standard deviation of y shifts across all frames\n    x_std           : float     # (pixels) standard deviation of x shifts across all frames\n    z_std=null      : float     # (pixels) standard deviation of z shifts across all frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.Summary", "title": "<code>Summary</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Summary images for each field and channel after corrections.</p> <p>Attributes:</p> Name Type Description <code>MotionCorrection</code> <code>foreign key</code> <p>Primary key from MotionCorrection.</p> <code>scan.ScanInfo.Field</code> <code>foreign key</code> <p>Primary key from scan.ScanInfo.Field.</p> <code>ref_image</code> <code>longblob</code> <p>Image used as alignment template.</p> <code>average_image</code> <code>longblob</code> <p>Mean of registered frames.</p> <code>correlation_image</code> <code>longblob</code> <p>Correlation map (computed during cell detection).</p> <code>max_proj_image</code> <code>longblob</code> <p>Max of registered frames.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Summary(dj.Part):\n\"\"\"Summary images for each field and channel after corrections.\n\n    Attributes:\n        MotionCorrection (foreign key): Primary key from MotionCorrection.\n        scan.ScanInfo.Field (foreign key): Primary key from scan.ScanInfo.Field.\n        ref_image (longblob): Image used as alignment template.\n        average_image (longblob): Mean of registered frames.\n        correlation_image (longblob, optional): Correlation map (computed during\n            cell detection).\n        max_proj_image (longblob, optional): Max of registered frames.\n    \"\"\"\n\n    definition = \"\"\"# Summary images for each field and channel after corrections\n    -&gt; master\n    -&gt; scan.ScanInfo.Field\n    ---\n    ref_image               : longblob  # image used as alignment template\n    average_image           : longblob  # mean of registered frames\n    correlation_image=null  : longblob  # correlation map (computed during cell detection)\n    max_proj_image=null     : longblob  # max of registered frames\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MotionCorrection.make", "title": "<code>make(key)</code>", "text": "<p>Populate MotionCorrection with results parsed from analysis outputs</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate MotionCorrection with results parsed from analysis outputs\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    field_keys, _ = (scan.ScanInfo.Field &amp; key).fetch(\n        \"KEY\", \"field_z\", order_by=\"field_z\"\n    )\n\n    if method in [\"suite2p\", \"extract\"]:\n        suite2p_dataset = imaging_dataset\n\n        motion_correct_channel = suite2p_dataset.planes[0].alignment_channel\n\n        # ---- iterate through all s2p plane outputs ----\n        rigid_correction, nonrigid_correction, nonrigid_blocks = {}, {}, {}\n        summary_images = []\n        for idx, (plane, s2p) in enumerate(suite2p_dataset.planes.items()):\n            # -- rigid motion correction --\n            if idx == 0:\n                rigid_correction = {\n                    **key,\n                    \"y_shifts\": s2p.ops[\"yoff\"],\n                    \"x_shifts\": s2p.ops[\"xoff\"],\n                    \"z_shifts\": np.full_like(s2p.ops[\"xoff\"], 0),\n                    \"y_std\": np.nanstd(s2p.ops[\"yoff\"]),\n                    \"x_std\": np.nanstd(s2p.ops[\"xoff\"]),\n                    \"z_std\": np.nan,\n                    \"outlier_frames\": s2p.ops[\"badframes\"],\n                }\n            else:\n                rigid_correction[\"y_shifts\"] = np.vstack(\n                    [rigid_correction[\"y_shifts\"], s2p.ops[\"yoff\"]]\n                )\n                rigid_correction[\"y_std\"] = np.nanstd(\n                    rigid_correction[\"y_shifts\"].flatten()\n                )\n                rigid_correction[\"x_shifts\"] = np.vstack(\n                    [rigid_correction[\"x_shifts\"], s2p.ops[\"xoff\"]]\n                )\n                rigid_correction[\"x_std\"] = np.nanstd(\n                    rigid_correction[\"x_shifts\"].flatten()\n                )\n                rigid_correction[\"outlier_frames\"] = np.logical_or(\n                    rigid_correction[\"outlier_frames\"], s2p.ops[\"badframes\"]\n                )\n            # -- non-rigid motion correction --\n            if s2p.ops[\"nonrigid\"]:\n                if idx == 0:\n                    nonrigid_correction = {\n                        **key,\n                        \"block_height\": s2p.ops[\"block_size\"][0],\n                        \"block_width\": s2p.ops[\"block_size\"][1],\n                        \"block_depth\": 1,\n                        \"block_count_y\": s2p.ops[\"nblocks\"][0],\n                        \"block_count_x\": s2p.ops[\"nblocks\"][1],\n                        \"block_count_z\": len(suite2p_dataset.planes),\n                        \"outlier_frames\": s2p.ops[\"badframes\"],\n                    }\n                else:\n                    nonrigid_correction[\"outlier_frames\"] = np.logical_or(\n                        nonrigid_correction[\"outlier_frames\"],\n                        s2p.ops[\"badframes\"],\n                    )\n                for b_id, (b_y, b_x, bshift_y, bshift_x) in enumerate(\n                    zip(\n                        s2p.ops[\"xblock\"],\n                        s2p.ops[\"yblock\"],\n                        s2p.ops[\"yoff1\"].T,\n                        s2p.ops[\"xoff1\"].T,\n                    )\n                ):\n                    if b_id in nonrigid_blocks:\n                        nonrigid_blocks[b_id][\"y_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"y_shifts\"], bshift_y]\n                        )\n                        nonrigid_blocks[b_id][\"y_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"y_shifts\"].flatten()\n                        )\n                        nonrigid_blocks[b_id][\"x_shifts\"] = np.vstack(\n                            [nonrigid_blocks[b_id][\"x_shifts\"], bshift_x]\n                        )\n                        nonrigid_blocks[b_id][\"x_std\"] = np.nanstd(\n                            nonrigid_blocks[b_id][\"x_shifts\"].flatten()\n                        )\n                    else:\n                        nonrigid_blocks[b_id] = {\n                            **key,\n                            \"block_id\": b_id,\n                            \"block_y\": b_y,\n                            \"block_x\": b_x,\n                            \"block_z\": np.full_like(b_x, plane),\n                            \"y_shifts\": bshift_y,\n                            \"x_shifts\": bshift_x,\n                            \"z_shifts\": np.full(\n                                (\n                                    len(suite2p_dataset.planes),\n                                    len(bshift_x),\n                                ),\n                                0,\n                            ),\n                            \"y_std\": np.nanstd(bshift_y),\n                            \"x_std\": np.nanstd(bshift_x),\n                            \"z_std\": np.nan,\n                        }\n\n            # -- summary images --\n            motion_correction_key = (\n                scan.ScanInfo.Field * Curation &amp; key &amp; field_keys[plane]\n            ).fetch1(\"KEY\")\n            summary_images.append(\n                {\n                    **motion_correction_key,\n                    \"ref_image\": s2p.ref_image,\n                    \"average_image\": s2p.mean_image,\n                    \"correlation_image\": s2p.correlation_map,\n                    \"max_proj_image\": s2p.max_proj_image,\n                }\n            )\n\n        self.insert1({**key, \"motion_correct_channel\": motion_correct_channel})\n        if rigid_correction:\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        if nonrigid_correction:\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks.values())\n        self.Summary.insert(summary_images)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        self.insert1(\n            {\n                **key,\n                \"motion_correct_channel\": caiman_dataset.alignment_channel,\n            }\n        )\n\n        is3D = caiman_dataset.params.motion[\"is3D\"]\n        if not caiman_dataset.params.motion[\"pw_rigid\"]:\n            # -- rigid motion correction --\n            rigid_correction = {\n                **key,\n                \"x_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                \"y_shifts\": caiman_dataset.motion_correction[\"shifts_rig\"][:, 1],\n                \"z_shifts\": (\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 2]\n                    if is3D\n                    else np.full_like(\n                        caiman_dataset.motion_correction[\"shifts_rig\"][:, 0],\n                        0,\n                    )\n                ),\n                \"x_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 0]\n                ),\n                \"y_std\": np.nanstd(\n                    caiman_dataset.motion_correction[\"shifts_rig\"][:, 1]\n                ),\n                \"z_std\": (\n                    np.nanstd(caiman_dataset.motion_correction[\"shifts_rig\"][:, 2])\n                    if is3D\n                    else np.nan\n                ),\n                \"outlier_frames\": None,\n            }\n\n            self.RigidMotionCorrection.insert1(rigid_correction)\n        else:\n            # -- non-rigid motion correction --\n            nonrigid_correction = {\n                **key,\n                \"block_height\": (\n                    caiman_dataset.params.motion[\"strides\"][0]\n                    + caiman_dataset.params.motion[\"overlaps\"][0]\n                ),\n                \"block_width\": (\n                    caiman_dataset.params.motion[\"strides\"][1]\n                    + caiman_dataset.params.motion[\"overlaps\"][1]\n                ),\n                \"block_depth\": (\n                    caiman_dataset.params.motion[\"strides\"][2]\n                    + caiman_dataset.params.motion[\"overlaps\"][2]\n                    if is3D\n                    else 1\n                ),\n                \"block_count_x\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 0])\n                ),\n                \"block_count_y\": len(\n                    set(caiman_dataset.motion_correction[\"coord_shifts_els\"][:, 2])\n                ),\n                \"block_count_z\": (\n                    len(\n                        set(\n                            caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                :, 4\n                            ]\n                        )\n                    )\n                    if is3D\n                    else 1\n                ),\n                \"outlier_frames\": None,\n            }\n\n            nonrigid_blocks = []\n            for b_id in range(\n                len(caiman_dataset.motion_correction[\"x_shifts_els\"][0, :])\n            ):\n                nonrigid_blocks.append(\n                    {\n                        **key,\n                        \"block_id\": b_id,\n                        \"block_x\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 0:2\n                            ]\n                        ),\n                        \"block_y\": np.arange(\n                            *caiman_dataset.motion_correction[\"coord_shifts_els\"][\n                                b_id, 2:4\n                            ]\n                        ),\n                        \"block_z\": (\n                            np.arange(\n                                *caiman_dataset.motion_correction[\n                                    \"coord_shifts_els\"\n                                ][b_id, 4:6]\n                            )\n                            if is3D\n                            else np.full_like(\n                                np.arange(\n                                    *caiman_dataset.motion_correction[\n                                        \"coord_shifts_els\"\n                                    ][b_id, 0:2]\n                                ),\n                                0,\n                            )\n                        ),\n                        \"x_shifts\": caiman_dataset.motion_correction[\n                            \"x_shifts_els\"\n                        ][:, b_id],\n                        \"y_shifts\": caiman_dataset.motion_correction[\n                            \"y_shifts_els\"\n                        ][:, b_id],\n                        \"z_shifts\": (\n                            caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                :, b_id\n                            ]\n                            if is3D\n                            else np.full_like(\n                                caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                    :, b_id\n                                ],\n                                0,\n                            )\n                        ),\n                        \"x_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"x_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"y_std\": np.nanstd(\n                            caiman_dataset.motion_correction[\"y_shifts_els\"][\n                                :, b_id\n                            ]\n                        ),\n                        \"z_std\": (\n                            np.nanstd(\n                                caiman_dataset.motion_correction[\"z_shifts_els\"][\n                                    :, b_id\n                                ]\n                            )\n                            if is3D\n                            else np.nan\n                        ),\n                    }\n                )\n\n            self.NonRigidMotionCorrection.insert1(nonrigid_correction)\n            self.Block.insert(nonrigid_blocks)\n\n        # -- summary images --\n        summary_images = [\n            {\n                **key,\n                **fkey,\n                \"ref_image\": ref_image,\n                \"average_image\": ave_img,\n                \"correlation_image\": corr_img,\n                \"max_proj_image\": max_img,\n            }\n            for fkey, ref_image, ave_img, corr_img, max_img in zip(\n                field_keys,\n                caiman_dataset.motion_correction[\"reference_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"reference_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"average_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"average_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"correlation_image\"].transpose(\n                    2, 0, 1\n                )\n                if is3D\n                else caiman_dataset.motion_correction[\"correlation_image\"][...][\n                    np.newaxis, ...\n                ],\n                caiman_dataset.motion_correction[\"max_image\"].transpose(2, 0, 1)\n                if is3D\n                else caiman_dataset.motion_correction[\"max_image\"][...][\n                    np.newaxis, ...\n                ],\n            )\n        ]\n        self.Summary.insert(summary_images)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation", "title": "<code>Segmentation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Result of the Segmentation process.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Primary key from Curation.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Segmentation(dj.Computed):\n\"\"\"Result of the Segmentation process.\n\n    Attributes:\n        Curation (foreign key): Primary key from Curation.\n    \"\"\"\n\n    definition = \"\"\"# Different mask segmentations.\n    -&gt; Curation\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n        Attributes:\n            Segmentation (foreign key): Primary key from Segmentation.\n            mask (int): Unique mask ID.\n            scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n                used for segmentation.\n            mask_npix (int): Number of pixels in ROIs.\n            mask_center_x (int): Center x coordinate in pixel.\n            mask_center_y (int): Center y coordinate in pixel.\n            mask_center_z (int): Center z coordinate in pixel.\n            mask_xpix (longblob): X coordinates in pixels.\n            mask_ypix (longblob): Y coordinates in pixels.\n            mask_zpix (longblob): Z coordinates in pixels.\n            mask_weights (longblob): Weights of the mask at the indices above.\n        \"\"\"\n\n        definition = \"\"\" # A mask produced by segmentation.\n        -&gt; master\n        mask               : smallint\n        ---\n        -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n        mask_npix          : int       # number of pixels in ROIs\n        mask_center_x      : int       # center x coordinate in pixel\n        mask_center_y      : int       # center y coordinate in pixel\n        mask_center_z=null : int       # center z coordinate in pixel\n        mask_xpix          : longblob  # x coordinates in pixels\n        mask_ypix          : longblob  # y coordinates in pixels\n        mask_zpix=null     : longblob  # z coordinates in pixels\n        mask_weights       : longblob  # weights of the mask at the indices above\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            masks, cells = [], []\n            for plane, s2p in suite2p_dataset.planes.items():\n                mask_count = len(masks)  # increment mask id from all \"plane\"\n                for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                    zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n                ):\n                    masks.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"segmentation_channel\": s2p.segmentation_channel,\n                            \"mask_npix\": mask_stat[\"npix\"],\n                            \"mask_center_x\": mask_stat[\"med\"][1],\n                            \"mask_center_y\": mask_stat[\"med\"][0],\n                            \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                            \"mask_xpix\": mask_stat[\"xpix\"],\n                            \"mask_ypix\": mask_stat[\"ypix\"],\n                            \"mask_zpix\": np.full(\n                                mask_stat[\"npix\"],\n                                mask_stat.get(\"iplane\", plane),\n                            ),\n                            \"mask_weights\": mask_stat[\"lam\"],\n                        }\n                    )\n                    if is_cell:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"suite2p_default_classifier\",\n                                \"mask\": mask_idx + mask_count,\n                                \"mask_type\": \"soma\",\n                                \"confidence\": cell_prob,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"suite2p_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            masks, cells = [], []\n            for mask in caiman_dataset.masks:\n                masks.append(\n                    {\n                        **key,\n                        \"segmentation_channel\": segmentation_channel,\n                        \"mask\": mask[\"mask_id\"],\n                        \"mask_npix\": mask[\"mask_npix\"],\n                        \"mask_center_x\": mask[\"mask_center_x\"],\n                        \"mask_center_y\": mask[\"mask_center_y\"],\n                        \"mask_center_z\": mask[\"mask_center_z\"],\n                        \"mask_xpix\": mask[\"mask_xpix\"],\n                        \"mask_ypix\": mask[\"mask_ypix\"],\n                        \"mask_zpix\": mask[\"mask_zpix\"],\n                        \"mask_weights\": mask[\"mask_weights\"],\n                    }\n                )\n                if caiman_dataset.cnmf.estimates.idx_components is not None:\n                    if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                        cells.append(\n                            {\n                                **key,\n                                \"mask_classification_method\": \"caiman_default_classifier\",\n                                \"mask\": mask[\"mask_id\"],\n                                \"mask_type\": \"soma\",\n                            }\n                        )\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n\n            if cells:\n                MaskClassification.insert1(\n                    {\n                        **key,\n                        \"mask_classification_method\": \"caiman_default_classifier\",\n                    },\n                    allow_direct_insert=True,\n                )\n                MaskClassification.MaskType.insert(\n                    cells, ignore_extra_fields=True, allow_direct_insert=True\n                )\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n            masks = [\n                dict(\n                    **key,\n                    segmentation_channel=0,\n                    mask=mask[\"mask_id\"],\n                    mask_npix=mask[\"mask_npix\"],\n                    mask_center_x=mask[\"mask_center_x\"],\n                    mask_center_y=mask[\"mask_center_y\"],\n                    mask_center_z=mask[\"mask_center_z\"],\n                    mask_xpix=mask[\"mask_xpix\"],\n                    mask_ypix=mask[\"mask_ypix\"],\n                    mask_zpix=mask[\"mask_zpix\"],\n                    mask_weights=mask[\"mask_weights\"],\n                )\n                for mask in extract_dataset.load_results()\n            ]\n\n            self.insert1(key)\n            self.Mask.insert(masks, ignore_extra_fields=True)\n        else:\n            raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of the masks identified from the Segmentation procedure.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>mask</code> <code>int</code> <p>Unique mask ID.</p> <code>scan.Channel.proj(segmentation_channel='channel')</code> <code>foreign key</code> <p>Channel used for segmentation.</p> <code>mask_npix</code> <code>int</code> <p>Number of pixels in ROIs.</p> <code>mask_center_x</code> <code>int</code> <p>Center x coordinate in pixel.</p> <code>mask_center_y</code> <code>int</code> <p>Center y coordinate in pixel.</p> <code>mask_center_z</code> <code>int</code> <p>Center z coordinate in pixel.</p> <code>mask_xpix</code> <code>longblob</code> <p>X coordinates in pixels.</p> <code>mask_ypix</code> <code>longblob</code> <p>Y coordinates in pixels.</p> <code>mask_zpix</code> <code>longblob</code> <p>Z coordinates in pixels.</p> <code>mask_weights</code> <code>longblob</code> <p>Weights of the mask at the indices above.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Details of the masks identified from the Segmentation procedure.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        mask (int): Unique mask ID.\n        scan.Channel.proj(segmentation_channel='channel') (foreign key): Channel\n            used for segmentation.\n        mask_npix (int): Number of pixels in ROIs.\n        mask_center_x (int): Center x coordinate in pixel.\n        mask_center_y (int): Center y coordinate in pixel.\n        mask_center_z (int): Center z coordinate in pixel.\n        mask_xpix (longblob): X coordinates in pixels.\n        mask_ypix (longblob): Y coordinates in pixels.\n        mask_zpix (longblob): Z coordinates in pixels.\n        mask_weights (longblob): Weights of the mask at the indices above.\n    \"\"\"\n\n    definition = \"\"\" # A mask produced by segmentation.\n    -&gt; master\n    mask               : smallint\n    ---\n    -&gt; scan.Channel.proj(segmentation_channel='channel')  # channel used for segmentation\n    mask_npix          : int       # number of pixels in ROIs\n    mask_center_x      : int       # center x coordinate in pixel\n    mask_center_y      : int       # center y coordinate in pixel\n    mask_center_z=null : int       # center z coordinate in pixel\n    mask_xpix          : longblob  # x coordinates in pixels\n    mask_ypix          : longblob  # y coordinates in pixels\n    mask_zpix=null     : longblob  # z coordinates in pixels\n    mask_weights       : longblob  # weights of the mask at the indices above\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Segmentation.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Segmentation with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Segmentation with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        masks, cells = [], []\n        for plane, s2p in suite2p_dataset.planes.items():\n            mask_count = len(masks)  # increment mask id from all \"plane\"\n            for mask_idx, (is_cell, cell_prob, mask_stat) in enumerate(\n                zip(s2p.iscell, s2p.cell_prob, s2p.stat)\n            ):\n                masks.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"segmentation_channel\": s2p.segmentation_channel,\n                        \"mask_npix\": mask_stat[\"npix\"],\n                        \"mask_center_x\": mask_stat[\"med\"][1],\n                        \"mask_center_y\": mask_stat[\"med\"][0],\n                        \"mask_center_z\": mask_stat.get(\"iplane\", plane),\n                        \"mask_xpix\": mask_stat[\"xpix\"],\n                        \"mask_ypix\": mask_stat[\"ypix\"],\n                        \"mask_zpix\": np.full(\n                            mask_stat[\"npix\"],\n                            mask_stat.get(\"iplane\", plane),\n                        ),\n                        \"mask_weights\": mask_stat[\"lam\"],\n                    }\n                )\n                if is_cell:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"suite2p_default_classifier\",\n                            \"mask\": mask_idx + mask_count,\n                            \"mask_type\": \"soma\",\n                            \"confidence\": cell_prob,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"suite2p_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        masks, cells = [], []\n        for mask in caiman_dataset.masks:\n            masks.append(\n                {\n                    **key,\n                    \"segmentation_channel\": segmentation_channel,\n                    \"mask\": mask[\"mask_id\"],\n                    \"mask_npix\": mask[\"mask_npix\"],\n                    \"mask_center_x\": mask[\"mask_center_x\"],\n                    \"mask_center_y\": mask[\"mask_center_y\"],\n                    \"mask_center_z\": mask[\"mask_center_z\"],\n                    \"mask_xpix\": mask[\"mask_xpix\"],\n                    \"mask_ypix\": mask[\"mask_ypix\"],\n                    \"mask_zpix\": mask[\"mask_zpix\"],\n                    \"mask_weights\": mask[\"mask_weights\"],\n                }\n            )\n            if caiman_dataset.cnmf.estimates.idx_components is not None:\n                if mask[\"mask_id\"] in caiman_dataset.cnmf.estimates.idx_components:\n                    cells.append(\n                        {\n                            **key,\n                            \"mask_classification_method\": \"caiman_default_classifier\",\n                            \"mask\": mask[\"mask_id\"],\n                            \"mask_type\": \"soma\",\n                        }\n                    )\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n\n        if cells:\n            MaskClassification.insert1(\n                {\n                    **key,\n                    \"mask_classification_method\": \"caiman_default_classifier\",\n                },\n                allow_direct_insert=True,\n            )\n            MaskClassification.MaskType.insert(\n                cells, ignore_extra_fields=True, allow_direct_insert=True\n            )\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n        masks = [\n            dict(\n                **key,\n                segmentation_channel=0,\n                mask=mask[\"mask_id\"],\n                mask_npix=mask[\"mask_npix\"],\n                mask_center_x=mask[\"mask_center_x\"],\n                mask_center_y=mask[\"mask_center_y\"],\n                mask_center_z=mask[\"mask_center_z\"],\n                mask_xpix=mask[\"mask_xpix\"],\n                mask_ypix=mask[\"mask_ypix\"],\n                mask_zpix=mask[\"mask_zpix\"],\n                mask_weights=mask[\"mask_weights\"],\n            )\n            for mask in extract_dataset.load_results()\n        ]\n\n        self.insert1(key)\n        self.Mask.insert(masks, ignore_extra_fields=True)\n    else:\n        raise NotImplementedError(f\"Unknown/unimplemented method: {method}\")\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassificationMethod", "title": "<code>MaskClassificationMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available mask classification methods.</p> <p>Attributes:</p> Name Type Description <code>mask_classification_method</code> <code>str</code> <p>Mask classification method.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass MaskClassificationMethod(dj.Lookup):\n\"\"\"Available mask classification methods.\n\n    Attributes:\n        mask_classification_method (str): Mask classification method.\n    \"\"\"\n\n    definition = \"\"\"\n    mask_classification_method: varchar(48)\n    \"\"\"\n\n    contents = zip([\"suite2p_default_classifier\", \"caiman_default_classifier\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassification", "title": "<code>MaskClassification</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Classes assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> <code>MaskClassificationMethod</code> <code>foreign key</code> <p>Primary key from MaskClassificationMethod.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass MaskClassification(dj.Computed):\n\"\"\"Classes assigned to each mask.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n        MaskClassificationMethod (foreign key): Primary key from\n            MaskClassificationMethod.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Segmentation\n    -&gt; MaskClassificationMethod\n    \"\"\"\n\n    class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n        Attributes:\n            MaskClassification (foreign key): Primary key from MaskClassification.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            MaskType: Primary key from MaskType.\n            confidence (float, optional): Confidence level of the mask classification.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        -&gt; MaskType\n        confidence=null: float\n        \"\"\"\n\n    def make(self, key):\n        pass\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.MaskClassification.MaskType", "title": "<code>MaskType</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Type assigned to each mask.</p> <p>Attributes:</p> Name Type Description <code>MaskClassification</code> <code>foreign key</code> <p>Primary key from MaskClassification.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>MaskType</code> <code>foreign key</code> <p>Primary key from MaskType.</p> <code>confidence</code> <code>float</code> <p>Confidence level of the mask classification.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class MaskType(dj.Part):\n\"\"\"Type assigned to each mask.\n\n    Attributes:\n        MaskClassification (foreign key): Primary key from MaskClassification.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        MaskType: Primary key from MaskType.\n        confidence (float, optional): Confidence level of the mask classification.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    -&gt; MaskType\n    confidence=null: float\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence", "title": "<code>Fluorescence</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from Segmentation.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Fluorescence(dj.Computed):\n\"\"\"Fluorescence traces.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from Segmentation.\n    \"\"\"\n\n    definition = \"\"\"# Fluorescence traces before spike extraction or filtering\n    -&gt; Segmentation\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n                comes from.\n            fluorescence (longblob): Fluorescence trace associated with this mask.\n            neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from\n        ---\n        fluorescence                : longblob  # Fluorescence trace associated with this mask\n        neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            suite2p_dataset = imaging_dataset\n\n            # ---- iterate through all s2p plane outputs ----\n            fluo_traces, fluo_chn2_traces = [], []\n            for s2p in suite2p_dataset.planes.values():\n                mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n                for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                    fluo_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_count,\n                            \"fluo_channel\": 0,\n                            \"fluorescence\": f,\n                            \"neuropil_fluorescence\": fneu,\n                        }\n                    )\n                if len(s2p.F_chan2):\n                    mask_chn2_count = len(\n                        fluo_chn2_traces\n                    )  # increment mask id from all planes\n                    for mask_idx, (f2, fneu2) in enumerate(\n                        zip(s2p.F_chan2, s2p.Fneu_chan2)\n                    ):\n                        fluo_chn2_traces.append(\n                            {\n                                **key,\n                                \"mask\": mask_idx + mask_chn2_count,\n                                \"fluo_channel\": 1,\n                                \"fluorescence\": f2,\n                                \"neuropil_fluorescence\": fneu2,\n                            }\n                        )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces + fluo_chn2_traces)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            fluo_traces = []\n            for mask in caiman_dataset.masks:\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask[\"mask_id\"],\n                        \"fluo_channel\": segmentation_channel,\n                        \"fluorescence\": mask[\"inferred_trace\"],\n                    }\n                )\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n        elif method == \"extract\":\n            extract_dataset = imaging_dataset\n\n            fluo_traces = [\n                {\n                    **key,\n                    \"mask\": mask_id,\n                    \"fluo_channel\": 0,\n                    \"fluorescence\": fluorescence,\n                }\n                for mask_id, fluorescence in enumerate(extract_dataset.T)\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(fluo_traces)\n\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Traces obtained from segmented region of interests.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>scan.Channel.proj(fluo_channel='channel')</code> <code>int</code> <p>The channel that this trace comes from.</p> <code>fluorescence</code> <code>longblob</code> <p>Fluorescence trace associated with this mask.</p> <code>neuropil_fluorescence</code> <code>longblob</code> <p>Neuropil fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Traces obtained from segmented region of interests.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        scan.Channel.proj(fluo_channel='channel') (int): The channel that this trace\n            comes from.\n        fluorescence (longblob): Fluorescence trace associated with this mask.\n        neuropil_fluorescence (longblob, optional): Neuropil fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    -&gt; scan.Channel.proj(fluo_channel='channel')  # The channel that this trace comes from\n    ---\n    fluorescence                : longblob  # Fluorescence trace associated with this mask\n    neuropil_fluorescence=null  : longblob  # Neuropil fluorescence trace\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Fluorescence.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Fluorescence with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Fluorescence with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        suite2p_dataset = imaging_dataset\n\n        # ---- iterate through all s2p plane outputs ----\n        fluo_traces, fluo_chn2_traces = [], []\n        for s2p in suite2p_dataset.planes.values():\n            mask_count = len(fluo_traces)  # increment mask id from all \"plane\"\n            for mask_idx, (f, fneu) in enumerate(zip(s2p.F, s2p.Fneu)):\n                fluo_traces.append(\n                    {\n                        **key,\n                        \"mask\": mask_idx + mask_count,\n                        \"fluo_channel\": 0,\n                        \"fluorescence\": f,\n                        \"neuropil_fluorescence\": fneu,\n                    }\n                )\n            if len(s2p.F_chan2):\n                mask_chn2_count = len(\n                    fluo_chn2_traces\n                )  # increment mask id from all planes\n                for mask_idx, (f2, fneu2) in enumerate(\n                    zip(s2p.F_chan2, s2p.Fneu_chan2)\n                ):\n                    fluo_chn2_traces.append(\n                        {\n                            **key,\n                            \"mask\": mask_idx + mask_chn2_count,\n                            \"fluo_channel\": 1,\n                            \"fluorescence\": f2,\n                            \"neuropil_fluorescence\": fneu2,\n                        }\n                    )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces + fluo_chn2_traces)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        # infer \"segmentation_channel\" - from params if available, else from caiman loader\n        params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n        segmentation_channel = params.get(\n            \"segmentation_channel\", caiman_dataset.segmentation_channel\n        )\n\n        fluo_traces = []\n        for mask in caiman_dataset.masks:\n            fluo_traces.append(\n                {\n                    **key,\n                    \"mask\": mask[\"mask_id\"],\n                    \"fluo_channel\": segmentation_channel,\n                    \"fluorescence\": mask[\"inferred_trace\"],\n                }\n            )\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n    elif method == \"extract\":\n        extract_dataset = imaging_dataset\n\n        fluo_traces = [\n            {\n                **key,\n                \"mask\": mask_id,\n                \"fluo_channel\": 0,\n                \"fluorescence\": fluorescence,\n            }\n            for mask_id, fluorescence in enumerate(extract_dataset.T)\n        ]\n\n        self.insert1(key)\n        self.Trace.insert(fluo_traces)\n\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ActivityExtractionMethod", "title": "<code>ActivityExtractionMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Available activity extraction methods.</p> <p>Attributes:</p> Name Type Description <code>extraction_method</code> <code>str</code> <p>Extraction method.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass ActivityExtractionMethod(dj.Lookup):\n\"\"\"Available activity extraction methods.\n\n    Attributes:\n        extraction_method (str): Extraction method.\n    \"\"\"\n\n    definition = \"\"\"# Activity extraction method\n    extraction_method: varchar(32)\n    \"\"\"\n\n    contents = zip([\"suite2p_deconvolution\", \"caiman_deconvolution\", \"caiman_dff\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity", "title": "<code>Activity</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>ActivityExtractionMethod</code> <code>foreign key</code> <p>Primary key from ActivityExtractionMethod.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass Activity(dj.Computed):\n\"\"\"Inferred neural activity from fluorescence trace (e.g. dff, spikes, etc.).\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        ActivityExtractionMethod (foreign key): Primary key from\n            ActivityExtractionMethod.\n    \"\"\"\n\n    definition = \"\"\"# Neural Activity\n    -&gt; Fluorescence\n    -&gt; ActivityExtractionMethod\n    \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n        Attributes:\n            Activity (foreign key): Primary key from Activity.\n            Fluorescence.Trace (foreign key): Fluorescence.Trace.\n            activity_trace (longblob): Neural activity from fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        activity_trace: longblob\n        \"\"\"\n\n    @property\n    def key_source(self):\n        suite2p_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"suite2p\"'\n            &amp; 'extraction_method LIKE \"suite2p%\"'\n        )\n        caiman_key_source = (\n            Fluorescence\n            * ActivityExtractionMethod\n            * ProcessingParamSet.proj(\"processing_method\")\n            &amp; 'processing_method = \"caiman\"'\n            &amp; 'extraction_method LIKE \"caiman%\"'\n        )\n        return suite2p_key_source.proj() + caiman_key_source.proj()\n\n    def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\"\n\n        method, imaging_dataset = get_loader_result(key, Curation)\n\n        if method == \"suite2p\":\n            if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n                suite2p_dataset = imaging_dataset\n                # ---- iterate through all s2p plane outputs ----\n                spikes = [\n                    dict(\n                        key,\n                        mask=mask_idx,\n                        fluo_channel=0,\n                        activity_trace=spks,\n                    )\n                    for mask_idx, spks in enumerate(\n                        s\n                        for plane in suite2p_dataset.planes.values()\n                        for s in plane.spks\n                    )\n                ]\n\n                self.insert1(key)\n                self.Trace.insert(spikes)\n        elif method == \"caiman\":\n            caiman_dataset = imaging_dataset\n\n            if key[\"extraction_method\"] in (\n                \"caiman_deconvolution\",\n                \"caiman_dff\",\n            ):\n                attr_mapper = {\n                    \"caiman_deconvolution\": \"spikes\",\n                    \"caiman_dff\": \"dff\",\n                }\n\n                # infer \"segmentation_channel\" - from params if available, else from caiman loader\n                params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n                segmentation_channel = params.get(\n                    \"segmentation_channel\", caiman_dataset.segmentation_channel\n                )\n\n                self.insert1(key)\n                self.Trace.insert(\n                    dict(\n                        key,\n                        mask=mask[\"mask_id\"],\n                        fluo_channel=segmentation_channel,\n                        activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                    )\n                    for mask in caiman_dataset.masks\n                )\n        else:\n            raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trace(s) for each mask.</p> <p>Attributes:</p> Name Type Description <code>Activity</code> <code>foreign key</code> <p>Primary key from Activity.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Fluorescence.Trace.</p> <code>activity_trace</code> <code>longblob</code> <p>Neural activity from fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Trace(s) for each mask.\n\n    Attributes:\n        Activity (foreign key): Primary key from Activity.\n        Fluorescence.Trace (foreign key): Fluorescence.Trace.\n        activity_trace (longblob): Neural activity from fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    activity_trace: longblob\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.Activity.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Activity with the results parsed from analysis outputs.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Activity with the results parsed from analysis outputs.\"\"\"\n\n    method, imaging_dataset = get_loader_result(key, Curation)\n\n    if method == \"suite2p\":\n        if key[\"extraction_method\"] == \"suite2p_deconvolution\":\n            suite2p_dataset = imaging_dataset\n            # ---- iterate through all s2p plane outputs ----\n            spikes = [\n                dict(\n                    key,\n                    mask=mask_idx,\n                    fluo_channel=0,\n                    activity_trace=spks,\n                )\n                for mask_idx, spks in enumerate(\n                    s\n                    for plane in suite2p_dataset.planes.values()\n                    for s in plane.spks\n                )\n            ]\n\n            self.insert1(key)\n            self.Trace.insert(spikes)\n    elif method == \"caiman\":\n        caiman_dataset = imaging_dataset\n\n        if key[\"extraction_method\"] in (\n            \"caiman_deconvolution\",\n            \"caiman_dff\",\n        ):\n            attr_mapper = {\n                \"caiman_deconvolution\": \"spikes\",\n                \"caiman_dff\": \"dff\",\n            }\n\n            # infer \"segmentation_channel\" - from params if available, else from caiman loader\n            params = (ProcessingParamSet * ProcessingTask &amp; key).fetch1(\"params\")\n            segmentation_channel = params.get(\n                \"segmentation_channel\", caiman_dataset.segmentation_channel\n            )\n\n            self.insert1(key)\n            self.Trace.insert(\n                dict(\n                    key,\n                    mask=mask[\"mask_id\"],\n                    fluo_channel=segmentation_channel,\n                    activity_trace=mask[attr_mapper[key[\"extraction_method\"]]],\n                )\n                for mask in caiman_dataset.masks\n            )\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingQualityMetrics", "title": "<code>ProcessingQualityMetrics</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Quality metrics used to evaluate the results of the calcium imaging analysis pipeline.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>@schema\nclass ProcessingQualityMetrics(dj.Computed):\n\"\"\"Quality metrics used to evaluate the results of the calcium imaging analysis pipeline.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Fluorescence\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Quality metrics used to evaluate the masks.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n            mask_area (float): Mask area in square micrometer.\n            roundness (float): Roundness between 0 and 1. Values closer to 1 are rounder.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Mask\n        ---\n        mask_area=null: float  # Mask area in square micrometer.\n        roundness: float       # Roundness between 0 and 1. Values closer to 1 are rounder.\n        \"\"\"\n\n    class Trace(dj.Part):\n\"\"\"Quality metrics used to evaluate the fluorescence traces.\n\n        Attributes:\n            Fluorescence (foreign key): Primary key from Fluorescence.\n            Fluorescence.Trace (foreign key): Primary key from Fluorescence.Trace.\n            skewness (float): Skewness of the fluorescence trace.\n            variance (float): Variance of the fluorescence trace.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Fluorescence.Trace\n        ---\n        skewness: float   # Skewness of the fluorescence trace.\n        variance: float   # Variance of the fluorescence trace.\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the ProcessingQualityMetrics table and its part tables.\"\"\"\n        from scipy.stats import skew\n\n        (\n            mask_xpixs,\n            mask_ypixs,\n            mask_weights,\n            fluorescence,\n            fluo_channels,\n            mask_ids,\n            mask_npix,\n            px_height,\n            px_width,\n            um_height,\n            um_width,\n        ) = (Segmentation.Mask * scan.ScanInfo.Field * Fluorescence.Trace &amp; key).fetch(\n            \"mask_xpix\",\n            \"mask_ypix\",\n            \"mask_weights\",\n            \"fluorescence\",\n            \"fluo_channel\",\n            \"mask\",\n            \"mask_npix\",\n            \"px_height\",\n            \"px_width\",\n            \"um_height\",\n            \"um_width\",\n        )\n\n        norm_mean = lambda x: x.mean() / x.max()\n        roundnesses = [\n            norm_mean(np.linalg.eigvals(np.cov(x, y, aweights=w)))\n            for x, y, w in zip(mask_xpixs, mask_ypixs, mask_weights)\n        ]\n\n        fluorescence = np.stack(fluorescence)\n\n        self.insert1(key)\n\n        self.Mask.insert(\n            dict(key, mask=mask_id, mask_area=mask_area, roundness=roundness)\n            for mask_id, mask_area, roundness in zip(\n                mask_ids,\n                mask_npix * (um_height / px_height) * (um_width / px_width),\n                roundnesses,\n            )\n        )\n\n        self.Trace.insert(\n            dict(\n                key,\n                fluo_channel=fluo_channel,\n                mask=mask_id,\n                skewness=skewness,\n                variance=variance,\n            )\n            for fluo_channel, mask_id, skewness, variance in zip(\n                fluo_channels,\n                mask_ids,\n                skew(fluorescence, axis=1),\n                fluorescence.std(axis=1),\n            )\n        )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingQualityMetrics.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Quality metrics used to evaluate the masks.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from Segmentation.Mask.</p> <code>mask_area</code> <code>float</code> <p>Mask area in square micrometer.</p> <code>roundness</code> <code>float</code> <p>Roundness between 0 and 1. Values closer to 1 are rounder.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Quality metrics used to evaluate the masks.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Segmentation.Mask (foreign key): Primary key from Segmentation.Mask.\n        mask_area (float): Mask area in square micrometer.\n        roundness (float): Roundness between 0 and 1. Values closer to 1 are rounder.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Segmentation.Mask\n    ---\n    mask_area=null: float  # Mask area in square micrometer.\n    roundness: float       # Roundness between 0 and 1. Values closer to 1 are rounder.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingQualityMetrics.Trace", "title": "<code>Trace</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Quality metrics used to evaluate the fluorescence traces.</p> <p>Attributes:</p> Name Type Description <code>Fluorescence</code> <code>foreign key</code> <p>Primary key from Fluorescence.</p> <code>Fluorescence.Trace</code> <code>foreign key</code> <p>Primary key from Fluorescence.Trace.</p> <code>skewness</code> <code>float</code> <p>Skewness of the fluorescence trace.</p> <code>variance</code> <code>float</code> <p>Variance of the fluorescence trace.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>class Trace(dj.Part):\n\"\"\"Quality metrics used to evaluate the fluorescence traces.\n\n    Attributes:\n        Fluorescence (foreign key): Primary key from Fluorescence.\n        Fluorescence.Trace (foreign key): Primary key from Fluorescence.Trace.\n        skewness (float): Skewness of the fluorescence trace.\n        variance (float): Variance of the fluorescence trace.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Fluorescence.Trace\n    ---\n    skewness: float   # Skewness of the fluorescence trace.\n    variance: float   # Variance of the fluorescence trace.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.ProcessingQualityMetrics.make", "title": "<code>make(key)</code>", "text": "<p>Populate the ProcessingQualityMetrics table and its part tables.</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the ProcessingQualityMetrics table and its part tables.\"\"\"\n    from scipy.stats import skew\n\n    (\n        mask_xpixs,\n        mask_ypixs,\n        mask_weights,\n        fluorescence,\n        fluo_channels,\n        mask_ids,\n        mask_npix,\n        px_height,\n        px_width,\n        um_height,\n        um_width,\n    ) = (Segmentation.Mask * scan.ScanInfo.Field * Fluorescence.Trace &amp; key).fetch(\n        \"mask_xpix\",\n        \"mask_ypix\",\n        \"mask_weights\",\n        \"fluorescence\",\n        \"fluo_channel\",\n        \"mask\",\n        \"mask_npix\",\n        \"px_height\",\n        \"px_width\",\n        \"um_height\",\n        \"um_width\",\n    )\n\n    norm_mean = lambda x: x.mean() / x.max()\n    roundnesses = [\n        norm_mean(np.linalg.eigvals(np.cov(x, y, aweights=w)))\n        for x, y, w in zip(mask_xpixs, mask_ypixs, mask_weights)\n    ]\n\n    fluorescence = np.stack(fluorescence)\n\n    self.insert1(key)\n\n    self.Mask.insert(\n        dict(key, mask=mask_id, mask_area=mask_area, roundness=roundness)\n        for mask_id, mask_area, roundness in zip(\n            mask_ids,\n            mask_npix * (um_height / px_height) * (um_width / px_width),\n            roundnesses,\n        )\n    )\n\n    self.Trace.insert(\n        dict(\n            key,\n            fluo_channel=fluo_channel,\n            mask=mask_id,\n            skewness=skewness,\n            variance=variance,\n        )\n        for fluo_channel, mask_id, skewness, variance in zip(\n            fluo_channels,\n            mask_ids,\n            skew(fluorescence, axis=1),\n            fluorescence.std(axis=1),\n        )\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_preprocess/#element_calcium_imaging.imaging_preprocess.get_loader_result", "title": "<code>get_loader_result(key, table)</code>", "text": "<p>Retrieve the processed imaging results from a suite2p or caiman loader.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>The <code>key</code> to one entry of ProcessingTask or Curation</p> required <code>table</code> <code>dj.Table</code> <p>A datajoint table to retrieve the loaded results from (e.g. ProcessingTask, Curation)</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the processing_method is different than 'suite2p' or 'caiman'.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,</p> <code>Callable</code> <p>see element-interface for more information on the loaders.)</p> Source code in <code>element_calcium_imaging/imaging_preprocess.py</code> <pre><code>def get_loader_result(key: dict, table: dj.Table) -&gt; Callable:\n\"\"\"Retrieve the processed imaging results from a suite2p or caiman loader.\n\n    Args:\n        key (dict): The `key` to one entry of ProcessingTask or Curation\n        table (dj.Table): A datajoint table to retrieve the loaded results from (e.g.\n            ProcessingTask, Curation)\n\n    Raises:\n        NotImplementedError: If the processing_method is different than 'suite2p' or\n            'caiman'.\n\n    Returns:\n        A loader object of the loaded results (e.g. suite2p.Suite2p or caiman.CaImAn,\n        see element-interface for more information on the loaders.)\n    \"\"\"\n    method, output_dir = (ProcessingParamSet * table &amp; key).fetch1(\n        \"processing_method\", _table_attribute_mapper[table.__name__]\n    )\n\n    output_path = find_full_path(get_imaging_root_data_dir(), output_dir)\n\n    if method == \"suite2p\" or (\n        method == \"extract\" and table.__name__ == \"MotionCorrection\"\n    ):\n        from element_interface import suite2p_loader\n\n        loaded_dataset = suite2p_loader.Suite2p(output_path)\n    elif method == \"caiman\":\n        from element_interface import caiman_loader\n\n        loaded_dataset = caiman_loader.CaImAn(output_path)\n    elif method == \"extract\":\n        from element_interface import extract_loader\n\n        loaded_dataset = extract_loader.EXTRACT(output_path)\n    else:\n        raise NotImplementedError(\"Unknown/unimplemented method: {}\".format(method))\n\n    return method, loaded_dataset\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/", "title": "imaging_report.py", "text": ""}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.activate", "title": "<code>activate(schema_name, imaging_schema_name, *, create_schema=True, create_tables=True)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>imaging_report</code> schema</p> required <code>imaging_schema_name</code> <code>str</code> <p>Schema name of the activated imaging element for which this imaging_report schema will be downstream from</p> required <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>def activate(\n    schema_name: str,\n    imaging_schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True\n):\n\"\"\"Activate this schema.\n\n    Args:\n        schema_name (str): Schema name on the database server to activate the\n            `imaging_report` schema\n        imaging_schema_name (str): Schema name of the activated imaging element for\n            which this imaging_report schema will be downstream from\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n    \"\"\"\n    global imaging\n    imaging = dj.create_virtual_module(\"imaging\", imaging_schema_name)\n\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=imaging.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.ScanLevelReport", "title": "<code>ScanLevelReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Scan level report with figures.</p> <p>Attributes:</p> Name Type Description <code>imaging.Segmentation</code> <code>foreign key</code> <p>Primary key from imaging.Segmentation.</p> <code>cell_overlayed_image</code> <code>longblob</code> <p>Plotly figure object showing the segmented cells on the average image.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>@schema\nclass ScanLevelReport(dj.Computed):\n\"\"\"Scan level report with figures.\n\n    Attributes:\n        imaging.Segmentation (foreign key): Primary key from imaging.Segmentation.\n        cell_overlayed_image (longblob): Plotly figure object showing the segmented\n            cells on the average image.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; imaging.Segmentation\n    ---\n    cell_overlayed_image: longblob\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n        image_fig = cell_plot.plot_cell_overlayed_image(imaging, key)\n        self.insert1({**key, \"cell_overlayed_image\": image_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.ScanLevelReport.make", "title": "<code>make(key)</code>", "text": "<p>Compute and ingest the plotly figure objects.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n    image_fig = cell_plot.plot_cell_overlayed_image(imaging, key)\n    self.insert1({**key, \"cell_overlayed_image\": image_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.TraceReport", "title": "<code>TraceReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Figures of traces.</p> <p>Attributes:</p> Name Type Description <code>imaging.Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from imaging.Segmentation.Mask.</p> <code>cell_traces</code> <code>longblob</code> <p>Plotly figure object showing the cell traces.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>@schema\nclass TraceReport(dj.Computed):\n\"\"\"Figures of traces.\n\n    Attributes:\n        imaging.Segmentation.Mask (foreign key): Primary key from\n            imaging.Segmentation.Mask.\n        cell_traces (longblob): Plotly figure object showing the cell traces.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; imaging.Segmentation.Mask\n    ---\n    cell_traces: longblob\n    \"\"\"\n\n    @property\n    def key_source(self):\n\"\"\"Limit the TraceReport to Masks that have Activity table populated.\n        database.\"\"\"\n\n        return imaging.Segmentation.Mask &amp; imaging.Activity\n\n    def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n        trace_fig = cell_plot.plot_cell_traces(imaging, key)\n        self.insert1({**key, \"cell_traces\": trace_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.TraceReport.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the TraceReport to Masks that have Activity table populated. database.</p>"}, {"location": "api/element_calcium_imaging/imaging_report/#element_calcium_imaging.imaging_report.TraceReport.make", "title": "<code>make(key)</code>", "text": "<p>Compute and ingest the plotly figure objects.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n    trace_fig = cell_plot.plot_cell_traces(imaging, key)\n    self.insert1({**key, \"cell_traces\": trace_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/", "title": "scan.py", "text": ""}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.activate", "title": "<code>activate(scan_schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>scan_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>scan</code> module</p> required <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>scan</code> module.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <ul> <li>Session: Parent table to Scan, typically identifying a recording session</li> <li>Equipment: Reference table for Scan, specifying the equipment used for the     acquisition of this scan.</li> <li>Location: Reference table for ScanLocation, specifying the scanned regions's     anatomical location in the brain.</li> </ul> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def activate(\n    scan_schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        scan_schema_name (str): Schema name on the database server to activate the\n            `scan` module\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `scan` module.\n\n    Dependencies:\n    Upstream tables:\n        + Session: Parent table to Scan, typically identifying a recording session\n        + Equipment: Reference table for Scan, specifying the equipment used for the\n            acquisition of this scan.\n        + Location: Reference table for ScanLocation, specifying the scanned regions's\n            anatomical location in the brain.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    schema.activate(\n        scan_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Return imaging root data director(y/ies)</p> <p>Retrieve the root data director(y/ies) containing the imaging data for all subjects/sessions (e.g. acquired ScanImage raw files, output files from processing routines, etc.). All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dirs</code> <code>list</code> <p>A list of string(s) or Path(s) for the absolute paths of the imaging root data director(y/ies).</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_imaging_root_data_dir() -&gt; list:\n\"\"\"Return imaging root data director(y/ies)\n\n    Retrieve the root data director(y/ies) containing the imaging data\n    for all subjects/sessions (e.g. acquired ScanImage raw files, output files from\n    processing routines, etc.). All data paths and directories in DataJoint Elements are\n    recommended to be stored as relative paths (posix format), with respect to some\n    user-configured \"root\" directory, which varies from machine to machine\n    (e.g. different mounted drive locations).\n\n    Returns:\n        dirs (list): A list of string(s) or Path(s) for the absolute paths of the imaging root data\n            director(y/ies).\n    \"\"\"\n\n    root_directories = _linking_module.get_imaging_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Retrieve the root directory for all processed data.</p> <p>All data paths and directories in DataJoint Elements are recommended to be stored as relative paths (posix format), with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations).</p> <p>Returns:</p> Name Type Description <code>dir</code> <code>str | pathlib.Path</code> <p>Absolute path of the processed imaging root data directory.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_processed_root_data_dir() -&gt; Union[str, pathlib.Path]:\n\"\"\"Retrieve the root directory for all processed data.\n\n    All data paths and directories in DataJoint Elements are recommended to be stored as\n    relative paths (posix format), with respect to some user-configured \"root\"\n    directory, which varies from machine to machine (e.g. different mounted drive\n    locations).\n\n    Returns:\n        dir (str| pathlib.Path): Absolute path of the processed imaging root data\n            directory.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_imaging_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_scan_image_files", "title": "<code>get_scan_image_files(scan_key)</code>", "text": "<p>Retrieve the list of ScanImage files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of ScanImage files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_image_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of ScanImage files associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of ScanImage files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_image_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_scan_box_files", "title": "<code>get_scan_box_files(scan_key)</code>", "text": "<p>Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Scanbox files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_scan_box_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Scanbox files (*.sbx) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Scanbox files' full file-paths.\n    \"\"\"\n    return _linking_module.get_scan_box_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_nd2_files", "title": "<code>get_nd2_files(scan_key)</code>", "text": "<p>Retrieve the list of Nikon files (*.nd2) associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Nikon files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_nd2_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Nikon files (*.nd2) associated with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Nikon files' full file-paths.\n    \"\"\"\n    return _linking_module.get_nd2_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.get_prairieview_files", "title": "<code>get_prairieview_files(scan_key)</code>", "text": "<p>Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Bruker PrairieView files' full file-paths.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def get_prairieview_files(scan_key: dict) -&gt; list:\n\"\"\"Retrieve the list of Bruker PrairieView tif files (*.tif) with a given Scan.\n\n    Args:\n        scan_key: Primary key of a Scan entry.\n\n    Returns:\n        A list of Bruker PrairieView files' full file-paths.\n    \"\"\"\n    return _linking_module.get_prairieview_files(scan_key)\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.AcquisitionSoftware", "title": "<code>AcquisitionSoftware</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>A list of acquisition softwares supported by the Element.</p> <p>Required to define a scan.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code>str</code> <p>Acquisition software</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"A list of acquisition softwares supported by the Element.\n\n    Required to define a scan.\n\n    Attributes:\n        acq_software (str): Acquisition software\n    \"\"\"\n\n    definition = \"\"\"  # Acquisition softwares\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"ScanImage\", \"Scanbox\", \"NIS\", \"PrairieView\"])\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.Channel", "title": "<code>Channel</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Recording channels for the imaging wavelengths.</p> <p>Attributes:</p> Name Type Description <code>channel</code> <code>int</code> <p>Channel index</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass Channel(dj.Lookup):\n\"\"\"Recording channels for the imaging wavelengths.\n\n    Attributes:\n        channel (int): Channel index\n    \"\"\"\n\n    definition = \"\"\"  # A recording channel\n    channel     : tinyint  # 0-based indexing\n    \"\"\"\n    contents = zip(range(5))\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.Scan", "title": "<code>Scan</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Scan defined by a measurement done using a scanner and an acquisition software.</p> <p>The details of the scanning data is placed in other tables, including, ScanLocation, ScanInfo, and ScanInfo's part tables.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>A primary key from Session.</p> <code>scan_id</code> <code>int</code> <p>Unique Scan ID.</p> <code>Equipment</code> <code>foreign key</code> <p>A primary key from Equipment.</p> <code>AcquisitionSoftware</code> <code>foreign key</code> <p>A primary key from AcquisitionSoftware.</p> <code>scan_notes</code> <code>str</code> <p>Notes of the experimenter regarding the scan.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass Scan(dj.Manual):\n\"\"\"Scan defined by a measurement done using a scanner and an acquisition software.\n\n    The details of the scanning data is placed in other tables, including,\n    ScanLocation, ScanInfo, and ScanInfo's part tables.\n\n    Attributes:\n        Session (foreign key): A primary key from Session.\n        scan_id (int): Unique Scan ID.\n        Equipment (foreign key, optional): A primary key from Equipment.\n        AcquisitionSoftware (foreign key): A primary key from AcquisitionSoftware.\n        scan_notes (str, optional): Notes of the experimenter regarding the scan.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Session\n    scan_id: int\n    ---\n    -&gt; [nullable] Equipment\n    -&gt; AcquisitionSoftware\n    scan_notes='' : varchar(4095)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanLocation", "title": "<code>ScanLocation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Anatomical location of the scanned region in the brain</p> <p>Attributes:</p> Name Type Description <code>Scan</code> <code>foreign key</code> <p>A primary key from Scan.</p> <code>Locaton</code> <code>foreign key</code> <p>A primary key from Location.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass ScanLocation(dj.Manual):\n\"\"\"Anatomical location of the scanned region in the brain\n\n    Attributes:\n        Scan (foreign key): A primary key from Scan.\n        Locaton (foreign key): A primary key from Location.\n    \"\"\"\n\n    definition = \"\"\" # Anatomical location\n    -&gt; Scan\n    ---\n    -&gt; Location\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo", "title": "<code>ScanInfo</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Information about the scan extracted from the recorded files.</p> <p>Attributes:</p> Name Type Description <code>Scan</code> <code>foreign key</code> <p>A primary key from Scan.</p> <code>nfields</code> <code>int</code> <p>Number of fields.</p> <code>nchannels</code> <code>int</code> <p>Number of channels.</p> <code>ndepths</code> <code>int</code> <p>Number of scanning depths (planes).</p> <code>nframes</code> <code>int</code> <p>Number of recorded frames.</p> <code>nrois</code> <code>int</code> <p>Number of ROIs (see scanimage's multi ROI imaging).</p> <code>x</code> <code>float</code> <p>ScanImage's 0 point in the motor coordinate system (um).</p> <code>y</code> <code>float</code> <p>ScanImage's 0 point in the motor coordinate system (um).</p> <code>z</code> <code>float</code> <p>ScanImage's 0 point in the motor coordinate system (um).</p> <code>fps</code> <code>float) </code> <p>Frames per second (Hz) - Volumetric Scan Rate.</p> <code>bidirectional</code> <code>bool</code> <p>True = bidirectional scanning.</p> <code>usecs_per_line</code> <code>float</code> <p>Microseconds per scan line.</p> <code>fill_fraction</code> <code>float</code> <p>Raster scan temporal fill fraction (see scanimage)</p> <code>scan_datetime</code> <code>datetime</code> <p>Datetime of the scan.</p> <code>scan_duration</code> <code>float</code> <p>Duration of the scan (s).</p> <code>bidirectional_z</code> <code>bool</code> <p>True = bidirectional z-scan.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass ScanInfo(dj.Imported):\n\"\"\"\n    Information about the scan extracted from the recorded files.\n\n    Attributes:\n        Scan (foreign key): A primary key from Scan.\n        nfields (int): Number of fields.\n        nchannels (int): Number of channels.\n        ndepths (int): Number of scanning depths (planes).\n        nframes (int): Number of recorded frames.\n        nrois (int): Number of ROIs (see scanimage's multi ROI imaging).\n        x (float, optional): ScanImage's 0 point in the motor coordinate system (um).\n        y (float, optional): ScanImage's 0 point in the motor coordinate system (um).\n        z (float, optional): ScanImage's 0 point in the motor coordinate system (um).\n        fps (float) : Frames per second (Hz) - Volumetric Scan Rate.\n        bidirectional (bool): True = bidirectional scanning.\n        usecs_per_line (float, optional): Microseconds per scan line.\n        fill_fraction (float, optional): Raster scan temporal fill fraction (see\n            scanimage)\n        scan_datetime (datetime, optional): Datetime of the scan.\n        scan_duration (float, optional): Duration of the scan (s).\n        bidirectional_z (bool, optional): True = bidirectional z-scan.\n    \"\"\"\n\n    definition = \"\"\" # General data about the resoscans/mesoscans from header\n    -&gt; Scan\n    ---\n    nfields              : tinyint   # number of fields\n    nchannels            : tinyint   # number of channels\n    ndepths              : int       # Number of scanning depths (planes)\n    nframes              : int       # number of recorded frames\n    nrois                : tinyint   # number of ROIs (see scanimage's multi ROI imaging)\n    x=null               : float     # (um) ScanImage's 0 point in the motor coordinate system\n    y=null               : float     # (um) ScanImage's 0 point in the motor coordinate system\n    z=null               : float     # (um) ScanImage's 0 point in the motor coordinate system\n    fps                  : float     # (Hz) frames per second - Volumetric Scan Rate\n    bidirectional        : boolean   # true = bidirectional scanning\n    usecs_per_line=null  : float     # microseconds per scan line\n    fill_fraction=null   : float     # raster scan temporal fill fraction (see scanimage)\n    scan_datetime=null   : datetime  # datetime of the scan\n    scan_duration=null   : float     # (seconds) duration of the scan\n    bidirectional_z=null : boolean   # true = bidirectional z-scan\n    \"\"\"\n\n    class Field(dj.Part):\n\"\"\"Stores field information of scan, including its coordinates, size, pixel\n        pitch, etc.\n\n        Attributes:\n            ScanInfo (foreign key): A primary key from ScanInfo.\n            field_idx (int): Unique field index.\n            px_height (int): Image height in pixels.\n            px_width (int): Image width in pixels.\n            um_height (float, optional): Image height in microns.\n            um_width (float, optional): Image width in microns.\n            field_x (float, optional): X coordinate of the center of field in the motor\n                coordinate system (um).\n            field_y (float, optional): Y coordinate of the center of field in the motor\n                coordinate system (um).\n            field_z (float, optional): Relative depth of field (um).\n            delay_image (longblob, optional): Delay between the start of the scan and\n                pixels in this field (ms).\n            roi (int, optional): The scanning roi (as recorded in the acquisition\n                software) containing this field - only relevant to mesoscale scans.\n        \"\"\"\n\n        definition = \"\"\" # field-specific scan information\n        -&gt; master\n        field_idx         : int\n        ---\n        px_height         : smallint  # height in pixels\n        px_width          : smallint  # width in pixels\n        um_height=null    : float     # height in microns\n        um_width=null     : float     # width in microns\n        field_x=null      : float     # (um) center of field in the motor coordinate system\n        field_y=null      : float     # (um) center of field in the motor coordinate system\n        field_z=null      : float     # (um) relative depth of field\n        delay_image=null  : longblob  # (ms) delay between the start of the scan and pixels in this field\n        roi=null          : int       # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans\n        \"\"\"\n\n    class ScanFile(dj.Part):\n\"\"\"Filepath of the scan relative to root data directory.\n\n        Attributes:\n            ScanInfo (foreign key): A primary key from ScanInfo.\n            file_path (str): Path of the scan file relative to the root data directory.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        file_path: varchar(255)  # Filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the ScanInfo with the information parsed from image files.\"\"\"\n\n        acq_software = (Scan &amp; key).fetch1(\"acq_software\")\n\n        if acq_software == \"ScanImage\":\n            import scanreader\n\n            # Read the scan\n            scan_filepaths = get_scan_image_files(key)\n            scan = scanreader.read_scan(scan_filepaths)\n\n            # Insert in ScanInfo\n            x_zero, y_zero, z_zero = scan.motor_position_at_zero or (None, None, None)\n\n            self.insert1(\n                dict(\n                    key,\n                    nfields=scan.num_fields,\n                    nchannels=scan.num_channels,\n                    nframes=scan.num_frames,\n                    ndepths=scan.num_scanning_depths,\n                    x=x_zero,\n                    y=y_zero,\n                    z=z_zero,\n                    fps=scan.fps,\n                    bidirectional=scan.is_bidirectional,\n                    usecs_per_line=scan.seconds_per_line * 1e6,\n                    fill_fraction=scan.temporal_fill_fraction,\n                    nrois=scan.num_rois if scan.is_multiROI else 0,\n                    scan_duration=scan.num_frames / scan.fps,\n                )\n            )\n            # Insert Field(s)\n            if scan.is_multiROI:\n                self.Field.insert(\n                    [\n                        dict(\n                            key,\n                            field_idx=field_id,\n                            px_height=scan.field_heights[field_id],\n                            px_width=scan.field_widths[field_id],\n                            um_height=scan.field_heights_in_microns[field_id],\n                            um_width=scan.field_widths_in_microns[field_id],\n                            field_x=x_zero\n                            + scan._degrees_to_microns(scan.fields[field_id].x)\n                            if x_zero\n                            else None,\n                            field_y=y_zero\n                            + scan._degrees_to_microns(scan.fields[field_id].y)\n                            if y_zero\n                            else None,\n                            field_z=z_zero + scan.fields[field_id].depth\n                            if z_zero\n                            else None,\n                            delay_image=scan.field_offsets[field_id],\n                            roi=scan.field_rois[field_id][0],\n                        )\n                        for field_id in range(scan.num_fields)\n                    ]\n                )\n            else:\n                self.Field.insert(\n                    [\n                        dict(\n                            key,\n                            field_idx=plane_idx,\n                            px_height=scan.image_height,\n                            px_width=scan.image_width,\n                            um_height=getattr(scan, \"image_height_in_microns\", None),\n                            um_width=getattr(scan, \"image_width_in_microns\", None),\n                            field_x=x_zero if x_zero else None,\n                            field_y=y_zero if y_zero else None,\n                            field_z=z_zero + scan.scanning_depths[plane_idx]\n                            if z_zero\n                            else None,\n                            delay_image=scan.field_offsets[plane_idx],\n                        )\n                        for plane_idx in range(scan.num_scanning_depths)\n                    ]\n                )\n        elif acq_software == \"Scanbox\":\n            import sbxreader\n\n            # Read the scan\n            scan_filepaths = get_scan_box_files(key)\n            sbx_meta = sbxreader.sbx_get_metadata(scan_filepaths[0])\n            sbx_matinfo = sbxreader.sbx_get_info(scan_filepaths[0])\n            is_multiROI = bool(\n                sbx_matinfo.mesoscope.enabled\n            )  # currently not handling \"multiROI\" ingestion\n\n            if is_multiROI:\n                raise NotImplementedError(\n                    \"Loading routine not implemented for Scanbox multiROI scan mode\"\n                )\n\n            # Insert in ScanInfo\n            x_zero, y_zero, z_zero = sbx_meta[\"stage_pos\"]\n            self.insert1(\n                dict(\n                    key,\n                    nfields=sbx_meta[\"num_fields\"]\n                    if is_multiROI\n                    else sbx_meta[\"num_planes\"],\n                    nchannels=sbx_meta[\"num_channels\"],\n                    nframes=sbx_meta[\"num_frames\"],\n                    ndepths=sbx_meta[\"num_planes\"],\n                    x=x_zero,\n                    y=y_zero,\n                    z=z_zero,\n                    fps=sbx_meta[\"frame_rate\"],\n                    bidirectional=sbx_meta == \"bidirectional\",\n                    nrois=sbx_meta[\"num_rois\"] if is_multiROI else 0,\n                    scan_duration=(sbx_meta[\"num_frames\"] / sbx_meta[\"frame_rate\"]),\n                )\n            )\n            # Insert Field(s)\n            if not is_multiROI:\n                px_width, px_height = sbx_meta[\"frame_size\"]\n                self.Field.insert(\n                    [\n                        dict(\n                            key,\n                            field_idx=plane_idx,\n                            px_height=px_height,\n                            px_width=px_width,\n                            um_height=px_height * sbx_meta[\"um_per_pixel_y\"]\n                            if sbx_meta[\"um_per_pixel_y\"]\n                            else None,\n                            um_width=px_width * sbx_meta[\"um_per_pixel_x\"]\n                            if sbx_meta[\"um_per_pixel_x\"]\n                            else None,\n                            field_x=x_zero,\n                            field_y=y_zero,\n                            field_z=z_zero + sbx_meta[\"etl_pos\"][plane_idx],\n                        )\n                        for plane_idx in range(sbx_meta[\"num_planes\"])\n                    ]\n                )\n        elif acq_software == \"NIS\":\n            import nd2\n\n            # Read the scan\n            scan_filepaths = get_nd2_files(key)\n            nd2_file = nd2.ND2File(scan_filepaths[0])\n            is_multiROI = False  # MultiROI to be implemented later\n\n            # Frame per second\n            try:\n                fps = 1000 / nd2_file.experiment[0].parameters.periods[0].periodDiff.avg\n            except:  # noqa: E722\n                fps = 1000 / nd2_file.experiment[0].parameters.periodDiff.avg\n\n            # Estimate ND2 file scan duration\n            def estimate_nd2_scan_duration(nd2_scan_obj):\n                # Calculates scan duration for Nikon images\n                ti = (\n                    nd2_scan_obj.frame_metadata(0)\n                    .channels[0]\n                    .time.absoluteJulianDayNumber\n                )  # Initial frame's JD.\n                tf = (\n                    nd2_scan_obj.frame_metadata(nd2_scan_obj.shape[0] - 1)\n                    .channels[0]\n                    .time.absoluteJulianDayNumber\n                )  # Final frame's JD.\n\n                return (tf - ti) * 86400 + 1 / fps\n\n            scan_duration = sum(\n                estimate_nd2_scan_duration(nd2.ND2File(f)) for f in scan_filepaths\n            )\n\n            try:\n                scan_datetime = nd2_file.text_info[\"date\"]\n                scan_datetime = datetime.strptime(\n                    scan_datetime,\n                    \"%m/%d/%Y %H:%M:%S %p\"\n                    if re.search((\"AM|PM\"), scan_datetime)\n                    else \"%m/%d/%Y %H:%M:%S\",\n                )\n                scan_datetime = datetime.strftime(scan_datetime, \"%Y-%m-%d %H:%M:%S\")\n            except:  # noqa: E722\n                scan_datetime = None\n\n            # Insert in ScanInfo\n            self.insert1(\n                dict(\n                    key,\n                    nfields=nd2_file.sizes.get(\"P\", 1),\n                    nchannels=nd2_file.attributes.channelCount,\n                    nframes=nd2_file.metadata.contents.frameCount,\n                    ndepths=nd2_file.sizes.get(\"Z\", 1),\n                    x=None,\n                    y=None,\n                    z=None,\n                    fps=fps,\n                    bidirectional=bool(\n                        nd2_file.custom_data[\"GrabberCameraSettingsV1_0\"][\n                            \"GrabberCameraSettings\"\n                        ][\"PropertiesQuality\"][\"ScanDirection\"]\n                    ),\n                    nrois=0,\n                    scan_datetime=scan_datetime,\n                    scan_duration=scan_duration,\n                )\n            )\n\n            # MultiROI to be implemented later\n\n            # Insert in Field\n            if not is_multiROI:\n                self.Field.insert(\n                    [\n                        dict(\n                            key,\n                            field_idx=plane_idx,\n                            px_height=nd2_file.attributes.heightPx,\n                            px_width=nd2_file.attributes.widthPx,\n                            um_height=nd2_file.attributes.heightPx\n                            * nd2_file.voxel_size().y,\n                            um_width=nd2_file.attributes.widthPx\n                            * nd2_file.voxel_size().x,\n                            field_x=None,\n                            field_y=None,\n                            field_z=None,\n                        )\n                        for plane_idx in range(nd2_file.sizes.get(\"Z\", 1))\n                    ]\n                )\n        elif acq_software == \"PrairieView\":\n            from element_interface import prairieviewreader\n\n            scan_filepaths = get_prairieview_files(key)\n            PVScan_info = prairieviewreader.get_pv_metadata(scan_filepaths[0])\n            self.insert1(\n                dict(\n                    key,\n                    nfields=PVScan_info[\"num_fields\"],\n                    nchannels=PVScan_info[\"num_channels\"],\n                    ndepths=PVScan_info[\"num_planes\"],\n                    nframes=PVScan_info[\"num_frames\"],\n                    nrois=PVScan_info[\"num_rois\"],\n                    x=PVScan_info[\"x_pos\"],\n                    y=PVScan_info[\"y_pos\"],\n                    z=PVScan_info[\"z_pos\"],\n                    fps=PVScan_info[\"frame_rate\"],\n                    bidirectional=PVScan_info[\"bidirectional\"],\n                    bidirectional_z=PVScan_info[\"bidirectional_z\"],\n                    usecs_per_line=PVScan_info[\"usecs_per_line\"],\n                    scan_datetime=PVScan_info[\"scan_datetime\"],\n                    scan_duration=PVScan_info[\"scan_duration\"],\n                )\n            )\n\n            self.Field.insert(\n                dict(\n                    key,\n                    field_idx=plane_idx,\n                    px_height=PVScan_info[\"height_in_pixels\"],\n                    px_width=PVScan_info[\"width_in_pixels\"],\n                    um_height=PVScan_info[\"height_in_um\"],\n                    um_width=PVScan_info[\"width_in_um\"],\n                    field_x=PVScan_info[\"fieldX\"],\n                    field_y=PVScan_info[\"fieldY\"],\n                    field_z=PVScan_info[\"fieldZ\"]\n                    if PVScan_info[\"num_planes\"] == 1\n                    else PVScan_info[\"fieldZ\"][plane_idx],\n                )\n                for plane_idx in range(PVScan_info[\"num_planes\"])\n            )\n        else:\n            raise NotImplementedError(\n                f\"Loading routine not implemented for {acq_software} \"\n                \"acquisition software\"\n            )\n\n        # Insert file(s)\n        root_dir = find_root_directory(get_imaging_root_data_dir(), scan_filepaths[0])\n\n        scan_files = [\n            pathlib.Path(f).relative_to(root_dir).as_posix() for f in scan_filepaths\n        ]\n        self.ScanFile.insert([{**key, \"file_path\": f} for f in scan_files])\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.Field", "title": "<code>Field</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Stores field information of scan, including its coordinates, size, pixel pitch, etc.</p> <p>Attributes:</p> Name Type Description <code>ScanInfo</code> <code>foreign key</code> <p>A primary key from ScanInfo.</p> <code>field_idx</code> <code>int</code> <p>Unique field index.</p> <code>px_height</code> <code>int</code> <p>Image height in pixels.</p> <code>px_width</code> <code>int</code> <p>Image width in pixels.</p> <code>um_height</code> <code>float</code> <p>Image height in microns.</p> <code>um_width</code> <code>float</code> <p>Image width in microns.</p> <code>field_x</code> <code>float</code> <p>X coordinate of the center of field in the motor coordinate system (um).</p> <code>field_y</code> <code>float</code> <p>Y coordinate of the center of field in the motor coordinate system (um).</p> <code>field_z</code> <code>float</code> <p>Relative depth of field (um).</p> <code>delay_image</code> <code>longblob</code> <p>Delay between the start of the scan and pixels in this field (ms).</p> <code>roi</code> <code>int</code> <p>The scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>class Field(dj.Part):\n\"\"\"Stores field information of scan, including its coordinates, size, pixel\n    pitch, etc.\n\n    Attributes:\n        ScanInfo (foreign key): A primary key from ScanInfo.\n        field_idx (int): Unique field index.\n        px_height (int): Image height in pixels.\n        px_width (int): Image width in pixels.\n        um_height (float, optional): Image height in microns.\n        um_width (float, optional): Image width in microns.\n        field_x (float, optional): X coordinate of the center of field in the motor\n            coordinate system (um).\n        field_y (float, optional): Y coordinate of the center of field in the motor\n            coordinate system (um).\n        field_z (float, optional): Relative depth of field (um).\n        delay_image (longblob, optional): Delay between the start of the scan and\n            pixels in this field (ms).\n        roi (int, optional): The scanning roi (as recorded in the acquisition\n            software) containing this field - only relevant to mesoscale scans.\n    \"\"\"\n\n    definition = \"\"\" # field-specific scan information\n    -&gt; master\n    field_idx         : int\n    ---\n    px_height         : smallint  # height in pixels\n    px_width          : smallint  # width in pixels\n    um_height=null    : float     # height in microns\n    um_width=null     : float     # width in microns\n    field_x=null      : float     # (um) center of field in the motor coordinate system\n    field_y=null      : float     # (um) center of field in the motor coordinate system\n    field_z=null      : float     # (um) relative depth of field\n    delay_image=null  : longblob  # (ms) delay between the start of the scan and pixels in this field\n    roi=null          : int       # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.ScanFile", "title": "<code>ScanFile</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Filepath of the scan relative to root data directory.</p> <p>Attributes:</p> Name Type Description <code>ScanInfo</code> <code>foreign key</code> <p>A primary key from ScanInfo.</p> <code>file_path</code> <code>str</code> <p>Path of the scan file relative to the root data directory.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>class ScanFile(dj.Part):\n\"\"\"Filepath of the scan relative to root data directory.\n\n    Attributes:\n        ScanInfo (foreign key): A primary key from ScanInfo.\n        file_path (str): Path of the scan file relative to the root data directory.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    file_path: varchar(255)  # Filepath relative to root data directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanInfo.make", "title": "<code>make(key)</code>", "text": "<p>Populate the ScanInfo with the information parsed from image files.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the ScanInfo with the information parsed from image files.\"\"\"\n\n    acq_software = (Scan &amp; key).fetch1(\"acq_software\")\n\n    if acq_software == \"ScanImage\":\n        import scanreader\n\n        # Read the scan\n        scan_filepaths = get_scan_image_files(key)\n        scan = scanreader.read_scan(scan_filepaths)\n\n        # Insert in ScanInfo\n        x_zero, y_zero, z_zero = scan.motor_position_at_zero or (None, None, None)\n\n        self.insert1(\n            dict(\n                key,\n                nfields=scan.num_fields,\n                nchannels=scan.num_channels,\n                nframes=scan.num_frames,\n                ndepths=scan.num_scanning_depths,\n                x=x_zero,\n                y=y_zero,\n                z=z_zero,\n                fps=scan.fps,\n                bidirectional=scan.is_bidirectional,\n                usecs_per_line=scan.seconds_per_line * 1e6,\n                fill_fraction=scan.temporal_fill_fraction,\n                nrois=scan.num_rois if scan.is_multiROI else 0,\n                scan_duration=scan.num_frames / scan.fps,\n            )\n        )\n        # Insert Field(s)\n        if scan.is_multiROI:\n            self.Field.insert(\n                [\n                    dict(\n                        key,\n                        field_idx=field_id,\n                        px_height=scan.field_heights[field_id],\n                        px_width=scan.field_widths[field_id],\n                        um_height=scan.field_heights_in_microns[field_id],\n                        um_width=scan.field_widths_in_microns[field_id],\n                        field_x=x_zero\n                        + scan._degrees_to_microns(scan.fields[field_id].x)\n                        if x_zero\n                        else None,\n                        field_y=y_zero\n                        + scan._degrees_to_microns(scan.fields[field_id].y)\n                        if y_zero\n                        else None,\n                        field_z=z_zero + scan.fields[field_id].depth\n                        if z_zero\n                        else None,\n                        delay_image=scan.field_offsets[field_id],\n                        roi=scan.field_rois[field_id][0],\n                    )\n                    for field_id in range(scan.num_fields)\n                ]\n            )\n        else:\n            self.Field.insert(\n                [\n                    dict(\n                        key,\n                        field_idx=plane_idx,\n                        px_height=scan.image_height,\n                        px_width=scan.image_width,\n                        um_height=getattr(scan, \"image_height_in_microns\", None),\n                        um_width=getattr(scan, \"image_width_in_microns\", None),\n                        field_x=x_zero if x_zero else None,\n                        field_y=y_zero if y_zero else None,\n                        field_z=z_zero + scan.scanning_depths[plane_idx]\n                        if z_zero\n                        else None,\n                        delay_image=scan.field_offsets[plane_idx],\n                    )\n                    for plane_idx in range(scan.num_scanning_depths)\n                ]\n            )\n    elif acq_software == \"Scanbox\":\n        import sbxreader\n\n        # Read the scan\n        scan_filepaths = get_scan_box_files(key)\n        sbx_meta = sbxreader.sbx_get_metadata(scan_filepaths[0])\n        sbx_matinfo = sbxreader.sbx_get_info(scan_filepaths[0])\n        is_multiROI = bool(\n            sbx_matinfo.mesoscope.enabled\n        )  # currently not handling \"multiROI\" ingestion\n\n        if is_multiROI:\n            raise NotImplementedError(\n                \"Loading routine not implemented for Scanbox multiROI scan mode\"\n            )\n\n        # Insert in ScanInfo\n        x_zero, y_zero, z_zero = sbx_meta[\"stage_pos\"]\n        self.insert1(\n            dict(\n                key,\n                nfields=sbx_meta[\"num_fields\"]\n                if is_multiROI\n                else sbx_meta[\"num_planes\"],\n                nchannels=sbx_meta[\"num_channels\"],\n                nframes=sbx_meta[\"num_frames\"],\n                ndepths=sbx_meta[\"num_planes\"],\n                x=x_zero,\n                y=y_zero,\n                z=z_zero,\n                fps=sbx_meta[\"frame_rate\"],\n                bidirectional=sbx_meta == \"bidirectional\",\n                nrois=sbx_meta[\"num_rois\"] if is_multiROI else 0,\n                scan_duration=(sbx_meta[\"num_frames\"] / sbx_meta[\"frame_rate\"]),\n            )\n        )\n        # Insert Field(s)\n        if not is_multiROI:\n            px_width, px_height = sbx_meta[\"frame_size\"]\n            self.Field.insert(\n                [\n                    dict(\n                        key,\n                        field_idx=plane_idx,\n                        px_height=px_height,\n                        px_width=px_width,\n                        um_height=px_height * sbx_meta[\"um_per_pixel_y\"]\n                        if sbx_meta[\"um_per_pixel_y\"]\n                        else None,\n                        um_width=px_width * sbx_meta[\"um_per_pixel_x\"]\n                        if sbx_meta[\"um_per_pixel_x\"]\n                        else None,\n                        field_x=x_zero,\n                        field_y=y_zero,\n                        field_z=z_zero + sbx_meta[\"etl_pos\"][plane_idx],\n                    )\n                    for plane_idx in range(sbx_meta[\"num_planes\"])\n                ]\n            )\n    elif acq_software == \"NIS\":\n        import nd2\n\n        # Read the scan\n        scan_filepaths = get_nd2_files(key)\n        nd2_file = nd2.ND2File(scan_filepaths[0])\n        is_multiROI = False  # MultiROI to be implemented later\n\n        # Frame per second\n        try:\n            fps = 1000 / nd2_file.experiment[0].parameters.periods[0].periodDiff.avg\n        except:  # noqa: E722\n            fps = 1000 / nd2_file.experiment[0].parameters.periodDiff.avg\n\n        # Estimate ND2 file scan duration\n        def estimate_nd2_scan_duration(nd2_scan_obj):\n            # Calculates scan duration for Nikon images\n            ti = (\n                nd2_scan_obj.frame_metadata(0)\n                .channels[0]\n                .time.absoluteJulianDayNumber\n            )  # Initial frame's JD.\n            tf = (\n                nd2_scan_obj.frame_metadata(nd2_scan_obj.shape[0] - 1)\n                .channels[0]\n                .time.absoluteJulianDayNumber\n            )  # Final frame's JD.\n\n            return (tf - ti) * 86400 + 1 / fps\n\n        scan_duration = sum(\n            estimate_nd2_scan_duration(nd2.ND2File(f)) for f in scan_filepaths\n        )\n\n        try:\n            scan_datetime = nd2_file.text_info[\"date\"]\n            scan_datetime = datetime.strptime(\n                scan_datetime,\n                \"%m/%d/%Y %H:%M:%S %p\"\n                if re.search((\"AM|PM\"), scan_datetime)\n                else \"%m/%d/%Y %H:%M:%S\",\n            )\n            scan_datetime = datetime.strftime(scan_datetime, \"%Y-%m-%d %H:%M:%S\")\n        except:  # noqa: E722\n            scan_datetime = None\n\n        # Insert in ScanInfo\n        self.insert1(\n            dict(\n                key,\n                nfields=nd2_file.sizes.get(\"P\", 1),\n                nchannels=nd2_file.attributes.channelCount,\n                nframes=nd2_file.metadata.contents.frameCount,\n                ndepths=nd2_file.sizes.get(\"Z\", 1),\n                x=None,\n                y=None,\n                z=None,\n                fps=fps,\n                bidirectional=bool(\n                    nd2_file.custom_data[\"GrabberCameraSettingsV1_0\"][\n                        \"GrabberCameraSettings\"\n                    ][\"PropertiesQuality\"][\"ScanDirection\"]\n                ),\n                nrois=0,\n                scan_datetime=scan_datetime,\n                scan_duration=scan_duration,\n            )\n        )\n\n        # MultiROI to be implemented later\n\n        # Insert in Field\n        if not is_multiROI:\n            self.Field.insert(\n                [\n                    dict(\n                        key,\n                        field_idx=plane_idx,\n                        px_height=nd2_file.attributes.heightPx,\n                        px_width=nd2_file.attributes.widthPx,\n                        um_height=nd2_file.attributes.heightPx\n                        * nd2_file.voxel_size().y,\n                        um_width=nd2_file.attributes.widthPx\n                        * nd2_file.voxel_size().x,\n                        field_x=None,\n                        field_y=None,\n                        field_z=None,\n                    )\n                    for plane_idx in range(nd2_file.sizes.get(\"Z\", 1))\n                ]\n            )\n    elif acq_software == \"PrairieView\":\n        from element_interface import prairieviewreader\n\n        scan_filepaths = get_prairieview_files(key)\n        PVScan_info = prairieviewreader.get_pv_metadata(scan_filepaths[0])\n        self.insert1(\n            dict(\n                key,\n                nfields=PVScan_info[\"num_fields\"],\n                nchannels=PVScan_info[\"num_channels\"],\n                ndepths=PVScan_info[\"num_planes\"],\n                nframes=PVScan_info[\"num_frames\"],\n                nrois=PVScan_info[\"num_rois\"],\n                x=PVScan_info[\"x_pos\"],\n                y=PVScan_info[\"y_pos\"],\n                z=PVScan_info[\"z_pos\"],\n                fps=PVScan_info[\"frame_rate\"],\n                bidirectional=PVScan_info[\"bidirectional\"],\n                bidirectional_z=PVScan_info[\"bidirectional_z\"],\n                usecs_per_line=PVScan_info[\"usecs_per_line\"],\n                scan_datetime=PVScan_info[\"scan_datetime\"],\n                scan_duration=PVScan_info[\"scan_duration\"],\n            )\n        )\n\n        self.Field.insert(\n            dict(\n                key,\n                field_idx=plane_idx,\n                px_height=PVScan_info[\"height_in_pixels\"],\n                px_width=PVScan_info[\"width_in_pixels\"],\n                um_height=PVScan_info[\"height_in_um\"],\n                um_width=PVScan_info[\"width_in_um\"],\n                field_x=PVScan_info[\"fieldX\"],\n                field_y=PVScan_info[\"fieldY\"],\n                field_z=PVScan_info[\"fieldZ\"]\n                if PVScan_info[\"num_planes\"] == 1\n                else PVScan_info[\"fieldZ\"][plane_idx],\n            )\n            for plane_idx in range(PVScan_info[\"num_planes\"])\n        )\n    else:\n        raise NotImplementedError(\n            f\"Loading routine not implemented for {acq_software} \"\n            \"acquisition software\"\n        )\n\n    # Insert file(s)\n    root_dir = find_root_directory(get_imaging_root_data_dir(), scan_filepaths[0])\n\n    scan_files = [\n        pathlib.Path(f).relative_to(root_dir).as_posix() for f in scan_filepaths\n    ]\n    self.ScanFile.insert([{**key, \"file_path\": f} for f in scan_files])\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanQualityMetrics", "title": "<code>ScanQualityMetrics</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Metrics to assess the quality of the scan.</p> <p>Attributes:</p> Name Type Description <code>ScanInfo.Field</code> <code>foreign key</code> <p>Primary key from ScanInfo.Field.</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>@schema\nclass ScanQualityMetrics(dj.Computed):\n\"\"\"Metrics to assess the quality of the scan.\n\n    Attributes:\n        ScanInfo.Field (foreign key): Primary key from ScanInfo.Field.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ScanInfo.Field\n    \"\"\"\n\n    class Frames(dj.Part):\n\"\"\"Metrics used to evaluate each frame.\n\n        Attributes:\n            ScanInfo.Field (foreign key): Primary key from ScanInfo.Field.\n            Channel (foreign key): Primary key from Channel.\n            min_intensity (longblob): Minimum value of each frame.\n            mean_intensity (longblob): Mean value of each frame.\n            max_intensity (longblob): Maximum value of each frame.\n            contrast (longblob): Contrast of each frame (i.e. difference between the 99 and 1 percentiles)\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Channel\n        ---\n        min_intensity: longblob   # Minimum value of each frame.\n        mean_intensity: longblob  # Mean value of each frame.\n        max_intensity: longblob   # Maximum value of each frame.\n        contrast: longblob        # Contrast of each frame (i.e. difference between the 99 and 1 percentiles)\n        \"\"\"\n\n    def make(self, key):\n        acq_software, nchannels = (Scan * ScanInfo &amp; key).fetch1(\n            \"acq_software\", \"nchannels\"\n        )\n\n        if acq_software == \"ScanImage\":\n            import scanreader\n\n            # Switch from FYXCT to TCYX\n            data = scanreader.read_scan(get_scan_image_files(key))[\n                key[\"field_idx\"]\n            ].transpose(3, 2, 0, 1)\n        elif acq_software == \"Scanbox\":\n            from sbxreader import sbx_memmap\n\n            # Switch from TFCYX to TCYX\n            data = sbx_memmap(get_scan_box_files(key))[:, key[\"field_idx\"]]\n        elif acq_software == \"NIS\":\n            import nd2\n\n            nd2_file = nd2.ND2File(get_nd2_files(key)[0])\n\n            nd2_dims = {k: i for i, k in enumerate(nd2_file.sizes)}\n\n            valid_dimensions = \"TZCYX\"\n            assert set(nd2_dims) &lt;= set(\n                valid_dimensions\n            ), f\"Unknown dimensions {set(nd2_dims)-set(valid_dimensions)} in file {get_nd2_files(key)[0]}.\"\n\n            # Sort the dimensions in the order of TZCYX, skipping the missing ones.\n            data = nd2_file.asarray().transpose(\n                [nd2_dims[x] for x in valid_dimensions if x in nd2_dims]\n            )\n\n            # Expand array to include the missing dimensions.\n            for i, dim in enumerate(\"TZC\"):\n                if dim not in nd2_dims:\n                    data = np.expand_dims(data, i)\n\n            data = data[:, key[\"field_idx\"]]  # Switch from TFCYX to TCYX\n\n        self.insert1(key)\n\n        for channel in range(nchannels):\n            movie = data[:, channel, :, :]\n\n            self.Frames.insert1(\n                dict(\n                    key,\n                    channel=channel,\n                    min_intensity=movie.min(axis=(1, 2)),\n                    mean_intensity=movie.mean(axis=(1, 2)),\n                    max_intensity=movie.max(axis=(1, 2)),\n                    contrast=np.percentile(movie, 99, axis=(1, 2))\n                    - np.percentile(movie, 1, axis=(1, 2)),\n                )\n            )\n</code></pre>"}, {"location": "api/element_calcium_imaging/scan/#element_calcium_imaging.scan.ScanQualityMetrics.Frames", "title": "<code>Frames</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Metrics used to evaluate each frame.</p> <p>Attributes:</p> Name Type Description <code>ScanInfo.Field</code> <code>foreign key</code> <p>Primary key from ScanInfo.Field.</p> <code>Channel</code> <code>foreign key</code> <p>Primary key from Channel.</p> <code>min_intensity</code> <code>longblob</code> <p>Minimum value of each frame.</p> <code>mean_intensity</code> <code>longblob</code> <p>Mean value of each frame.</p> <code>max_intensity</code> <code>longblob</code> <p>Maximum value of each frame.</p> <code>contrast</code> <code>longblob</code> <p>Contrast of each frame (i.e. difference between the 99 and 1 percentiles)</p> Source code in <code>element_calcium_imaging/scan.py</code> <pre><code>class Frames(dj.Part):\n\"\"\"Metrics used to evaluate each frame.\n\n    Attributes:\n        ScanInfo.Field (foreign key): Primary key from ScanInfo.Field.\n        Channel (foreign key): Primary key from Channel.\n        min_intensity (longblob): Minimum value of each frame.\n        mean_intensity (longblob): Mean value of each frame.\n        max_intensity (longblob): Maximum value of each frame.\n        contrast (longblob): Contrast of each frame (i.e. difference between the 99 and 1 percentiles)\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; Channel\n    ---\n    min_intensity: longblob   # Minimum value of each frame.\n    mean_intensity: longblob  # Mean value of each frame.\n    max_intensity: longblob   # Maximum value of each frame.\n    contrast: longblob        # Contrast of each frame (i.e. difference between the 99 and 1 percentiles)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/", "title": "cell_plot.py", "text": ""}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.mask_overlayed_image", "title": "<code>mask_overlayed_image(image, mask_xpix, mask_ypix, cell_mask_ids, low_q=0, high_q=0.99)</code>", "text": "<p>Overlay transparent cell masks on average image.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def mask_overlayed_image(\n    image, mask_xpix, mask_ypix, cell_mask_ids, low_q=0, high_q=0.99\n):\n\"\"\"Overlay transparent cell masks on average image.\"\"\"\n\n    q_min, q_max = np.quantile(image, [low_q, high_q])\n    image = np.clip(image, q_min, q_max)\n    image = (image - q_min) / (q_max - q_min)\n\n    SATURATION = 0.7\n    image = image[:, :, None] * np.array([0, 0, 1])\n    maskid_image = np.full(image.shape[:2], -1)\n    for xpix, ypix, roi_id in zip(mask_xpix, mask_ypix, cell_mask_ids):\n        image[ypix, xpix, :2] = [np.random.rand(), SATURATION]\n        maskid_image[ypix, xpix] = roi_id\n    image = (colors.hsv_to_rgb(image) * 255).astype(int)\n    return image, maskid_image\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.get_tracelayout", "title": "<code>get_tracelayout(key, width=600, height=600)</code>", "text": "<p>Returns a dictionary of layout settings for the trace figures.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def get_tracelayout(key, width=600, height=600) -&gt; dict:\n\"\"\"Returns a dictionary of layout settings for the trace figures.\"\"\"\n    text = f\"Trace for Cell {key['mask']}\" if isinstance(key, dict) else \"Trace\"\n\n    return dict(\n        margin=dict(l=0, r=0, b=0, t=65, pad=0),\n        width=width,\n        height=height,\n        transition={\"duration\": 0},\n        title={\n            \"text\": text,\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n            \"y\": 0.97,\n            \"x\": 0.5,\n        },\n        xaxis={\n            \"title\": \"Time (sec)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": True,\n        },\n        yaxis={\n            \"title\": \"Fluorescence (a.u.)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": True,\n            \"anchor\": \"free\",\n            \"overlaying\": \"y\",\n            \"side\": \"left\",\n            \"position\": 0,\n        },\n        yaxis2={\n            \"title\": \"Calcium Event (a.u.)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": True,\n            \"anchor\": \"free\",\n            \"overlaying\": \"y\",\n            \"side\": \"right\",\n            \"position\": 1,\n        },\n        shapes=[\n            go.layout.Shape(\n                type=\"rect\",\n                xref=\"paper\",\n                yref=\"paper\",\n                x0=0,\n                y0=0,\n                x1=1.0,\n                y1=1.0,\n                line={\"width\": 1, \"color\": \"black\"},\n            )\n        ],\n        legend={\n            \"traceorder\": \"normal\",\n            \"yanchor\": \"top\",\n            \"y\": 0.99,\n            \"xanchor\": \"right\",\n            \"x\": 0.99,\n        },\n        plot_bgcolor=\"rgba(0,0,0,0.05)\",\n        modebar_remove=[\n            \"zoom\",\n            \"resetScale\",\n            \"pan\",\n            \"select\",\n            \"zoomIn\",\n            \"zoomOut\",\n            \"autoScale2d\",\n        ],\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.figure_data", "title": "<code>figure_data(imaging, segmentation_key)</code>", "text": "<p>Prepare the images for a given segmentation_key.</p> <p>Parameters:</p> Name Type Description Default <code>imaging</code> <code>dj.Table</code> <p>imaging table.</p> required <code>segmentation_key</code> <code>dict</code> <p>A primary key from Segmentation table.</p> required <p>Returns:</p> Name Type Description <code>background_with_cells</code> <code>np.array</code> <p>Average image with transparently overlayed cells.</p> <code>cells_maskid_image</code> <code>np.array</code> <p>Mask ID image.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def figure_data(imaging, segmentation_key) -&gt; Tuple[np.array, np.array]:\n\"\"\"Prepare the images for a given segmentation_key.\n\n    Args:\n        imaging (dj.Table): imaging table.\n        segmentation_key (dict): A primary key from Segmentation table.\n\n    Returns:\n        background_with_cells (np.array): Average image with transparently overlayed\n            cells.\n        cells_maskid_image (np.array): Mask ID image.\n    \"\"\"\n\n    image = (imaging.MotionCorrection.Summary &amp; segmentation_key).fetch1(\n        \"average_image\"\n    )\n\n    cell_mask_ids, mask_xpix, mask_ypix = (\n        imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n        &amp; segmentation_key\n    ).fetch(\"mask\", \"mask_xpix\", \"mask_ypix\")\n\n    background_with_cells, cells_maskid_image = mask_overlayed_image(\n        image, mask_xpix, mask_ypix, cell_mask_ids, low_q=0, high_q=0.99\n    )\n\n    return background_with_cells, cells_maskid_image\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.plot_cell_overlayed_image", "title": "<code>plot_cell_overlayed_image(imaging, segmentation_key)</code>", "text": "<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>imaging</code> <code>dj.Table</code> <p>imaging table.</p> required <code>segmentation_key</code> <code>dict</code> <p>A primary key from Segmentation table.</p> required <p>Returns:</p> Name Type Description <code>image_fig</code> <code>plotly.Fig</code> <p>Plotly figure object of the average image with transparently overlayed cells.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def plot_cell_overlayed_image(imaging, segmentation_key) -&gt; go.Figure:\n\"\"\"_summary_\n\n    Args:\n        imaging (dj.Table): imaging table.\n        segmentation_key (dict): A primary key from Segmentation table.\n\n    Returns:\n        image_fig (plotly.Fig): Plotly figure object of the average image with\n            transparently overlayed cells.\n    \"\"\"\n\n    background_with_cells, cells_maskid_image = figure_data(imaging, segmentation_key)\n\n    image_fig = go.Figure(\n        go.Image(\n            z=background_with_cells,\n            hovertemplate=\"x: %{x} &lt;br&gt;y: %{y} &lt;br&gt;mask_id: %{customdata}\",\n            customdata=cells_maskid_image,\n        )\n    )\n    image_fig.update_layout(\n        title=\"Average Image with Cells\",\n        xaxis={\n            \"title\": \"X (px)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": False,\n        },\n        yaxis={\n            \"title\": \"Y (px)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": False,\n        },\n        paper_bgcolor=\"rgba(0,0,0,0)\",\n        plot_bgcolor=\"rgba(0,0,0,0)\",\n    )\n\n    return image_fig\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/cell_plot/#element_calcium_imaging.plotting.cell_plot.plot_cell_traces", "title": "<code>plot_cell_traces(imaging, cell_key)</code>", "text": "<p>Prepare plotly trace figure.</p> <p>Parameters:</p> Name Type Description Default <code>imaging</code> <code>dj.Table</code> <p>imaging table.</p> required <code>cell_key</code> <code>dict</code> <p>A primary key from imaging.Activity.Trace table.</p> required <p>Returns:</p> Name Type Description <code>trace_fig</code> <code>go.Figure</code> <p>Plotly figure object of the traces.</p> Source code in <code>element_calcium_imaging/plotting/cell_plot.py</code> <pre><code>def plot_cell_traces(imaging, cell_key) -&gt; go.Figure:\n\"\"\"Prepare plotly trace figure.\n\n    Args:\n        imaging (dj.Table): imaging table.\n        cell_key (dict): A primary key from imaging.Activity.Trace table.\n\n    Returns:\n        trace_fig: Plotly figure object of the traces.\n    \"\"\"\n    activity_trace = (\n        imaging.Activity.Trace &amp; \"extraction_method LIKE '%deconvolution'\" &amp; cell_key\n    ).fetch1(\"activity_trace\")\n    fluorescence, fps = (scan.ScanInfo * imaging.Fluorescence.Trace &amp; cell_key).fetch1(\n        \"fluorescence\", \"fps\"\n    )\n\n    trace_fig = go.Figure(\n        [\n            go.Scatter(\n                x=np.arange(len(fluorescence)) / fps,\n                y=fluorescence,\n                name=\"Fluorescence\",\n                yaxis=\"y1\",\n            ),\n            go.Scatter(\n                x=np.arange(len(activity_trace)) / fps,\n                y=activity_trace,\n                name=\"Calcium Event\",\n                yaxis=\"y2\",\n            ),\n        ]\n    )\n\n    trace_fig.update_layout(get_tracelayout(cell_key))\n\n    return trace_fig\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/widget/", "title": "widget.py", "text": ""}, {"location": "api/element_calcium_imaging/plotting/widget/#element_calcium_imaging.plotting.widget.main", "title": "<code>main(imaging, usedb=False)</code>", "text": "<p>Display the widget.</p> <p>Parameters:</p> Name Type Description Default <code>imaging</code> <code>dj.Table</code> <p>imaging table in the database.</p> required <code>usedb</code> <code>bool</code> <p>Whether to use the figures in the database or compute the figures on the fly.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>widget</code> <code>wg</code> <p>Widget to display the figures.</p> Source code in <code>element_calcium_imaging/plotting/widget.py</code> <pre><code>def main(imaging: ModuleType, usedb: bool = False) -&gt; wg:\n\"\"\"Display the widget.\n\n    Args:\n        imaging (dj.Table): imaging table in the database.\n        usedb (bool, optional): Whether to use the figures in the database or compute\n            the figures on the fly.\n\n    Returns:\n        widget: Widget to display the figures.\n    \"\"\"\n\n    motioncorrection_dropdown = wg.Dropdown(\n        options=imaging.Segmentation.fetch(\"KEY\"),\n        description=\"Result:\",\n        description_tooltip='Press \"Load\" to visualize the cells identified.',\n        disabled=False,\n        layout=wg.Layout(\n            width=\"95%\",\n            display=\"flex\",\n            flex_flow=\"row\",\n            justify_content=\"space-between\",\n            grid_area=\"motioncorrection_dropdown\",\n        ),\n        style={\"description_width\": \"80px\"},\n    )\n\n    load_button = wg.Button(\n        description=\"Load Image\",\n        tooltip=\"Load the average image.\",\n        layout=wg.Layout(width=\"120px\", grid_area=\"load_button\"),\n    )\n\n    FIG1_WIDTH = 600\n    FIG1_LAYOUT = go.Layout(\n        margin=dict(l=0, r=40, b=0, t=65, pad=0),\n        width=FIG1_WIDTH,\n        height=600,\n        transition={\"duration\": 0},\n        title={\n            \"text\": \"Average Image with Cells\",\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n            \"y\": 0.97,\n            \"x\": 0.5,\n        },\n        xaxis={\n            \"title\": \"X (px)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": False,\n        },\n        yaxis={\n            \"title\": \"Y (px)\",\n            \"visible\": True,\n            \"showticklabels\": True,\n            \"showgrid\": False,\n        },\n        paper_bgcolor=\"rgba(0,0,0,0)\",\n        plot_bgcolor=\"rgba(0,0,0,0)\",\n        modebar_remove=[\n            \"zoom\",\n            \"resetScale\",\n            \"pan\",\n            \"select\",\n            \"zoomIn\",\n            \"zoomOut\",\n            \"autoScale2d\",\n        ],\n        shapes=[\n            go.layout.Shape(\n                type=\"rect\",\n                xref=\"paper\",\n                yref=\"paper\",\n                x0=0.035,\n                y0=0,\n                x1=0.965,\n                y1=1.0,\n                line={\"width\": 1, \"color\": \"black\"},\n            )\n        ],\n    )\n    fig1 = go.Figure(\n        go.Image(\n            z=None,\n            hovertemplate=\"x: %{x} &lt;br&gt;y: %{y} &lt;br&gt;mask_id: %{customdata} &lt;extra&gt;&lt;/extra&gt;\",\n            customdata=None,\n        ),\n        layout=FIG1_LAYOUT,\n    )\n\n    FIG2_WIDTH = 600\n    FIG2_HEIGHT = 600\n    fig2_layout = cell_plot.get_tracelayout(None, width=FIG2_WIDTH, height=FIG2_HEIGHT)\n\n    fig2 = go.Figure(\n        [\n            go.Scatter(\n                x=None,\n                y=None,\n                name=\"Fluorescence\",\n                yaxis=\"y1\",\n            ),\n            go.Scatter(x=None, y=None, name=\"Calcium Event\", yaxis=\"y2\"),\n        ],\n        layout=fig2_layout,\n    )\n\n    fig1_widget = go.FigureWidget(fig1)\n    fig2_widget = go.FigureWidget(fig2)\n\n    def tooltip_click(trace, points, selector):\n        mask_id = trace.customdata[points.ys[0]][points.xs[0]]\n\n        if mask_id &gt; -1:\n            cell_traces_figobj = from_json(\n                (\n                    TraceReport &amp; motioncorrection_dropdown.value &amp; f\"mask='{mask_id}'\"\n                ).fetch1(\"cell_traces\")\n            )\n\n            with fig2_widget.batch_update():\n                fig2_widget.data[0].x = cell_traces_figobj.data[0].x\n                fig2_widget.data[0].y = cell_traces_figobj.data[0].y\n                fig2_widget.data[0].name = cell_traces_figobj.data[0].name\n                fig2_widget.data[1].x = cell_traces_figobj.data[1].x\n                fig2_widget.data[1].y = cell_traces_figobj.data[1].y\n                fig2_widget.data[1].name = cell_traces_figobj.data[1].name\n                fig2_widget.layout[\"title\"] = {\n                    \"text\": f\"Trace for Cell {mask_id}\",\n                    \"xanchor\": \"center\",\n                    \"yanchor\": \"top\",\n                    \"y\": 0.97,\n                    \"x\": 0.5,\n                }\n\n    def response(change, usedb=False):\n        if usedb:\n            cell_overlayed_image = from_json(\n                (ScanLevelReport &amp; motioncorrection_dropdown.value).fetch1(\n                    \"cell_overlayed_image\"\n                )\n            )\n\n            with fig1_widget.batch_update():\n                fig1_widget.data[0].z = cell_overlayed_image.data[0].z\n                fig1_widget.data[0].customdata = cell_overlayed_image.data[0].customdata\n\n                fig2_widget.data[0].x = None\n                fig2_widget.data[0].y = None\n                fig2_widget.data[1].x = None\n                fig2_widget.data[1].y = None\n        else:\n            background_with_cells, cells_maskid_image = cell_plot.figure_data(\n                imaging, motioncorrection_dropdown.value\n            )\n\n            with fig1_widget.batch_update():\n                fig1_widget.data[0].z = background_with_cells\n                fig1_widget.data[0].customdata = cells_maskid_image\n\n                fig2_widget.layout.title = {\n                    \"text\": \"Trace\",\n                    \"xanchor\": \"center\",\n                    \"yanchor\": \"top\",\n                    \"y\": 0.97,\n                    \"x\": 0.5,\n                }\n                fig2_widget.data[0].x = None\n                fig2_widget.data[0].y = None\n                fig2_widget.data[1].x = None\n                fig2_widget.data[1].y = None\n\n    fig1_widget.data[0].on_click(tooltip_click)\n    load_button.on_click(partial(response, usedb=usedb))\n\n    return wg.VBox(\n        [\n            wg.HBox(\n                [motioncorrection_dropdown, load_button],\n                layout=wg.Layout(width=f\"{FIG1_WIDTH+FIG2_WIDTH}px\"),\n            ),\n            wg.HBox([fig1_widget, fig2_widget]),\n        ]\n    )\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/widget/#element_calcium_imaging.plotting.widget.ScanLevelReport", "title": "<code>ScanLevelReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Scan level report with figures.</p> <p>Attributes:</p> Name Type Description <code>imaging.Segmentation</code> <code>foreign key</code> <p>Primary key from imaging.Segmentation.</p> <code>cell_overlayed_image</code> <code>longblob</code> <p>Plotly figure object showing the segmented cells on the average image.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>@schema\nclass ScanLevelReport(dj.Computed):\n\"\"\"Scan level report with figures.\n\n    Attributes:\n        imaging.Segmentation (foreign key): Primary key from imaging.Segmentation.\n        cell_overlayed_image (longblob): Plotly figure object showing the segmented\n            cells on the average image.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; imaging.Segmentation\n    ---\n    cell_overlayed_image: longblob\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n        image_fig = cell_plot.plot_cell_overlayed_image(imaging, key)\n        self.insert1({**key, \"cell_overlayed_image\": image_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/widget/#element_calcium_imaging.imaging_report.ScanLevelReport.make", "title": "<code>make(key)</code>", "text": "<p>Compute and ingest the plotly figure objects.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n    image_fig = cell_plot.plot_cell_overlayed_image(imaging, key)\n    self.insert1({**key, \"cell_overlayed_image\": image_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/widget/#element_calcium_imaging.plotting.widget.TraceReport", "title": "<code>TraceReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Figures of traces.</p> <p>Attributes:</p> Name Type Description <code>imaging.Segmentation.Mask</code> <code>foreign key</code> <p>Primary key from imaging.Segmentation.Mask.</p> <code>cell_traces</code> <code>longblob</code> <p>Plotly figure object showing the cell traces.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>@schema\nclass TraceReport(dj.Computed):\n\"\"\"Figures of traces.\n\n    Attributes:\n        imaging.Segmentation.Mask (foreign key): Primary key from\n            imaging.Segmentation.Mask.\n        cell_traces (longblob): Plotly figure object showing the cell traces.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; imaging.Segmentation.Mask\n    ---\n    cell_traces: longblob\n    \"\"\"\n\n    @property\n    def key_source(self):\n\"\"\"Limit the TraceReport to Masks that have Activity table populated.\n        database.\"\"\"\n\n        return imaging.Segmentation.Mask &amp; imaging.Activity\n\n    def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n        trace_fig = cell_plot.plot_cell_traces(imaging, key)\n        self.insert1({**key, \"cell_traces\": trace_fig.to_json()})\n</code></pre>"}, {"location": "api/element_calcium_imaging/plotting/widget/#element_calcium_imaging.imaging_report.TraceReport.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limit the TraceReport to Masks that have Activity table populated. database.</p>"}, {"location": "api/element_calcium_imaging/plotting/widget/#element_calcium_imaging.imaging_report.TraceReport.make", "title": "<code>make(key)</code>", "text": "<p>Compute and ingest the plotly figure objects.</p> Source code in <code>element_calcium_imaging/imaging_report.py</code> <pre><code>def make(self, key):\n\"\"\"Compute and ingest the plotly figure objects.\"\"\"\n\n    trace_fig = cell_plot.plot_cell_traces(imaging, key)\n    self.insert1({**key, \"cell_traces\": trace_fig.to_json()})\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/", "title": "analysis.py", "text": ""}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.activate", "title": "<code>activate(schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>subject</code> element.</p> required <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>subject</code> element: Upstream schema: scan, session, trial.</p> <code>None</code> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>def activate(\n    schema_name, *, create_schema=True, create_tables=True, linking_module=None\n):\n\"\"\"Activate this schema.\n\n    Args:\n        schema_name (str): Schema name on the database server to activate the `subject`\n            element.\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if they\n            do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `subject` element: Upstream schema: scan,\n            session, trial.\n    \"\"\"\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(linking_module), (\n        \"The argument 'dependency' must \" + \"be a module's name or a module\"\n    )\n\n    global _linking_module\n    _linking_module = linking_module\n\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignmentCondition", "title": "<code>ActivityAlignmentCondition</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Activity alignment condition.</p> <p>Attributes:</p> Name Type Description <code>imaging.Activity</code> <code>foreign key</code> <p>Primary key from imaging.Activity.</p> <code>event.AlignmentEvent</code> <code>foreign key</code> <p>Primary key from event.AlignmentEvent.</p> <code>trial_condition</code> <code>str</code> <p>User-friendly name of condition.</p> <code>bin_size</code> <code>float</code> <p>bin-size (in second) used to compute the PSTH,</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>@schema\nclass ActivityAlignmentCondition(dj.Manual):\n\"\"\"Activity alignment condition.\n\n    Attributes:\n        imaging.Activity (foreign key): Primary key from imaging.Activity.\n        event.AlignmentEvent (foreign key): Primary key from event.AlignmentEvent.\n        trial_condition (str): User-friendly name of condition.\n        condition_description (str). Optional. Description. Default is ''.\n        bin_size (float): bin-size (in second) used to compute the PSTH,\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; imaging.Activity\n    -&gt; event.AlignmentEvent\n    trial_condition: varchar(128) # user-friendly name of condition\n    ---\n    condition_description='': varchar(1000)\n    bin_size=0.04: float # bin-size (in second) used to compute the PSTH\n    \"\"\"\n\n    class Trial(dj.Part):\n\"\"\"Trial\n\n        Attributes:\n            ActivityAlignmentCondition (foreign key): Primary key from\n                ActivityAlignmentCondition.\n            trial.Trial: Primary key from trial.Trial.\n        \"\"\"\n\n        definition = \"\"\"  # Trials (or subset) to compute event-aligned activity\n        -&gt; master\n        -&gt; trial.Trial\n        \"\"\"\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignmentCondition.Trial", "title": "<code>Trial</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trial</p> <p>Attributes:</p> Name Type Description <code>ActivityAlignmentCondition</code> <code>foreign key</code> <p>Primary key from ActivityAlignmentCondition.</p> <code>trial.Trial</code> <code>foreign key</code> <p>Primary key from trial.Trial.</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>class Trial(dj.Part):\n\"\"\"Trial\n\n    Attributes:\n        ActivityAlignmentCondition (foreign key): Primary key from\n            ActivityAlignmentCondition.\n        trial.Trial: Primary key from trial.Trial.\n    \"\"\"\n\n    definition = \"\"\"  # Trials (or subset) to compute event-aligned activity\n    -&gt; master\n    -&gt; trial.Trial\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment", "title": "<code>ActivityAlignment</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Attributes:</p> Name Type Description <code>ActivityAlignmentCondition</code> <code>foreign key</code> <p>Primary key from ActivityAlignmentCondition.</p> <code>aligned_timestamps</code> <code>longblob</code> <p>Aligned timestamps.</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>@schema\nclass ActivityAlignment(dj.Computed):\n\"\"\"\n    Attributes:\n        ActivityAlignmentCondition (foreign key): Primary key from\n            ActivityAlignmentCondition.\n        aligned_timestamps (longblob): Aligned timestamps.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ActivityAlignmentCondition\n    ---\n    aligned_timestamps: longblob\n    \"\"\"\n\n    class AlignedTrialActivity(dj.Part):\n\"\"\"Aligned trial activity.\n\n        Attributes:\n            ActivityAlignment (foreign key): Primary key from ActivityAlignment.\n            imaging.Activity.Trace (foreign key): Primary key from\n                imaging.Activity.Trace.\n            ActivityAlignmentCondition.Trial (foreign key): Primary key from\n                ActivityAlignmentCondition.Trial.\n            aligned_trace (longblob): Calcium activity aligned to the event time (s).\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; imaging.Activity.Trace\n        -&gt; ActivityAlignmentCondition.Trial\n        ---\n        aligned_trace: longblob  # (s) Calcium activity aligned to the event time\n        \"\"\"\n\n    def make(self, key):\n        sess_time, scan_time, nframes, frame_rate = (\n            _linking_module.scan.ScanInfo * _linking_module.session.Session &amp; key\n        ).fetch1(\"session_datetime\", \"scan_datetime\", \"nframes\", \"fps\")\n\n        trialized_event_times = (\n            _linking_module.trial.get_trialized_alignment_event_times(\n                key,\n                _linking_module.trial.Trial &amp; (ActivityAlignmentCondition.Trial &amp; key),\n            )\n        )\n\n        min_limit = (trialized_event_times.event - trialized_event_times.start).max()\n        max_limit = (trialized_event_times.end - trialized_event_times.event).max()\n\n        aligned_timestamps = np.arange(-min_limit, max_limit, 1 / frame_rate)\n        nsamples = len(aligned_timestamps)\n\n        trace_keys, activity_traces = (\n            _linking_module.imaging.Activity.Trace &amp; key\n        ).fetch(\"KEY\", \"activity_trace\", order_by=\"mask\")\n        activity_traces = np.vstack(activity_traces)\n\n        aligned_trial_activities = []\n        for _, r in trialized_event_times.iterrows():\n            if r.event is None or np.isnan(r.event):\n                continue\n            alignment_start_idx = int((r.event - min_limit) * frame_rate)\n            roi_aligned_activities = activity_traces[\n                :, alignment_start_idx : (alignment_start_idx + nsamples)\n            ]\n            if roi_aligned_activities.shape[-1] != nsamples:\n                shape_diff = nsamples - roi_aligned_activities.shape[-1]\n                roi_aligned_activities = np.pad(\n                    roi_aligned_activities,\n                    ((0, 0), (0, shape_diff)),\n                    mode=\"constant\",\n                    constant_values=np.nan,\n                )\n\n            aligned_trial_activities.extend(\n                [\n                    {**key, **r.trial_key, **trace_key, \"aligned_trace\": aligned_trace}\n                    for trace_key, aligned_trace in zip(\n                        trace_keys, roi_aligned_activities\n                    )\n                ]\n            )\n\n        self.insert1({**key, \"aligned_timestamps\": aligned_timestamps})\n        self.AlignedTrialActivity.insert(aligned_trial_activities)\n\n    def plot_aligned_activities(self, key, roi, axs=None, title=None):\n\"\"\"Plot event-aligned activities for selected trials, and trial-averaged\n            activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.).\n\n        Args:\n            key (dict): key of ActivityAlignment master table\n            roi (int): imaging segmentation mask\n            axs (matplotlib.ax): optional definition of axes for plot.\n                Default is plt.subplots(2, 1, figsize=(12, 8))\n            title (str): Optional title label\n\n        Returns:\n            fig (matplotlib.pyplot.figure): Figure of the event aligned activities.\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        fig = None\n        if axs is None:\n            fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(12, 8))\n        else:\n            ax0, ax1 = axs\n\n        aligned_timestamps = (self &amp; key).fetch1(\"aligned_timestamps\")\n        trial_ids, aligned_spikes = (\n            self.AlignedTrialActivity &amp; key &amp; {\"mask\": roi}\n        ).fetch(\"trial_id\", \"aligned_trace\", order_by=\"trial_id\")\n\n        aligned_spikes = np.vstack(aligned_spikes)\n\n        ax0.imshow(\n            aligned_spikes,\n            cmap=\"inferno\",\n            interpolation=\"nearest\",\n            aspect=\"auto\",\n            extent=(\n                aligned_timestamps[0],\n                aligned_timestamps[-1],\n                0,\n                aligned_spikes.shape[0],\n            ),\n        )\n        ax0.axvline(x=0, linestyle=\"--\", color=\"white\")\n        ax0.set_axis_off()\n\n        ax1.plot(aligned_timestamps, np.nanmean(aligned_spikes, axis=0))\n        ax1.axvline(x=0, linestyle=\"--\", color=\"black\")\n        ax1.set_xlabel(\"Time (s)\")\n        ax1.set_xlim(aligned_timestamps[0], aligned_timestamps[-1])\n\n        if title:\n            plt.suptitle(title)\n\n        return fig\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment.AlignedTrialActivity", "title": "<code>AlignedTrialActivity</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Aligned trial activity.</p> <p>Attributes:</p> Name Type Description <code>ActivityAlignment</code> <code>foreign key</code> <p>Primary key from ActivityAlignment.</p> <code>imaging.Activity.Trace</code> <code>foreign key</code> <p>Primary key from imaging.Activity.Trace.</p> <code>ActivityAlignmentCondition.Trial</code> <code>foreign key</code> <p>Primary key from ActivityAlignmentCondition.Trial.</p> <code>aligned_trace</code> <code>longblob</code> <p>Calcium activity aligned to the event time (s).</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>class AlignedTrialActivity(dj.Part):\n\"\"\"Aligned trial activity.\n\n    Attributes:\n        ActivityAlignment (foreign key): Primary key from ActivityAlignment.\n        imaging.Activity.Trace (foreign key): Primary key from\n            imaging.Activity.Trace.\n        ActivityAlignmentCondition.Trial (foreign key): Primary key from\n            ActivityAlignmentCondition.Trial.\n        aligned_trace (longblob): Calcium activity aligned to the event time (s).\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; imaging.Activity.Trace\n    -&gt; ActivityAlignmentCondition.Trial\n    ---\n    aligned_trace: longblob  # (s) Calcium activity aligned to the event time\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/analysis/#workflow_calcium_imaging.analysis.ActivityAlignment.plot_aligned_activities", "title": "<code>plot_aligned_activities(key, roi, axs=None, title=None)</code>", "text": "<p>Plot event-aligned activities for selected trials, and trial-averaged     activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key of ActivityAlignment master table</p> required <code>roi</code> <code>int</code> <p>imaging segmentation mask</p> required <code>axs</code> <code>matplotlib.ax</code> <p>optional definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8))</p> <code>None</code> <code>title</code> <code>str</code> <p>Optional title label</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.pyplot.figure</code> <p>Figure of the event aligned activities.</p> Source code in <code>workflow_calcium_imaging/analysis.py</code> <pre><code>def plot_aligned_activities(self, key, roi, axs=None, title=None):\n\"\"\"Plot event-aligned activities for selected trials, and trial-averaged\n        activity (e.g. dF/F, neuropil-corrected dF/F, Calcium events, etc.).\n\n    Args:\n        key (dict): key of ActivityAlignment master table\n        roi (int): imaging segmentation mask\n        axs (matplotlib.ax): optional definition of axes for plot.\n            Default is plt.subplots(2, 1, figsize=(12, 8))\n        title (str): Optional title label\n\n    Returns:\n        fig (matplotlib.pyplot.figure): Figure of the event aligned activities.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    fig = None\n    if axs is None:\n        fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(12, 8))\n    else:\n        ax0, ax1 = axs\n\n    aligned_timestamps = (self &amp; key).fetch1(\"aligned_timestamps\")\n    trial_ids, aligned_spikes = (\n        self.AlignedTrialActivity &amp; key &amp; {\"mask\": roi}\n    ).fetch(\"trial_id\", \"aligned_trace\", order_by=\"trial_id\")\n\n    aligned_spikes = np.vstack(aligned_spikes)\n\n    ax0.imshow(\n        aligned_spikes,\n        cmap=\"inferno\",\n        interpolation=\"nearest\",\n        aspect=\"auto\",\n        extent=(\n            aligned_timestamps[0],\n            aligned_timestamps[-1],\n            0,\n            aligned_spikes.shape[0],\n        ),\n    )\n    ax0.axvline(x=0, linestyle=\"--\", color=\"white\")\n    ax0.set_axis_off()\n\n    ax1.plot(aligned_timestamps, np.nanmean(aligned_spikes, axis=0))\n    ax1.axvline(x=0, linestyle=\"--\", color=\"black\")\n    ax1.set_xlabel(\"Time (s)\")\n    ax1.set_xlim(aligned_timestamps[0], aligned_timestamps[-1])\n\n    if title:\n        plt.suptitle(title)\n\n    return fig\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/", "title": "ingest.py", "text": ""}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Retrieve imaging root data directory.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_imaging_root_data_dir():\n\"\"\"Retrieve imaging root data directory.\"\"\"\n    imaging_root_dirs = dj.config.get(\"custom\", {}).get(\"imaging_root_data_dir\", None)\n    if not imaging_root_dirs:\n        return None\n    elif not isinstance(imaging_root_dirs, abc.Sequence):\n        return list(imaging_root_dirs)\n    else:\n        return imaging_root_dirs\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.Equipment", "title": "<code>Equipment</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Equipment</p> <p>Attributes:</p> Name Type Description <code>scanner</code> <code>str</code> <p>Scanner used in imaging.</p> Source code in <code>workflow_calcium_imaging/reference.py</code> <pre><code>@schema\nclass Equipment(dj.Manual):\n\"\"\"Equipment\n\n    Attributes:\n        scanner (str): Scanner used in imaging.\n    \"\"\"\n\n    definition = \"\"\"\n    scanner: varchar(32)\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_subjects", "title": "<code>ingest_subjects(subject_csv_path='./user_data/subjects.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Inserts ./user_data/subject.csv data into corresponding subject schema tables.</p> <p>Parameters:</p> Name Type Description Default <code>subject_csv_path</code> <code>str</code> <p>relative path of subject csv.</p> <code>'./user_data/subjects.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Display number of entries inserted when ingesting.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/ingest.py</code> <pre><code>def ingest_subjects(\n    subject_csv_path: str = \"./user_data/subjects.csv\",\n    skip_duplicates: bool = True,\n    verbose: bool = True,\n):\n\"\"\"Inserts ./user_data/subject.csv data into corresponding subject schema tables.\n\n    Args:\n        subject_csv_path (str): relative path of subject csv.\n        skip_duplicates (bool): Default True. Passed to DataJoint insert.\n        verbose (bool): Display number of entries inserted when ingesting.\n    \"\"\"\n    csvs = [subject_csv_path]\n    tables = [subject.Subject()]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_sessions", "title": "<code>ingest_sessions(session_csv_path='./user_data/sessions.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingests all the manual table starting from session schema from ./user_data/sessions.csv.</p> <p>Parameters:</p> Name Type Description Default <code>session_csv_path</code> <code>str</code> <p>relative path of session csv.</p> <code>'./user_data/sessions.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Default True. Display number of entries inserted when ingesting.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/ingest.py</code> <pre><code>def ingest_sessions(\n    session_csv_path=\"./user_data/sessions.csv\", skip_duplicates=True, verbose=True\n):\n\"\"\"Ingests all the manual table starting from session schema from\n    ./user_data/sessions.csv.\n\n    Args:\n        session_csv_path (str): relative path of session csv.\n        skip_duplicates (bool): Default True. Passed to DataJoint insert.\n        verbose (bool): Default True. Display number of entries inserted when ingesting.\n    \"\"\"\n    root_data_dirs = get_imaging_root_data_dir()\n\n    # ---------- Insert new \"Session\" and \"Scan\" ---------\n    with open(session_csv_path, newline=\"\") as f:\n        input_sessions = list(csv.DictReader(f, delimiter=\",\"))\n\n    # Folder structure: root / subject / session / .tif (raw)\n    session_list, session_dir_list, scan_list, scanner_list = [], [], [], []\n\n    for sess in input_sessions:\n        sess_dir = find_full_path(root_data_dirs, Path(sess[\"session_dir\"]))\n        root_data_dir = find_root_directory(root_data_dirs, sess_dir)\n\n        # search for either ScanImage or Scanbox files (in that order)\n        for scan_pattern, scan_type, glob_func in zip(\n            [\"*.tif\", \"*.sbx\"],\n            [\"ScanImage\", \"Scanbox\"],\n            [sess_dir.glob, sess_dir.rglob],\n        ):\n            scan_filepaths = [fp.as_posix() for fp in glob_func(scan_pattern)]\n            if len(scan_filepaths):\n                acq_software = scan_type\n                break\n        else:\n            raise FileNotFoundError(\n                \"Unable to identify scan files from the supported \"\n                + \"acquisition softwares (ScanImage, Scanbox) at: \"\n                + f\"{sess_dir}\"\n            )\n\n        if acq_software == \"ScanImage\":\n            import scanreader\n            from element_interface import scanimage_utils\n\n            try:  # attempt to read .tif as a scanimage file\n                loaded_scan = scanreader.read_scan(scan_filepaths)\n                recording_time = scanimage_utils.get_scanimage_acq_time(loaded_scan)\n                header = scanimage_utils.parse_scanimage_header(loaded_scan)\n                scanner = header[\"SI_imagingSystem\"].strip(\"'\")\n            except Exception as e:\n                print(f\"ScanImage loading error: {scan_filepaths}\\n{str(e)}\")\n                continue\n        elif acq_software == \"Scanbox\":\n            import sbxreader\n\n            try:  # attempt to load Scanbox\n                sbx_fp = pathlib.Path(scan_filepaths[0])\n                sbx_meta = sbxreader.sbx_get_metadata(sbx_fp)\n                # read from file when Scanbox support this\n                recording_time = datetime.fromtimestamp(sbx_fp.stat().st_ctime)\n                scanner = sbx_meta.get(\"imaging_system\", \"Scanbox\")\n            except Exception as e:\n                print(f\"Scanbox loading error: {scan_filepaths}\\n{str(e)}\")\n                continue\n        else:\n            raise NotImplementedError(\n                \"Processing scan from acquisition software of \"\n                + f\"type {acq_software} is not yet implemented\"\n            )\n\n        session_key = {\"subject\": sess[\"subject\"], \"session_datetime\": recording_time}\n        if session_key not in session.Session():\n            scanner_list.append({\"scanner\": scanner})\n            session_list.append(session_key)\n            scan_list.append(\n                {\n                    **session_key,\n                    \"scan_id\": 0,\n                    \"scanner\": scanner,\n                    \"acq_software\": acq_software,\n                }\n            )\n\n            session_dir_list.append(\n                {\n                    **session_key,\n                    \"session_dir\": sess_dir.relative_to(root_data_dir).as_posix(),\n                }\n            )\n    new_equipment = set(val for dic in scanner_list for val in dic.values())\n    if verbose:\n        print(\n            f\"\\n---- Insert {len(new_equipment)} entry(s) into \"\n            + \"experiment.Equipment ----\"\n        )\n    Equipment.insert(scanner_list, skip_duplicates=skip_duplicates)\n\n    if verbose:\n        print(f\"\\n---- Insert {len(session_list)} entry(s) into session.Session ----\")\n    session.Session.insert(session_list, skip_duplicates=skip_duplicates)\n    session.SessionDirectory.insert(session_dir_list, skip_duplicates=skip_duplicates)\n\n    if verbose:\n        print(f\"\\n---- Insert {len(scan_list)} entry(s) into scan.Scan ----\")\n    scan.Scan.insert(scan_list, skip_duplicates=skip_duplicates)\n\n    if verbose:\n        print(\"\\n---- Successfully completed ingest_sessions ----\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_events", "title": "<code>ingest_events(recording_csv_path='./user_data/behavior_recordings.csv', block_csv_path='./user_data/blocks.csv', trial_csv_path='./user_data/trials.csv', event_csv_path='./user_data/events.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingest session, block, trial, and event data.</p> <p>Ingest each level of experiment hierarchy for element-trial: recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurrences within trial).</p> <p>This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging.</p> <p>Parameters:</p> Name Type Description Default <code>recording_csv_path</code> <code>str</code> <p>relative path of behavior_recordings.csv.</p> <code>'./user_data/behavior_recordings.csv'</code> <code>block_csv_path</code> <code>str</code> <p>relative path of blocks.csv.</p> <code>'./user_data/blocks.csv'</code> <code>trial_csv_path</code> <code>str</code> <p>relative path of trials.csv.</p> <code>'./user_data/trials.csv'</code> <code>event_csv_path</code> <code>str</code> <p>relative path of events.csv.</p> <code>'./user_data/events.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Display number of entries inserted when ingesting. Default True.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/ingest.py</code> <pre><code>def ingest_events(\n    recording_csv_path=\"./user_data/behavior_recordings.csv\",\n    block_csv_path=\"./user_data/blocks.csv\",\n    trial_csv_path=\"./user_data/trials.csv\",\n    event_csv_path=\"./user_data/events.csv\",\n    skip_duplicates=True,\n    verbose=True,\n):\n\"\"\"\n    Ingest session, block, trial, and event data.\n\n    Ingest each level of experiment hierarchy for element-trial: recording, block (i.e.,\n    phases of trials), trials (repeated units), events (optionally 0-duration occurrences\n    within trial).\n\n    This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging.\n\n    Args:\n        recording_csv_path (str, optional): relative path of behavior_recordings.csv.\n        block_csv_path (str, optional): relative path of blocks.csv.\n        trial_csv_path (str, optional): relative path of trials.csv.\n        event_csv_path (str, optional): relative path of events.csv.\n        skip_duplicates (bool, optional): Default True. Passed to DataJoint insert.\n        verbose (bool, optional): Display number of entries inserted when ingesting.\n            Default True.\n    \"\"\"\n    csvs = [\n        recording_csv_path,\n        recording_csv_path,\n        block_csv_path,\n        block_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        event_csv_path,\n        event_csv_path,\n        event_csv_path,\n    ]\n    tables = [\n        event.BehaviorRecording(),\n        event.BehaviorRecording.File(),\n        trial.Block(),\n        trial.Block.Attribute(),\n        trial.TrialType(),\n        trial.Trial(),\n        trial.Trial.Attribute(),\n        trial.BlockTrial(),\n        event.EventType(),\n        event.Event(),\n        trial.TrialEvent(),\n    ]\n\n    # Allow direct insert required bc element-trial has Imported that should be Manual\n    ingest_csv_to_table(\n        csvs,\n        tables,\n        skip_duplicates=skip_duplicates,\n        verbose=verbose,\n        allow_direct_insert=True,\n    )\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/ingest/#workflow_calcium_imaging.ingest.ingest_alignment", "title": "<code>ingest_alignment(alignment_csv_path='./user_data/alignments.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingest event alignment information</p> <p>This is duplicated across wf-array-ephys and wf-calcium-imaging.</p> <p>Parameters:</p> Name Type Description Default <code>alignment_csv_path</code> <code>str</code> <p>relative path of alignments.csv</p> <code>'./user_data/alignments.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Display number of entries inserted when ingesting. Default True.</p> <code>True</code> Source code in <code>workflow_calcium_imaging/ingest.py</code> <pre><code>def ingest_alignment(\n    alignment_csv_path=\"./user_data/alignments.csv\", skip_duplicates=True, verbose=True\n):\n\"\"\"Ingest event alignment information\n\n    This is duplicated across wf-array-ephys and wf-calcium-imaging.\n\n    Args:\n        alignment_csv_path (str): relative path of alignments.csv\n        skip_duplicates (bool, optional): Default True. Passed to DataJoint insert.\n        verbose (bool, optional): Display number of entries inserted when ingesting.\n            Default True.\n    \"\"\"\n\n    csvs = [alignment_csv_path]\n    tables = [event.AlignmentEvent()]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/", "title": "paths.py", "text": ""}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Retrieve imaging root data directory.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_imaging_root_data_dir():\n\"\"\"Retrieve imaging root data directory.\"\"\"\n    imaging_root_dirs = dj.config.get(\"custom\", {}).get(\"imaging_root_data_dir\", None)\n    if not imaging_root_dirs:\n        return None\n    elif not isinstance(imaging_root_dirs, abc.Sequence):\n        return list(imaging_root_dirs)\n    else:\n        return imaging_root_dirs\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_scan_image_files", "title": "<code>get_scan_image_files(scan_key)</code>", "text": "<p>Retrieve the list of ScanImage files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or tiff files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_scan_image_files(scan_key):\n\"\"\"Retrieve the list of ScanImage files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or tiff files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .tif (raw)\n    sess_dir, tiff_filepaths = _find_files_by_type(scan_key, \"*.tif\")\n    if tiff_filepaths:\n        return tiff_filepaths\n    else:\n        raise FileNotFoundError(f\"No tiff file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_scan_box_files", "title": "<code>get_scan_box_files(scan_key)</code>", "text": "<p>Retrieve the list of Scanbox files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or scanbox files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_scan_box_files(scan_key):\n\"\"\"Retrieve the list of Scanbox files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or scanbox files are not found.\n    \"\"\"\n\n    # Folder structure: root / subject / session / .sbx\n    sess_dir, sbx_filepaths = _find_files_by_type(scan_key, \"*.sbx\")\n    if sbx_filepaths:\n        return sbx_filepaths\n    else:\n        raise FileNotFoundError(f\"No .sbx file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_nd2_files", "title": "<code>get_nd2_files(scan_key)</code>", "text": "<p>Retrieve the list of Nikon files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or nd2 files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_nd2_files(scan_key):\n\"\"\"Retrieve the list of Nikon files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or nd2 files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .nd2\n    sess_dir, nd2_filepaths = _find_files_by_type(scan_key, \"*.nd2\")\n    if nd2_filepaths:\n        return nd2_filepaths\n    else:\n        raise FileNotFoundError(f\"No .nd2 file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/paths/#workflow_calcium_imaging.paths.get_prairieview_files", "title": "<code>get_prairieview_files(scan_key)</code>", "text": "<p>Retrieve the list of PrairieView files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or tiff files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_prairieview_files(scan_key):\n\"\"\"Retrieve the list of PrairieView files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or tiff files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .tif\n    sess_dir, pv_filepaths = _find_files_by_type(scan_key, \"*.tif\")\n    if pv_filepaths:\n        return pv_filepaths\n    else:\n        raise FileNotFoundError(f\"No .tif file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/pipeline/", "title": "pipeline.py", "text": ""}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_imaging_root_data_dir", "title": "<code>get_imaging_root_data_dir()</code>", "text": "<p>Retrieve imaging root data directory.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_imaging_root_data_dir():\n\"\"\"Retrieve imaging root data directory.\"\"\"\n    imaging_root_dirs = dj.config.get(\"custom\", {}).get(\"imaging_root_data_dir\", None)\n    if not imaging_root_dirs:\n        return None\n    elif not isinstance(imaging_root_dirs, abc.Sequence):\n        return list(imaging_root_dirs)\n    else:\n        return imaging_root_dirs\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.Equipment", "title": "<code>Equipment</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Equipment</p> <p>Attributes:</p> Name Type Description <code>scanner</code> <code>str</code> <p>Scanner used in imaging.</p> Source code in <code>workflow_calcium_imaging/reference.py</code> <pre><code>@schema\nclass Equipment(dj.Manual):\n\"\"\"Equipment\n\n    Attributes:\n        scanner (str): Scanner used in imaging.\n    \"\"\"\n\n    definition = \"\"\"\n    scanner: varchar(32)\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_scan_image_files", "title": "<code>get_scan_image_files(scan_key)</code>", "text": "<p>Retrieve the list of ScanImage files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or tiff files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_scan_image_files(scan_key):\n\"\"\"Retrieve the list of ScanImage files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or tiff files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .tif (raw)\n    sess_dir, tiff_filepaths = _find_files_by_type(scan_key, \"*.tif\")\n    if tiff_filepaths:\n        return tiff_filepaths\n    else:\n        raise FileNotFoundError(f\"No tiff file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_scan_box_files", "title": "<code>get_scan_box_files(scan_key)</code>", "text": "<p>Retrieve the list of Scanbox files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or scanbox files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_scan_box_files(scan_key):\n\"\"\"Retrieve the list of Scanbox files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or scanbox files are not found.\n    \"\"\"\n\n    # Folder structure: root / subject / session / .sbx\n    sess_dir, sbx_filepaths = _find_files_by_type(scan_key, \"*.sbx\")\n    if sbx_filepaths:\n        return sbx_filepaths\n    else:\n        raise FileNotFoundError(f\"No .sbx file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_nd2_files", "title": "<code>get_nd2_files(scan_key)</code>", "text": "<p>Retrieve the list of Nikon files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or nd2 files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_nd2_files(scan_key):\n\"\"\"Retrieve the list of Nikon files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or nd2 files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .nd2\n    sess_dir, nd2_filepaths = _find_files_by_type(scan_key, \"*.nd2\")\n    if nd2_filepaths:\n        return nd2_filepaths\n    else:\n        raise FileNotFoundError(f\"No .nd2 file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/pipeline/#workflow_calcium_imaging.pipeline.get_prairieview_files", "title": "<code>get_prairieview_files(scan_key)</code>", "text": "<p>Retrieve the list of PrairieView files associated with a given Scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key from Scan.</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>list</code> <p>Absolute path(s) of the scan files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the session directory or tiff files are not found.</p> Source code in <code>workflow_calcium_imaging/paths.py</code> <pre><code>def get_prairieview_files(scan_key):\n\"\"\"Retrieve the list of PrairieView files associated with a given Scan.\n\n    Args:\n        scan_key (dict): Primary key from Scan.\n\n    Returns:\n        path (list): Absolute path(s) of the scan files.\n\n    Raises:\n        FileNotFoundError: If the session directory or tiff files are not found.\n    \"\"\"\n    # Folder structure: root / subject / session / .tif\n    sess_dir, pv_filepaths = _find_files_by_type(scan_key, \"*.tif\")\n    if pv_filepaths:\n        return pv_filepaths\n    else:\n        raise FileNotFoundError(f\"No .tif file found in {sess_dir}\")\n</code></pre>"}, {"location": "api/workflow_calcium_imaging/reference/", "title": "reference.py", "text": ""}, {"location": "api/workflow_calcium_imaging/reference/#workflow_calcium_imaging.reference.Equipment", "title": "<code>Equipment</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Equipment</p> <p>Attributes:</p> Name Type Description <code>scanner</code> <code>str</code> <p>Scanner used in imaging.</p> Source code in <code>workflow_calcium_imaging/reference.py</code> <pre><code>@schema\nclass Equipment(dj.Manual):\n\"\"\"Equipment\n\n    Attributes:\n        scanner (str): Scanner used in imaging.\n    \"\"\"\n\n    definition = \"\"\"\n    scanner: varchar(32)\n    \"\"\"\n</code></pre>"}, {"location": "tutorials/", "title": "Tutorials", "text": "<ul> <li>DataJoint Elements are modular pipelines that can be connected into a complete workflow.  Workflow Calcium Imaging is an example that combines five DataJoint Elements - Lab, Animal, Session, Event, and Calcium Imaging.</li> </ul> <ul> <li>Workflow Calcium Imaging includes an interactive tutorial on GitHub Codespaces, which is configured for users to run the pipeline.</li> </ul> <ul> <li>In the interactive tutorial, the example notebooks describe the pipeline and provide instructions for running the pipeline.  For convenience, these notebooks are also rendered on this website:<ul> <li>Tutorial notebook</li> <li>Quality metrics notebook</li> </ul> </li> </ul>"}, {"location": "tutorials/#installation-instructions-for-active-projects", "title": "Installation Instructions for Active Projects", "text": "<ul> <li>The Workflow Calcium Imaging described above can be modified for a user's specific experimental requirements and thereby used in active projects.  </li> </ul> <ul> <li>The GitHub Codespace and Dev Container is configured for tutorials and prototyping. We recommend users to configure a database specifically for production pipelines.  Instructions for a local installation of the integrated development environment with a database can be found on the User Guide page.</li> </ul>"}, {"location": "tutorials/#videos", "title": "Videos", "text": "<ul> <li>The YouTube tutorial gives an overview  of the workflow files and notebooks, as well as core concepts related to calcium imaging analysis.</li> </ul>"}, {"location": "tutorials/#extract", "title": "EXTRACT", "text": "<ul> <li>Analysis with the EXTRACT package is currently supported for single channel, single plane scans using Suite2p for motion correction. For processing with EXTRACT, please set <code>processing_method=\"extract\"</code> in the ProcessingParamSet table, and provide the <code>params</code> attribute of the ProcessingParamSet table in the <code>{'suite2p': {...}, 'extract': {...}}</code> dictionary format. Please also install the MATLAB engine API for Python.</li> </ul>"}, {"location": "tutorials/demo_prepare/", "title": "Demo prepare", "text": "In\u00a0[\u00a0]: Copied! <pre># Runs in about 2m\nimport datajoint as dj\nimport datetime\nimport numpy as np\nfrom workflow_calcium_imaging.pipeline import subject, session, scan, imaging, Equipment\nfrom element_calcium_imaging import imaging_report\nimport suite2p\n</pre> # Runs in about 2m import datajoint as dj import datetime import numpy as np from workflow_calcium_imaging.pipeline import subject, session, scan, imaging, Equipment from element_calcium_imaging import imaging_report import suite2p In\u00a0[\u00a0]: Copied! <pre>subject.Subject.insert1(\n    dict(\n        subject='subject1',\n        subject_birth_date='2023-01-01',\n        sex='U',\n    )\n)\n</pre> subject.Subject.insert1(     dict(         subject='subject1',         subject_birth_date='2023-01-01',         sex='U',     ) ) In\u00a0[\u00a0]: Copied! <pre>Equipment.insert1(dict(scanner=\"Mesoscope\"))\n</pre> Equipment.insert1(dict(scanner=\"Mesoscope\")) In\u00a0[\u00a0]: Copied! <pre>session_key = dict(subject='subject1', \n                   session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00))\n\nsession.Session.insert1(session_key)\n\nsession.SessionDirectory.insert1(\n    dict(\n        session_key, \n        session_dir='subject1/session1'\n    )\n)\n</pre> session_key = dict(subject='subject1',                     session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00))  session.Session.insert1(session_key)  session.SessionDirectory.insert1(     dict(         session_key,          session_dir='subject1/session1'     ) ) In\u00a0[\u00a0]: Copied! <pre>scan.Scan.insert1(\n    dict(\n        session_key,\n        scan_id=0,\n        scanner=\"Mesoscope\",\n        acq_software='ScanImage',\n    )\n)\n</pre> scan.Scan.insert1(     dict(         session_key,         scan_id=0,         scanner=\"Mesoscope\",         acq_software='ScanImage',     ) ) In\u00a0[\u00a0]: Copied! <pre># Runs in about 35s\npopulate_settings = {\"display_progress\": True}\n\nscan.ScanInfo.populate(**populate_settings)\n</pre> # Runs in about 35s populate_settings = {\"display_progress\": True}  scan.ScanInfo.populate(**populate_settings) In\u00a0[\u00a0]: Copied! <pre>suite2p_params = suite2p.default_ops()\nsuite2p_params['nonrigid']=False\n\nimaging.ProcessingParamSet.insert_new_params(\n    processing_method=\"suite2p\",\n    paramset_idx=0,\n    params=suite2p_params,\n    paramset_desc='Default parameter set for suite2p'\n)\n</pre> suite2p_params = suite2p.default_ops() suite2p_params['nonrigid']=False  imaging.ProcessingParamSet.insert_new_params(     processing_method=\"suite2p\",     paramset_idx=0,     params=suite2p_params,     paramset_desc='Default parameter set for suite2p' ) In\u00a0[\u00a0]: Copied! <pre>imaging.ProcessingTask.insert1(\n    dict(\n        session_key,\n        scan_id=0,\n        paramset_idx=0,\n        task_mode='load', # load or trigger\n        processing_output_dir='subject1/session1/suite2p',\n    )\n)\n\nimaging.Processing.populate(**populate_settings)\n</pre> imaging.ProcessingTask.insert1(     dict(         session_key,         scan_id=0,         paramset_idx=0,         task_mode='load', # load or trigger         processing_output_dir='subject1/session1/suite2p',     ) )  imaging.Processing.populate(**populate_settings) In\u00a0[\u00a0]: Copied! <pre># Runs in about 3m\nimaging.Curation.insert1(\n    dict(\n        session_key,\n        scan_id=0,\n        paramset_idx=0,\n        curation_id=0,\n        curation_time=datetime.datetime(2023, 5, 11, 12, 00, 00),\n        curation_output_dir='subject1/session1/suite2p',\n        manual_curation=False,\n    )\n)\n\nimaging.MotionCorrection.populate(**populate_settings)\nimaging.Segmentation.populate(**populate_settings)\nimaging.Fluorescence.populate(**populate_settings)\nimaging.Activity.populate(**populate_settings)\nimaging_report.ScanLevelReport.populate(**populate_settings)\nimaging_report.TraceReport.populate(**populate_settings)\n</pre> # Runs in about 3m imaging.Curation.insert1(     dict(         session_key,         scan_id=0,         paramset_idx=0,         curation_id=0,         curation_time=datetime.datetime(2023, 5, 11, 12, 00, 00),         curation_output_dir='subject1/session1/suite2p',         manual_curation=False,     ) )  imaging.MotionCorrection.populate(**populate_settings) imaging.Segmentation.populate(**populate_settings) imaging.Fluorescence.populate(**populate_settings) imaging.Activity.populate(**populate_settings) imaging_report.ScanLevelReport.populate(**populate_settings) imaging_report.TraceReport.populate(**populate_settings) In\u00a0[\u00a0]: Copied! <pre>def drop_databases(databases):\n    import pymysql.err\n    conn = dj.conn()\n\n    with dj.config(safemode=False):\n        for database in databases:\n            schema = dj.Schema(f'{dj.config[\"custom\"][\"database.prefix\"]}{database}')\n            while schema.list_tables():\n                for table in schema.list_tables():\n                    try:\n                        conn.query(f\"DROP TABLE `{schema.database}`.`{table}`\")\n                    except pymysql.err.OperationalError:\n                        print(f\"Can't drop `{schema.database}`.`{table}`. Retrying...\")\n            schema.drop()\n\n# drop_databases(databases=['analysis', 'trial', 'event', 'imaging_report', 'imaging', 'scan', 'session', 'subject', 'lab', 'reference'])\n</pre> def drop_databases(databases):     import pymysql.err     conn = dj.conn()      with dj.config(safemode=False):         for database in databases:             schema = dj.Schema(f'{dj.config[\"custom\"][\"database.prefix\"]}{database}')             while schema.list_tables():                 for table in schema.list_tables():                     try:                         conn.query(f\"DROP TABLE `{schema.database}`.`{table}`\")                     except pymysql.err.OperationalError:                         print(f\"Can't drop `{schema.database}`.`{table}`. Retrying...\")             schema.drop()  # drop_databases(databases=['analysis', 'trial', 'event', 'imaging_report', 'imaging', 'scan', 'session', 'subject', 'lab', 'reference']) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "tutorials/demo_prepare/#demo-preparation-notebook", "title": "Demo Preparation Notebook\u00b6", "text": "<p>Please Note: This notebook and demo are NOT intended to be used as learning materials. To gain a thorough understanding of the DataJoint workflow for calcium imaging, please see the <code>tutorial</code> notebook.</p>"}, {"location": "tutorials/demo_prepare/#drop-schemas", "title": "Drop schemas\u00b6", "text": "<ul> <li>Schemas are not typically dropped in a production workflow with real data in it.</li> <li>At the developmental phase, it might be required for the table redesign.</li> <li>When dropping all schemas is needed, the following is the dependency order.</li> </ul>"}, {"location": "tutorials/demo_run/", "title": "Demo run", "text": "<p>Left to right: Raw scans, Motion corrected scans, Cell segmentations, Calcium events</p> In\u00a0[\u00a0]: hide-input Copied! <pre>import datajoint as dj\nfrom workflow_calcium_imaging.pipeline import subject, session, scan, imaging\nfrom element_calcium_imaging.plotting.widget import main\n</pre> import datajoint as dj from workflow_calcium_imaging.pipeline import subject, session, scan, imaging from element_calcium_imaging.plotting.widget import main In\u00a0[\u00a0]: Copied! <pre>dj.Diagram(subject.Subject) + dj.Diagram(session.Session) + dj.Diagram(scan) + dj.Diagram(imaging)\n</pre> dj.Diagram(subject.Subject) + dj.Diagram(session.Session) + dj.Diagram(scan) + dj.Diagram(imaging) In\u00a0[\u00a0]: Copied! <pre>main(imaging)\n</pre> main(imaging) <p>For an in-depth tutorial please see the tutorial notebook.</p>"}, {"location": "tutorials/demo_run/#datajoint-workflow-for-calcium-imaging", "title": "DataJoint Workflow for Calcium Imaging\u00b6", "text": "<ul> <li>This notebook demonstrates using the open-source DataJoint Element to build a workflow for</li> </ul> <p>calcium imaging.</p> <ul> <li>For a detailed tutorial, please see the tutorial notebook.</li> </ul>"}, {"location": "tutorials/demo_run/#import-dependencies", "title": "Import dependencies\u00b6", "text": ""}, {"location": "tutorials/demo_run/#view-workflow", "title": "View workflow\u00b6", "text": ""}, {"location": "tutorials/demo_run/#insert-an-entry-in-a-manual-table-by-calling-the-insert-method", "title": "Insert an entry in a manual table by calling the <code>insert()</code> method\u00b6", "text": "<pre>subject.Subject.insert1(\n     dict(subject='subject1',\n          subject_birth_date='2023-01-01',\n          sex='U'\n     )\n)\n</pre>"}, {"location": "tutorials/demo_run/#automatically-process-data-with-the-populate-method", "title": "Automatically process data with the <code>populate()</code> method\u00b6", "text": "<ul> <li><p>Once data is inserted into manual tables, the <code>populate()</code> function automatically runs the ingestion and processing routines.</p> </li> <li><p>For example, to run Suite2p processing in the <code>Processing</code> table:</p> <pre>imaging.Processing.populate()\n</pre> </li> </ul>"}, {"location": "tutorials/demo_run/#visualize-processed-data", "title": "Visualize processed data\u00b6", "text": ""}, {"location": "tutorials/quality_metrics/", "title": "Quality Metrics Notebook", "text": "In\u00a0[1]: Copied! <pre>import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom workflow_calcium_imaging.pipeline import scan, imaging\n</pre> import datetime import numpy as np import matplotlib.pyplot as plt from workflow_calcium_imaging.pipeline import scan, imaging <pre>[2023-05-14 22:26:07,594][WARNING]: lab.Project and related tables will be removed in a future version of Element Lab. Please use the project schema.\n[2023-05-14 22:26:07,602][INFO]: Connecting root@fakeservices.datajoint.io:3306\n[2023-05-14 22:26:07,616][INFO]: Connected root@fakeservices.datajoint.io:3306\n</pre> In\u00a0[2]: Copied! <pre>scan.ScanQualityMetrics.populate(display_progress=True)\nimaging.ProcessingQualityMetrics.populate(display_progress=True)\n</pre> scan.ScanQualityMetrics.populate(display_progress=True) imaging.ProcessingQualityMetrics.populate(display_progress=True) In\u00a0[3]: Copied! <pre>key = dict(\n        subject=\"subject1\",\n        session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00),\n        scan_id=0,\n        field_idx=0,\n        channel=0,\n    )\n\nquery = scan.ScanQualityMetrics.Frames &amp; key\nquery\n</pre> key = dict(         subject=\"subject1\",         session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00),         scan_id=0,         field_idx=0,         channel=0,     )  query = scan.ScanQualityMetrics.Frames &amp; key query Out[3]: <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>field_idx</p> <p>channel</p> 0-based indexing <p>min_intensity</p> Minimum value of each frame. <p>mean_intensity</p> Mean value of each frame. <p>max_intensity</p> Maximum value of each frame. <p>contrast</p> Contrast of each frame (i.e. difference between the 99 and 1 percentiles) subject1 2023-05-11 12:00:00 0 0 0 =BLOB= =BLOB= =BLOB= =BLOB= <p>Total: 1</p> In\u00a0[4]: Copied! <pre>scan_metrics = query.fetch1()\n</pre> scan_metrics = query.fetch1() In\u00a0[5]: Copied! <pre>plt.plot(range(len(scan_metrics['min_intensity'])), \n         scan_metrics['min_intensity'], \n         label='Minimum intensity',\n         color='red')\n\nplt.plot(range(len(scan_metrics['mean_intensity'])), \n         scan_metrics['mean_intensity'], \n         label='Mean intensity',\n         color='green')\n\nplt.plot(range(len(scan_metrics['max_intensity'])), \n         scan_metrics['max_intensity'], \n         label='Max intensity',\n         color='black')\n\nplt.plot(range(len(scan_metrics['contrast'])), \n         scan_metrics['contrast'], \n         label='Contrast',\n         color='blue')\n\nplt.title('Scan quality metrics')\nplt.xlabel('Frame (#)')\nplt.ylabel('Intensity')\nplt.legend(loc='center', bbox_to_anchor=(0.5, -0.2), ncol=4)\nplt.show()\n</pre> plt.plot(range(len(scan_metrics['min_intensity'])),           scan_metrics['min_intensity'],           label='Minimum intensity',          color='red')  plt.plot(range(len(scan_metrics['mean_intensity'])),           scan_metrics['mean_intensity'],           label='Mean intensity',          color='green')  plt.plot(range(len(scan_metrics['max_intensity'])),           scan_metrics['max_intensity'],           label='Max intensity',          color='black')  plt.plot(range(len(scan_metrics['contrast'])),           scan_metrics['contrast'],           label='Contrast',          color='blue')  plt.title('Scan quality metrics') plt.xlabel('Frame (#)') plt.ylabel('Intensity') plt.legend(loc='center', bbox_to_anchor=(0.5, -0.2), ncol=4) plt.show() In\u00a0[6]: Copied! <pre>key = dict(\n        subject=\"subject1\",\n        session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00),\n        scan_id=0,\n        paramset_idx=0,\n        curation_id=0,\n        field_idx=0,\n    )\nquery = imaging.MotionCorrection.Summary &amp; key\nquery\n</pre> key = dict(         subject=\"subject1\",         session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00),         scan_id=0,         paramset_idx=0,         curation_id=0,         field_idx=0,     ) query = imaging.MotionCorrection.Summary &amp; key query Out[6]: Summary images for each field and channel after corrections <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>paramset_idx</p> Unique parameter set ID. <p>curation_id</p> <p>field_idx</p> <p>ref_image</p> image used as alignment template <p>average_image</p> mean of registered frames <p>correlation_image</p> correlation map (computed during cell detection) <p>max_proj_image</p> max of registered frames subject1 2023-05-11 12:00:00 0 0 0 0 =BLOB= =BLOB= =BLOB= =BLOB= <p>Total: 1</p> In\u00a0[7]: Copied! <pre>summary_images = query.fetch1()\n</pre> summary_images = query.fetch1() In\u00a0[8]: Copied! <pre>fig, axes = plt.subplots(1, 3, figsize=(12, 9))\n\naxes[0].imshow(summary_images[\"average_image\"])\naxes[0].set_title('Average Image')\naxes[0].set_xlabel('x (pixels)')\naxes[0].set_ylabel('y (pixels)')\n\naxes[1].imshow(summary_images[\"correlation_image\"])\naxes[1].set_title('Correlation Image')\naxes[1].set_xlabel('x (pixels)')\naxes[1].set_yticks([])\naxes[1].set_yticklabels([])\n\naxes[2].imshow(summary_images[\"max_proj_image\"])\naxes[2].set_title('Max projection Image')\naxes[2].set_xlabel('x (pixels)')\naxes[2].set_yticks([])\naxes[2].set_yticklabels([])\n\nplt.show()\n</pre> fig, axes = plt.subplots(1, 3, figsize=(12, 9))  axes[0].imshow(summary_images[\"average_image\"]) axes[0].set_title('Average Image') axes[0].set_xlabel('x (pixels)') axes[0].set_ylabel('y (pixels)')  axes[1].imshow(summary_images[\"correlation_image\"]) axes[1].set_title('Correlation Image') axes[1].set_xlabel('x (pixels)') axes[1].set_yticks([]) axes[1].set_yticklabels([])  axes[2].imshow(summary_images[\"max_proj_image\"]) axes[2].set_title('Max projection Image') axes[2].set_xlabel('x (pixels)') axes[2].set_yticks([]) axes[2].set_yticklabels([])  plt.show() In\u00a0[9]: Copied! <pre>mask_xpix, mask_ypix = (\n    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n    &amp; key\n    &amp; \"mask_center_z=0\"\n    &amp; \"mask_npix &gt; 100\"\n    &amp; \"confidence &gt; 0.90\"\n).fetch(\"mask_xpix\", \"mask_ypix\")\n\nmask_image = np.zeros(np.shape(summary_images[\"correlation_image\"]), dtype=bool)\nfor xpix, ypix in zip(mask_xpix, mask_ypix):\n    try:\n        mask_image[ypix, xpix] = True\n    except Exception as e:\n        print(e)\n\nplt.xlabel('x (pixels)')\nplt.ylabel('y (pixels)')\nplt.imshow(summary_images[\"correlation_image\"])\nplt.contour(mask_image, colors=\"white\", linewidths=0.5)\nplt.show()\n</pre> mask_xpix, mask_ypix = (     imaging.Segmentation.Mask * imaging.MaskClassification.MaskType     &amp; key     &amp; \"mask_center_z=0\"     &amp; \"mask_npix &gt; 100\"     &amp; \"confidence &gt; 0.90\" ).fetch(\"mask_xpix\", \"mask_ypix\")  mask_image = np.zeros(np.shape(summary_images[\"correlation_image\"]), dtype=bool) for xpix, ypix in zip(mask_xpix, mask_ypix):     try:         mask_image[ypix, xpix] = True     except Exception as e:         print(e)  plt.xlabel('x (pixels)') plt.ylabel('y (pixels)') plt.imshow(summary_images[\"correlation_image\"]) plt.contour(mask_image, colors=\"white\", linewidths=0.5) plt.show() <pre>index 504 is out of bounds for axis 0 with size 504\nindex 504 is out of bounds for axis 0 with size 504\nindex 504 is out of bounds for axis 0 with size 504\nindex 504 is out of bounds for axis 0 with size 504\nindex 504 is out of bounds for axis 0 with size 504\nindex 506 is out of bounds for axis 1 with size 506\nindex 506 is out of bounds for axis 1 with size 506\nindex 504 is out of bounds for axis 0 with size 504\n</pre> In\u00a0[10]: Copied! <pre>key = dict(\n        subject=\"subject1\",\n        session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00),\n        scan_id=0,\n        paramset_idx=0,\n        curation_id=0,\n    )\n\nquery = imaging.ProcessingQualityMetrics.Mask &amp; key\nquery\n</pre> key = dict(         subject=\"subject1\",         session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00),         scan_id=0,         paramset_idx=0,         curation_id=0,     )  query = imaging.ProcessingQualityMetrics.Mask &amp; key query Out[10]: <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>paramset_idx</p> Unique parameter set ID. <p>curation_id</p> <p>mask</p> <p>mask_area</p> Mask area in square micrometer. <p>roundness</p> Roundness between 0 and 1. Values closer to 1 are rounder. subject1 2023-05-11 12:00:00 0 0 0 0 nan 0.753574subject1 2023-05-11 12:00:00 0 0 0 1 nan 0.749964subject1 2023-05-11 12:00:00 0 0 0 2 nan 0.843484subject1 2023-05-11 12:00:00 0 0 0 3 nan 0.646408subject1 2023-05-11 12:00:00 0 0 0 4 nan 0.721577subject1 2023-05-11 12:00:00 0 0 0 5 nan 0.712069subject1 2023-05-11 12:00:00 0 0 0 6 nan 0.734321subject1 2023-05-11 12:00:00 0 0 0 7 nan 0.684134subject1 2023-05-11 12:00:00 0 0 0 8 nan 0.600041subject1 2023-05-11 12:00:00 0 0 0 9 nan 0.903313subject1 2023-05-11 12:00:00 0 0 0 10 nan 0.808169subject1 2023-05-11 12:00:00 0 0 0 11 nan 0.877421 <p>...</p> <p>Total: 1276</p> In\u00a0[11]: Copied! <pre>roundness = query.fetch('roundness')\n</pre> roundness = query.fetch('roundness') In\u00a0[12]: Copied! <pre>plt.hist(roundness, bins=20, color='white', edgecolor='black')\n\nplt.title('Mask roundness')\nplt.xlabel('Roundness')\nplt.ylabel('Count (#)')\nplt.show()\n</pre> plt.hist(roundness, bins=20, color='white', edgecolor='black')  plt.title('Mask roundness') plt.xlabel('Roundness') plt.ylabel('Count (#)') plt.show() In\u00a0[13]: Copied! <pre>key = dict(\n        subject=\"subject1\",\n        session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00),\n        scan_id=0,\n        paramset_idx=0,\n        curation_id=0,\n        fluo_channel=0\n    )\n\nquery = imaging.ProcessingQualityMetrics.Trace &amp; key\nquery\n</pre> key = dict(         subject=\"subject1\",         session_datetime=datetime.datetime(2023, 5, 11, 12, 00, 00),         scan_id=0,         paramset_idx=0,         curation_id=0,         fluo_channel=0     )  query = imaging.ProcessingQualityMetrics.Trace &amp; key query Out[13]: <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>paramset_idx</p> Unique parameter set ID. <p>curation_id</p> <p>mask</p> <p>fluo_channel</p> 0-based indexing <p>skewness</p> Skewness of the fluorescence trace. <p>variance</p> Variance of the fluorescence trace. subject1 2023-05-11 12:00:00 0 0 0 0 0 2.28541 865.221subject1 2023-05-11 12:00:00 0 0 0 1 0 2.18231 342.718subject1 2023-05-11 12:00:00 0 0 0 2 0 2.49255 784.625subject1 2023-05-11 12:00:00 0 0 0 3 0 3.98893 153.951subject1 2023-05-11 12:00:00 0 0 0 4 0 2.83063 315.717subject1 2023-05-11 12:00:00 0 0 0 5 0 2.74945 194.228subject1 2023-05-11 12:00:00 0 0 0 6 0 6.43236 183.582subject1 2023-05-11 12:00:00 0 0 0 7 0 4.58705 184.103subject1 2023-05-11 12:00:00 0 0 0 8 0 3.45196 204.443subject1 2023-05-11 12:00:00 0 0 0 9 0 3.10743 237.697subject1 2023-05-11 12:00:00 0 0 0 10 0 5.04266 121.186subject1 2023-05-11 12:00:00 0 0 0 11 0 2.18527 335.129 <p>...</p> <p>Total: 1276</p> In\u00a0[14]: Copied! <pre>skewness, variance = query.fetch('skewness', 'variance')\n</pre> skewness, variance = query.fetch('skewness', 'variance') In\u00a0[15]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n\naxes[0].scatter(range(len(skewness)), \n         np.sort(skewness), \n         color='black', s=0.5)\naxes[0].set_title('Temporal skewness')\naxes[0].set_xlabel('Cell')\naxes[0].set_ylabel('Sorted skewness')\n\naxes[1].scatter(range(len(variance)), \n         np.sort(variance), \n         color='black', s=0.5)\naxes[1].set_title('Temporal variance')\naxes[1].set_xlabel('Cell')\naxes[1].set_ylabel('Sorted variance')\n\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(9, 4))  axes[0].scatter(range(len(skewness)),           np.sort(skewness),           color='black', s=0.5) axes[0].set_title('Temporal skewness') axes[0].set_xlabel('Cell') axes[0].set_ylabel('Sorted skewness')  axes[1].scatter(range(len(variance)),           np.sort(variance),           color='black', s=0.5) axes[1].set_title('Temporal variance') axes[1].set_xlabel('Cell') axes[1].set_ylabel('Sorted variance')  plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "tutorials/quality_metrics/#calcium-imaging-quality-metrics", "title": "Calcium Imaging Quality Metrics\u00b6", "text": "<p>Visualize the calcium imaging quality metrics for the raw and processed scans that are stored in the DataJoint pipeline (i.e. <code>element-calcium-imaging</code>).</p> <p>If you are new to using this DataJoint pipeline for analyzing calcium imaging data, please see the tutorial notebook for an in-depth explanation to set up and run the workflow.</p> <p>This quality metrics notebook can run in a GitHub Codespace, and requires the example data to be populated into the database using the demo_prepare notebook.</p>"}, {"location": "tutorials/quality_metrics/#populate-quality-metrics-tables", "title": "Populate quality metrics tables\u00b6", "text": ""}, {"location": "tutorials/quality_metrics/#scan-quality-metrics", "title": "Scan quality metrics\u00b6", "text": "<p>Intensity values (minimum, mean, max, contrast) can be used to evaluate the consistency of imaging for each frame, and changing values could indicate changes in imaging conditions or technical issues with the equipment or software (e.g. photobleaching, saturation, etc.). In which case, it may be necessary to adjust the imaging protocol, or troubleshoot the equipment or software.</p>"}, {"location": "tutorials/quality_metrics/#motion-corrected-summary-images", "title": "Motion corrected summary images\u00b6", "text": ""}, {"location": "tutorials/quality_metrics/#segmentation-masks", "title": "Segmentation masks\u00b6", "text": ""}, {"location": "tutorials/quality_metrics/#mask-quality-metrics", "title": "Mask quality metrics\u00b6", "text": "Metric Description Source Mask area Area can be used to evaluate the accuracy and consistency of the segmentation process. The mask area can be compared to the expected area of a single cell to determine if segmentation lead to false positives or false negatives. Stringer &amp; Pachitariu, Current Opinion in Neurobiology 2019 Mask roundness Roundness is to evaluate how closely a segmented mask's shape resembles a perfect circle. A perfectly round mask will have a value of 1, while a more elongated or irregular mask will have a lower roundness value. Roundness can help identify cells that have been improperly segmented. For example, cells that are elongated or irregular in shape may have been segmented incorrectly due to noise, overlapping cells, or other factors. By comparing the roundness of segmented masks to a threshold value, cells that are improperly segmented can be identified and corrected. Tegtmeier et al., Frontiers in Neuroscience 2018"}, {"location": "tutorials/quality_metrics/#trace-quality-metrics", "title": "Trace quality metrics\u00b6", "text": "<p>Temporal skewness and variance of the fluorescence activity can indicate the stability of the signal over time. Changes in this metric between imaging sessions could indicate technical issues in the experimental conditions or data processing. Additionally, changes in the animal's behavior or physiological state could also affect this metric, so it is important to interpret any changes within the context of the experimental conditions and the animal's behavior and physiology. (Stringer &amp; Pachitariu, Current Opinion in Neurobiology 2019)</p> <p>For illustrative purposes, below we will fetch and plot these metrics for a single session.</p>"}, {"location": "tutorials/tutorial/", "title": "Tutorial Notebook", "text": "<p>Let's start by importing the packages necessary to run this workflow.</p> In\u00a0[1]: Copied! <pre>import os\n\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\n\nimport datajoint as dj\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> import os  if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\")  import datajoint as dj import datetime import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre>from workflow_calcium_imaging.pipeline import lab, subject, session, scan, imaging, Equipment, event, trial, analysis\n</pre> from workflow_calcium_imaging.pipeline import lab, subject, session, scan, imaging, Equipment, event, trial, analysis <pre>[2023-03-17 22:30:23,259][WARNING]: lab.Project and related tables will be removed in a future version of Element Lab. Please use the project schema.\n[2023-03-17 22:30:23,274][INFO]: Connecting root@fakeservices.datajoint.io:3306\n[2023-03-17 22:30:23,282][INFO]: Connected root@fakeservices.datajoint.io:3306\n</pre> In\u00a0[3]: Copied! <pre>(\n    dj.Diagram(subject.Subject)\n    + dj.Diagram(session.Session)\n    + dj.Diagram(scan)\n    + dj.Diagram(imaging)\n)\n</pre> (     dj.Diagram(subject.Subject)     + dj.Diagram(session.Session)     + dj.Diagram(scan)     + dj.Diagram(imaging) ) Out[3]: %3 4 4 imaging.Segmentation.Mask imaging.Segmentation.Mask 4-&gt;imaging.Segmentation.Mask 3 3 imaging.MotionCorrection imaging.MotionCorrection 3-&gt;imaging.MotionCorrection 5 5 imaging.Fluorescence.Trace imaging.Fluorescence.Trace 5-&gt;imaging.Fluorescence.Trace scan.Scan scan.Scan scan.ScanLocation scan.ScanLocation scan.Scan-&gt;scan.ScanLocation scan.ScanInfo scan.ScanInfo scan.Scan-&gt;scan.ScanInfo imaging.ProcessingTask imaging.ProcessingTask scan.Scan-&gt;imaging.ProcessingTask imaging.Segmentation imaging.Segmentation imaging.Segmentation-&gt;imaging.Segmentation.Mask imaging.MaskClassification imaging.MaskClassification imaging.Segmentation-&gt;imaging.MaskClassification imaging.Fluorescence imaging.Fluorescence imaging.Segmentation-&gt;imaging.Fluorescence imaging.MotionCorrection.Summary imaging.MotionCorrection.Summary imaging.MotionCorrection-&gt;imaging.MotionCorrection.Summary imaging.MotionCorrection.RigidMotionCorrection imaging.MotionCorrection.RigidMotionCorrection imaging.MotionCorrection-&gt;imaging.MotionCorrection.RigidMotionCorrection imaging.MotionCorrection.NonRigidMotionCorrection imaging.MotionCorrection.NonRigidMotionCorrection imaging.MotionCorrection-&gt;imaging.MotionCorrection.NonRigidMotionCorrection imaging.ProcessingMethod imaging.ProcessingMethod imaging.ProcessingParamSet imaging.ProcessingParamSet imaging.ProcessingMethod-&gt;imaging.ProcessingParamSet imaging.ProcessingParamSet-&gt;imaging.ProcessingTask imaging.MotionCorrection.Block imaging.MotionCorrection.Block subject.Subject subject.Subject session.Session session.Session subject.Subject-&gt;session.Session scan.ScanInfo.Field scan.ScanInfo.Field scan.ScanInfo.Field-&gt;imaging.MotionCorrection.Summary scan.AcquisitionSoftware scan.AcquisitionSoftware scan.AcquisitionSoftware-&gt;scan.Scan imaging.MaskClassification.MaskType imaging.MaskClassification.MaskType imaging.Segmentation.Mask-&gt;imaging.MaskClassification.MaskType imaging.Segmentation.Mask-&gt;imaging.Fluorescence.Trace imaging.MaskClassificationMethod imaging.MaskClassificationMethod imaging.MaskClassificationMethod-&gt;imaging.MaskClassification imaging.CellCompartment imaging.CellCompartment imaging.Processing imaging.Processing imaging.Curation imaging.Curation imaging.Processing-&gt;imaging.Curation imaging.Activity.Trace imaging.Activity.Trace imaging.Fluorescence.Trace-&gt;imaging.Activity.Trace imaging.MaskType imaging.MaskType imaging.MaskType-&gt;imaging.MaskClassification.MaskType imaging.Activity imaging.Activity imaging.Activity-&gt;imaging.Activity.Trace imaging.MotionCorrection.NonRigidMotionCorrection-&gt;imaging.MotionCorrection.Block scan.ScanInfo-&gt;scan.ScanInfo.Field scan.ScanInfo.ScanFile scan.ScanInfo.ScanFile scan.ScanInfo-&gt;scan.ScanInfo.ScanFile imaging.ActivityExtractionMethod imaging.ActivityExtractionMethod imaging.ActivityExtractionMethod-&gt;imaging.Activity imaging.Curation-&gt;imaging.Segmentation imaging.Curation-&gt;imaging.MotionCorrection session.Session-&gt;scan.Scan imaging.MaskClassification-&gt;imaging.MaskClassification.MaskType imaging.ProcessingTask-&gt;imaging.Processing imaging.Fluorescence-&gt;imaging.Fluorescence.Trace imaging.Fluorescence-&gt;imaging.Activity scan.Channel scan.Channel scan.Channel-&gt;4 scan.Channel-&gt;3 scan.Channel-&gt;5 In\u00a0[4]: Copied! <pre>subject.Subject.describe()\n</pre> subject.Subject.describe() Out[4]: <pre>'subject              : varchar(8)                   \\n---\\nsubject_nickname=\"\"  : varchar(64)                  \\nsex                  : enum(\\'M\\',\\'F\\',\\'U\\')            \\nsubject_birth_date   : date                         \\nsubject_description=\"\" : varchar(1024)                \\n'</pre> In\u00a0[5]: Copied! <pre>subject.Subject.heading\n</pre> subject.Subject.heading Out[5]: <pre># \nsubject              : varchar(8)                   # \n---\nsubject_nickname=\"\"  : varchar(64)                  # \nsex                  : enum('M','F','U')            # \nsubject_birth_date   : date                         # \nsubject_description=\"\" : varchar(1024)                # </pre> <p>The cells above show all attributes of the subject table. These are particularly useful functions if you are new to DataJoint Elements and are unsure of the attributes required for each table. We will insert data into the <code>subject.Subject</code> table.</p> In\u00a0[6]: Copied! <pre>subject.Subject.insert1(\n    dict(\n        subject=\"subject1\",\n        sex=\"F\",\n        subject_birth_date=\"2020-01-01\",\n        subject_description=\"ScanImage acquisition. Suite2p processing.\",\n    )\n)\nsubject.Subject()\n</pre> subject.Subject.insert1(     dict(         subject=\"subject1\",         sex=\"F\",         subject_birth_date=\"2020-01-01\",         subject_description=\"ScanImage acquisition. Suite2p processing.\",     ) ) subject.Subject() Out[6]: <p>subject</p> <p>subject_nickname</p> <p>sex</p> <p>subject_birth_date</p> <p>subject_description</p> subject1 F 2020-01-01 ScanImage acquisition. Suite2p processing. <p>Total: 1</p> <p>Let's repeat the steps above for the <code>Session</code> table and see how the output varies between <code>.describe</code> and <code>.heading</code>.</p> In\u00a0[7]: Copied! <pre>session.Session.describe()\n</pre> session.Session.describe() Out[7]: <pre>'-&gt; subject.Subject\\nsession_datetime     : datetime                     \\n'</pre> In\u00a0[8]: Copied! <pre>session.Session.heading\n</pre> session.Session.heading Out[8]: <pre># \nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # </pre> <p>The cells above show the dependencies and attributes for the <code>session.Session</code> table. Notice that <code>describe</code> shows the dependencies of the table on upstream tables. The <code>Session</code> table depends on the upstream <code>Subject</code> table.</p> <p>Whereas <code>heading</code> lists all the attributes of the <code>Session</code> table, regardless of whether they are declared in an upstream table.</p> <p>Here we will demonstrate a very useful way of inserting data by assigning the dictionary to a variable <code>session_key</code>. This variable can be used to insert entries into tables that contain the <code>Session</code> table as one of its attributes.</p> In\u00a0[9]: Copied! <pre>session_key = dict(subject=\"subject1\", session_datetime=\"2021-04-30 12:22:15.032\")\n\nsession.Session.insert1(session_key)\nsession.Session()\n</pre> session_key = dict(subject=\"subject1\", session_datetime=\"2021-04-30 12:22:15.032\")  session.Session.insert1(session_key) session.Session() Out[9]: <p>subject</p> <p>session_datetime</p> subject1 2021-04-30 12:22:15 <p>Total: 1</p> <p>The <code>SessionDirectory</code> table locates the relevant data files in a directory path relative to the root directory defined in your <code>dj.config[\"custom\"]</code>. More information about <code>dj.config</code> is provided at the end of this tutorial and is particularly useful for local deployments of this workflow.</p> In\u00a0[10]: Copied! <pre>session.SessionDirectory.describe()\n</pre> session.SessionDirectory.describe() Out[10]: <pre>'-&gt; session.Session\\n---\\nsession_dir          : varchar(256)                 # Path to the data directory for a session\\n'</pre> In\u00a0[11]: Copied! <pre>session.SessionDirectory.heading\n</pre> session.SessionDirectory.heading Out[11]: <pre># \nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \n---\nsession_dir          : varchar(256)                 # Path to the data directory for a session</pre> In\u00a0[12]: Copied! <pre>session.SessionDirectory.insert1(\n    dict(**session_key,\n        session_dir=\"subject1/session1\")\n)\nsession.SessionDirectory()\n</pre> session.SessionDirectory.insert1(     dict(**session_key,         session_dir=\"subject1/session1\") ) session.SessionDirectory() Out[12]: <p>subject</p> <p>session_datetime</p> <p>session_dir</p> Path to the data directory for a session subject1 2021-04-30 12:22:15 subject1/session1 <p>Total: 1</p> <p>Next, we'll use <code>describe</code> and <code>heading</code> for the Scan table. Do you notice anything we might have missed here?</p> In\u00a0[13]: Copied! <pre>scan.Scan.describe()\n</pre> scan.Scan.describe() Out[13]: <pre>'-&gt; session.Session\\nscan_id              : int                          \\n---\\n-&gt; [nullable] Equipment\\n-&gt; scan.AcquisitionSoftware\\nscan_notes=\"\"        : varchar(4095)                \\n'</pre> In\u00a0[\u00a0]: Copied! <pre>scan.Scan.heading\n</pre> scan.Scan.heading <p>The <code>Scan</code> table's attributes include the <code>Session</code> table and the <code>Equipment</code> table. Let's insert into the Equipment table after checking its attributes. Then, we will insert into <code>Scan</code>.</p> In\u00a0[14]: Copied! <pre>Equipment.insert1(dict(scanner=\"ScannerA\"))\n</pre> Equipment.insert1(dict(scanner=\"ScannerA\")) In\u00a0[15]: Copied! <pre>scan.Scan.insert1(\n    dict(\n        **session_key,\n        scan_id=0,\n        scanner=\"ScannerA\",\n        acq_software=\"ScanImage\",\n        scan_notes=\"\",\n    )\n)\nscan.Scan()\n</pre> scan.Scan.insert1(     dict(         **session_key,         scan_id=0,         scanner=\"ScannerA\",         acq_software=\"ScanImage\",         scan_notes=\"\",     ) ) scan.Scan() Out[15]: <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>scanner</p> <p>acq_software</p> <p>scan_notes</p> subject1 2021-04-30 12:22:15 0 ScannerA ScanImage <p>Total: 1</p> In\u00a0[16]: Copied! <pre>scan.ScanInfo.heading\n</pre> scan.ScanInfo.heading Out[16]: <pre># General data about the resoscans/mesoscans from header\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \nscan_id              : int                          # \n---\nnfields              : tinyint                      # number of fields\nnchannels            : tinyint                      # number of channels\nndepths              : int                          # Number of scanning depths (planes)\nnframes              : int                          # number of recorded frames\nnrois                : tinyint                      # number of ROIs (see scanimage's multi ROI imaging)\nx=null               : float                        # (um) ScanImage's 0 point in the motor coordinate system\ny=null               : float                        # (um) ScanImage's 0 point in the motor coordinate system\nz=null               : float                        # (um) ScanImage's 0 point in the motor coordinate system\nfps                  : float                        # (Hz) frames per second - Volumetric Scan Rate\nbidirectional        : tinyint                      # true = bidirectional scanning\nusecs_per_line=null  : float                        # microseconds per scan line\nfill_fraction=null   : float                        # raster scan temporal fill fraction (see scanimage)\nscan_datetime=null   : datetime                     # datetime of the scan\nscan_duration=null   : float                        # (seconds) duration of the scan\nbidirectional_z=null : tinyint                      # true = bidirectional z-scan</pre> In\u00a0[17]: Copied! <pre>scan.ScanInfo.Field.heading\n</pre> scan.ScanInfo.Field.heading Out[17]: <pre># field-specific scan information\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \nscan_id              : int                          # \nfield_idx            : int                          # \n---\npx_height            : smallint                     # height in pixels\npx_width             : smallint                     # width in pixels\num_height=null       : float                        # height in microns\num_width=null        : float                        # width in microns\nfield_x=null         : float                        # (um) center of field in the motor coordinate system\nfield_y=null         : float                        # (um) center of field in the motor coordinate system\nfield_z=null         : float                        # (um) relative depth of field\ndelay_image=null     : longblob                     # (ms) delay between the start of the scan and pixels in this field\nroi=null             : int                          # the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans</pre> In\u00a0[18]: Copied! <pre>scan.ScanInfo()\n</pre> scan.ScanInfo() Out[18]: General data about the resoscans/mesoscans from header <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>nfields</p> number of fields <p>nchannels</p> number of channels <p>ndepths</p> Number of scanning depths (planes) <p>nframes</p> number of recorded frames <p>nrois</p> number of ROIs (see scanimage's multi ROI imaging) <p>x</p> (um) ScanImage's 0 point in the motor coordinate system <p>y</p> (um) ScanImage's 0 point in the motor coordinate system <p>z</p> (um) ScanImage's 0 point in the motor coordinate system <p>fps</p> (Hz) frames per second - Volumetric Scan Rate <p>bidirectional</p> true = bidirectional scanning <p>usecs_per_line</p> microseconds per scan line <p>fill_fraction</p> raster scan temporal fill fraction (see scanimage) <p>scan_datetime</p> datetime of the scan <p>scan_duration</p> (seconds) duration of the scan <p>bidirectional_z</p> true = bidirectional z-scan <p>Total: 0</p> In\u00a0[19]: Copied! <pre>scan.ScanInfo.Field()\n</pre> scan.ScanInfo.Field() Out[19]: field-specific scan information <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>field_idx</p> <p>px_height</p> height in pixels <p>px_width</p> width in pixels <p>um_height</p> height in microns <p>um_width</p> width in microns <p>field_x</p> (um) center of field in the motor coordinate system <p>field_y</p> (um) center of field in the motor coordinate system <p>field_z</p> (um) relative depth of field <p>delay_image</p> (ms) delay between the start of the scan and pixels in this field <p>roi</p> the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans <p>Total: 0</p> In\u00a0[20]: Copied! <pre># duration depends on your network bandwidth to s3\nscan.ScanInfo.populate(session_key, display_progress=True)\n</pre> # duration depends on your network bandwidth to s3 scan.ScanInfo.populate(session_key, display_progress=True) <pre>ScanInfo: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  2.59it/s]\n</pre> <p>Let's view the information was entered into each of these tables:</p> In\u00a0[21]: Copied! <pre>scan.ScanInfo()\n</pre> scan.ScanInfo() Out[21]: General data about the resoscans/mesoscans from header <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>nfields</p> number of fields <p>nchannels</p> number of channels <p>ndepths</p> Number of scanning depths (planes) <p>nframes</p> number of recorded frames <p>nrois</p> number of ROIs (see scanimage's multi ROI imaging) <p>x</p> (um) ScanImage's 0 point in the motor coordinate system <p>y</p> (um) ScanImage's 0 point in the motor coordinate system <p>z</p> (um) ScanImage's 0 point in the motor coordinate system <p>fps</p> (Hz) frames per second - Volumetric Scan Rate <p>bidirectional</p> true = bidirectional scanning <p>usecs_per_line</p> microseconds per scan line <p>fill_fraction</p> raster scan temporal fill fraction (see scanimage) <p>scan_datetime</p> datetime of the scan <p>scan_duration</p> (seconds) duration of the scan <p>bidirectional_z</p> true = bidirectional z-scan subject1 2021-04-30 12:22:15 0 1 1 1 3000 0 13441.9 15745.0 -205821.0 29.2398 1 63.0981 0.712867 None 102.6 None <p>Total: 1</p> In\u00a0[22]: Copied! <pre>scan.ScanInfo.Field()\n</pre> scan.ScanInfo.Field() Out[22]: field-specific scan information <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>field_idx</p> <p>px_height</p> height in pixels <p>px_width</p> width in pixels <p>um_height</p> height in microns <p>um_width</p> width in microns <p>field_x</p> (um) center of field in the motor coordinate system <p>field_y</p> (um) center of field in the motor coordinate system <p>field_z</p> (um) relative depth of field <p>delay_image</p> (ms) delay between the start of the scan and pixels in this field <p>roi</p> the scanning roi (as recorded in the acquisition software) containing this field - only relevant to mesoscale scans subject1 2021-04-30 12:22:15 0 0 512 512 nan nan 13441.9 15745.0 -205821.0 =BLOB= None <p>Total: 1</p> <p>We're almost ready to perform image processing with <code>Suite2p</code>. An important step before processing is managing the parameters which will be used in that step. To do so, we will import suite2p and insert the default parameters into a DataJoint table <code>ProcessingParamSet</code>. This table keeps track of all combinations of your image processing parameters. You can choose which parameters are used during processing in a later step.</p> <p>Let's view the attributes and insert data into <code>imaging.ProcessingParamSet</code>.</p> In\u00a0[23]: Copied! <pre>imaging.ProcessingParamSet.heading\n</pre> imaging.ProcessingParamSet.heading Out[23]: <pre># Processing Parameter Set\nparamset_idx         : smallint                     # Unique parameter set ID.\n---\nprocessing_method    : char(8)                      # \nparamset_desc        : varchar(1280)                # Parameter-set description\nparam_set_hash       : uuid                         # A universally unique identifier for the parameter set\nparams               : longblob                     # Parameter Set, a dictionary of all applicable parameters to the analysis suite.</pre> In\u00a0[24]: Copied! <pre>import suite2p\n\nparams_suite2p = suite2p.default_ops()\nparams_suite2p['nonrigid']=False\n\nimaging.ProcessingParamSet.insert_new_params(\n    processing_method=\"suite2p\",\n    paramset_idx=0,\n    params=params_suite2p,\n    paramset_desc=\"Calcium imaging analysis with Suite2p using default Suite2p parameters\",\n)\n</pre> import suite2p  params_suite2p = suite2p.default_ops() params_suite2p['nonrigid']=False  imaging.ProcessingParamSet.insert_new_params(     processing_method=\"suite2p\",     paramset_idx=0,     params=params_suite2p,     paramset_desc=\"Calcium imaging analysis with Suite2p using default Suite2p parameters\", ) <pre>Warning: cellpose did not import\nNo module named 'cellpose'\ncannot use anatomical mode, but otherwise suite2p will run normally\n</pre> <p>Now that we've inserted suite2p parameters into the <code>ProcessingParamSet</code> table, we're almost ready to run image processing. DataJoint uses a <code>ProcessingTask</code> table to manage which <code>Scan</code> and <code>ProcessingParamSet</code> should be used during processing.</p> <p>This table is important for defining several important aspects of downstream processing. Let's view the attributes to get a better understanding.</p> In\u00a0[25]: Copied! <pre>imaging.ProcessingTask.describe()\n</pre> imaging.ProcessingTask.describe() Out[25]: <pre>'# Manual table for defining a processing task ready to be run\\n-&gt; scan.Scan\\n-&gt; imaging.ProcessingParamSet\\n---\\nprocessing_output_dir : varchar(255)                 # Output directory of the processed scan relative to root data directory\\ntask_mode=\"load\"     : enum(\\'load\\',\\'trigger\\')       # \\'load\\': load computed analysis results, \\'trigger\\': trigger computation\\n'</pre> In\u00a0[26]: Copied! <pre>imaging.ProcessingTask.heading\n</pre> imaging.ProcessingTask.heading Out[26]: <pre># Manual table for defining a processing task ready to be run\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \nscan_id              : int                          # \nparamset_idx         : smallint                     # Unique parameter set ID.\n---\nprocessing_output_dir : varchar(255)                 # Output directory of the processed scan relative to root data directory\ntask_mode=\"load\"     : enum('load','trigger')       # 'load': load computed analysis results, 'trigger': trigger computation</pre> <p>The <code>ProcessingParamSet</code> table contains two important attributes:</p> <ul> <li><code>paramset_idx</code></li> <li><code>task_mode</code></li> </ul> <p>The <code>paramset_idx</code> attribute is tracks your image processing parameter sets. You can choose the parameter set on which you want to run image processing analysis based on this attribute. For example, <code>paramset_idx=0</code> may contain default parameters for suite2p processing whereas <code>paramset_idx=1</code> contains your custom parameters for suite2p processing. This attribute tells the <code>Processing</code> table which set of parameters you are processing in a given <code>populate()</code>.</p> <p>The <code>task_mode</code> attribute can be set to either <code>load</code> or <code>trigger</code>. When set to <code>load</code>, running the processing step initiates a search for exisiting output files of the image processing algorithm defined in <code>ProcessingParamSet</code>. When set to <code>trigger</code>, the processing step will run image processing on the raw data.</p> In\u00a0[27]: Copied! <pre>imaging.ProcessingTask.insert1(\n    dict(\n        **session_key,\n        scan_id=0,\n        paramset_idx=0,\n        task_mode='load', # load or trigger\n        processing_output_dir=\"subject1/session1/suite2p\",\n    )\n)\n</pre> imaging.ProcessingTask.insert1(     dict(         **session_key,         scan_id=0,         paramset_idx=0,         task_mode='load', # load or trigger         processing_output_dir=\"subject1/session1/suite2p\",     ) ) <p>Notice we are setting <code>task_mode</code> to <code>load</code>. Let's call populate on the <code>Processing</code> table in the pipeline.</p> In\u00a0[28]: Copied! <pre>imaging.Processing.populate(session_key, display_progress=True)\n</pre> imaging.Processing.populate(session_key, display_progress=True) <pre>Processing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 28.35it/s]\n</pre> <p>While processing is complete in the step above, you can optionally curate the output of image processing using the <code>Curation</code> table.</p> In\u00a0[29]: Copied! <pre>imaging.Curation.heading\n</pre> imaging.Curation.heading Out[29]: <pre># Curation(s) results\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \nscan_id              : int                          # \nparamset_idx         : smallint                     # Unique parameter set ID.\ncuration_id          : int                          # \n---\ncuration_time        : datetime                     # Time of generation of this set of curated results\ncuration_output_dir  : varchar(255)                 # Output directory of the curated results, relative to root data directory\nmanual_curation      : tinyint                      # Has manual curation been performed on this result?\ncuration_note=\"\"     : varchar(2000)                # </pre> In\u00a0[30]: Copied! <pre>imaging.Curation.insert1(\n    dict(\n        **session_key,\n        scan_id=0,\n        paramset_idx=0,\n        curation_id=0,\n        curation_time=\"2021-04-30 12:22:15.032\",\n        curation_output_dir=\"subject1/session1/suite2p\",\n        manual_curation=False,\n        curation_note=\"\",\n    )\n)\n</pre> imaging.Curation.insert1(     dict(         **session_key,         scan_id=0,         paramset_idx=0,         curation_id=0,         curation_time=\"2021-04-30 12:22:15.032\",         curation_output_dir=\"subject1/session1/suite2p\",         manual_curation=False,         curation_note=\"\",     ) ) <p>Now, we'll populate several tables that store the output of image processing, including <code>MotionCorrection</code>, <code>Segementation</code>, <code>Fluorescence</code>, and <code>Activity</code>.</p> <p>Feel free to create new cells in this notebook to view each table's dependencies and attributes.</p> In\u00a0[31]: Copied! <pre>imaging.MotionCorrection.populate(session_key, display_progress=True)\n</pre> imaging.MotionCorrection.populate(session_key, display_progress=True) <pre>MotionCorrection: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.43it/s]\n</pre> In\u00a0[32]: Copied! <pre>imaging.Segmentation.populate(session_key, display_progress=True)\n</pre> imaging.Segmentation.populate(session_key, display_progress=True) <pre>Segmentation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.42it/s]\n</pre> In\u00a0[33]: Copied! <pre>imaging.Fluorescence.populate(session_key, display_progress=True)\n</pre> imaging.Fluorescence.populate(session_key, display_progress=True) <pre>Fluorescence: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.97s/it]\n</pre> In\u00a0[34]: Copied! <pre>imaging.Activity.populate(session_key, display_progress=True)\n</pre> imaging.Activity.populate(session_key, display_progress=True) <pre>Activity: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.30it/s]\n</pre> <p>Now that we've populated the tables in this workflow, there are one of several options for next steps. If you have a standard workflow for aligning calcium activity to behavior data or other stimuli, you can easily invoke <code>element-event</code> or define your custom DataJoint tables to extend the pipeline.</p> <p>In this tutorial, we will fetch the data from the database and create a few plots.</p> In\u00a0[35]: Copied! <pre>fluor_trace = (imaging.Fluorescence.Trace &amp; \"mask = '10'\").fetch1(\"fluorescence\")\n</pre> fluor_trace = (imaging.Fluorescence.Trace &amp; \"mask = '10'\").fetch1(\"fluorescence\") <p>In the query above, we fetch a single <code>fluorescence</code> attribute from the <code>Trace</code> part table belonging to the <code>Fluorescence</code> table. We also restrict the query to mask number 10.</p> <p>Let's go ahead and plot this trace.</p> In\u00a0[36]: Copied! <pre>plt.plot(fluor_trace)\nplt.title(\"Fluorescence trace for mask 10\");\nplt.xlabel(\"Samples\")\nplt.ylabel(\"a.u.\");\n</pre> plt.plot(fluor_trace) plt.title(\"Fluorescence trace for mask 10\"); plt.xlabel(\"Samples\") plt.ylabel(\"a.u.\"); <p>DataJoint queries can be a highly flexible tool to manipulate and visualize your data. After all, plotting traces is likely just the start of your analysis workflow. This can also make the queries seem more complex at first. However, we'll walk through them slowly to simplify their content in this notebook.</p> <p>The examples below perform several operations using DataJoint queries:</p> <ul> <li>Use multiple restrictions to fetch one image into the variable <code>average_image</code>.</li> <li>Use a join operation and multiple restrictions to fetch mask coordinates from <code>imaging.Segmentation.Mask</code> and overlay these with the average image.</li> </ul> In\u00a0[37]: Copied! <pre>average_image = (\n    imaging.MotionCorrection.Summary &amp; session_key &amp; \"field_idx=0\"\n).fetch1(\"average_image\")\n</pre> average_image = (     imaging.MotionCorrection.Summary &amp; session_key &amp; \"field_idx=0\" ).fetch1(\"average_image\") <p>The query above restricts the <code>average_image</code> attribute of the <code>Summary</code> part table of the <code>MotionCorrection</code> table by the <code>session_key</code> variable containing primary key attributes of the current session and the <code>field_idx</code> attribute of the <code>Summary</code> table.</p> In\u00a0[38]: Copied! <pre>plt.imshow(average_image)\n</pre> plt.imshow(average_image) Out[38]: <pre>&lt;matplotlib.image.AxesImage at 0x7f937a966df0&gt;</pre> <p>The next query introduces the join feature of DataJoint queries. DataJoint tables can be joined to combine their primary key attributes and display the attributes of all the tables. It also performs 3 restrictions with the <code>&amp;</code> operator and simultaneously fetches the contents of 2 different attributes to two different variables.</p> <p>Let's see the full query and then build it up one step at a time:</p> <pre>mask_xpix, mask_ypix = (\n    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n    &amp; session_key\n    &amp; \"mask_center_z=0\"\n    &amp; \"mask_npix &gt; 130\"\n).fetch(\"mask_xpix\", \"mask_ypix\")\n</pre> <p>This query joins <code>imaging.Segmentation.Mask</code> with <code>imaging.MaskClassification.MaskType</code>. Below we will:</p> <ol> <li>See the contents of each table individually.</li> <li>See the contents of the two tables joined together without restrictions.</li> <li>See the contents of the two tables with the restrictions.</li> <li>Execute the query to fetch the data.</li> </ol> In\u00a0[39]: Copied! <pre>imaging.Segmentation.Mask()\n</pre> imaging.Segmentation.Mask() Out[39]: A mask produced by segmentation. <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>paramset_idx</p> Unique parameter set ID. <p>curation_id</p> <p>mask</p> <p>segmentation_channel</p> 0-based indexing <p>mask_npix</p> number of pixels in ROIs <p>mask_center_x</p> center x coordinate in pixel <p>mask_center_y</p> center y coordinate in pixel <p>mask_center_z</p> center z coordinate in pixel <p>mask_xpix</p> x coordinates in pixels <p>mask_ypix</p> y coordinates in pixels <p>mask_zpix</p> z coordinates in pixels <p>mask_weights</p> weights of the mask at the indices above subject1 2021-04-30 12:22:15 0 0 0 0 0 305 176 149 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 1 0 177 156 377 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 2 0 187 160 165 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 3 0 208 48 285 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 4 0 153 148 305 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 5 0 247 256 277 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 6 0 146 104 261 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 7 0 413 276 189 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 8 0 223 92 277 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 9 0 150 340 9 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 10 0 140 336 69 0 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2021-04-30 12:22:15 0 0 0 11 0 184 136 349 0 =BLOB= =BLOB= =BLOB= =BLOB= <p>...</p> <p>Total: 1276</p> In\u00a0[40]: Copied! <pre>imaging.MaskClassification.MaskType()\n</pre> imaging.MaskClassification.MaskType() Out[40]: <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>paramset_idx</p> Unique parameter set ID. <p>curation_id</p> <p>mask_classification_method</p> <p>mask</p> <p>mask_type</p> <p>confidence</p> subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 0 soma 0.81364subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 1 soma 0.850127subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 2 soma 0.747744subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 3 soma 0.856031subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 4 soma 0.985041subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 5 soma 0.825305subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 6 soma 0.99609subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 7 soma 0.947971subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 8 soma 0.963464subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 9 soma 0.913962subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 10 soma 0.951404subject1 2021-04-30 12:22:15 0 0 0 suite2p_default_classifier 11 soma 0.922488 <p>...</p> <p>Total: 481</p> In\u00a0[41]: Copied! <pre>imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n</pre> imaging.Segmentation.Mask * imaging.MaskClassification.MaskType Out[41]: <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>paramset_idx</p> Unique parameter set ID. <p>curation_id</p> <p>mask</p> <p>mask_classification_method</p> <p>segmentation_channel</p> 0-based indexing <p>mask_npix</p> number of pixels in ROIs <p>mask_center_x</p> center x coordinate in pixel <p>mask_center_y</p> center y coordinate in pixel <p>mask_center_z</p> center z coordinate in pixel <p>mask_xpix</p> x coordinates in pixels <p>mask_ypix</p> y coordinates in pixels <p>mask_zpix</p> z coordinates in pixels <p>mask_weights</p> weights of the mask at the indices above <p>mask_type</p> <p>confidence</p> subject1 2021-04-30 12:22:15 0 0 0 0 suite2p_default_classifier 0 305 176 149 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.81364subject1 2021-04-30 12:22:15 0 0 0 1 suite2p_default_classifier 0 177 156 377 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.850127subject1 2021-04-30 12:22:15 0 0 0 2 suite2p_default_classifier 0 187 160 165 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.747744subject1 2021-04-30 12:22:15 0 0 0 3 suite2p_default_classifier 0 208 48 285 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.856031subject1 2021-04-30 12:22:15 0 0 0 4 suite2p_default_classifier 0 153 148 305 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.985041subject1 2021-04-30 12:22:15 0 0 0 5 suite2p_default_classifier 0 247 256 277 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.825305subject1 2021-04-30 12:22:15 0 0 0 6 suite2p_default_classifier 0 146 104 261 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.99609subject1 2021-04-30 12:22:15 0 0 0 7 suite2p_default_classifier 0 413 276 189 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.947971subject1 2021-04-30 12:22:15 0 0 0 8 suite2p_default_classifier 0 223 92 277 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.963464subject1 2021-04-30 12:22:15 0 0 0 9 suite2p_default_classifier 0 150 340 9 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.913962subject1 2021-04-30 12:22:15 0 0 0 10 suite2p_default_classifier 0 140 336 69 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.951404subject1 2021-04-30 12:22:15 0 0 0 11 suite2p_default_classifier 0 184 136 349 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.922488 <p>...</p> <p>Total: 481</p> In\u00a0[42]: Copied! <pre>(\n    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n    &amp; session_key\n    &amp; \"mask_center_z=0\"\n    &amp; \"mask_npix &gt; 130\"\n)\n</pre> (     imaging.Segmentation.Mask * imaging.MaskClassification.MaskType     &amp; session_key     &amp; \"mask_center_z=0\"     &amp; \"mask_npix &gt; 130\" ) Out[42]: <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>paramset_idx</p> Unique parameter set ID. <p>curation_id</p> <p>mask</p> <p>mask_classification_method</p> <p>segmentation_channel</p> 0-based indexing <p>mask_npix</p> number of pixels in ROIs <p>mask_center_x</p> center x coordinate in pixel <p>mask_center_y</p> center y coordinate in pixel <p>mask_center_z</p> center z coordinate in pixel <p>mask_xpix</p> x coordinates in pixels <p>mask_ypix</p> y coordinates in pixels <p>mask_zpix</p> z coordinates in pixels <p>mask_weights</p> weights of the mask at the indices above <p>mask_type</p> <p>confidence</p> subject1 2021-04-30 12:22:15 0 0 0 0 suite2p_default_classifier 0 305 176 149 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.81364subject1 2021-04-30 12:22:15 0 0 0 1 suite2p_default_classifier 0 177 156 377 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.850127subject1 2021-04-30 12:22:15 0 0 0 2 suite2p_default_classifier 0 187 160 165 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.747744subject1 2021-04-30 12:22:15 0 0 0 3 suite2p_default_classifier 0 208 48 285 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.856031subject1 2021-04-30 12:22:15 0 0 0 4 suite2p_default_classifier 0 153 148 305 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.985041subject1 2021-04-30 12:22:15 0 0 0 5 suite2p_default_classifier 0 247 256 277 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.825305subject1 2021-04-30 12:22:15 0 0 0 6 suite2p_default_classifier 0 146 104 261 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.99609subject1 2021-04-30 12:22:15 0 0 0 7 suite2p_default_classifier 0 413 276 189 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.947971subject1 2021-04-30 12:22:15 0 0 0 8 suite2p_default_classifier 0 223 92 277 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.963464subject1 2021-04-30 12:22:15 0 0 0 9 suite2p_default_classifier 0 150 340 9 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.913962subject1 2021-04-30 12:22:15 0 0 0 10 suite2p_default_classifier 0 140 336 69 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.951404subject1 2021-04-30 12:22:15 0 0 0 11 suite2p_default_classifier 0 184 136 349 0 =BLOB= =BLOB= =BLOB= =BLOB= soma 0.922488 <p>...</p> <p>Total: 129</p> In\u00a0[43]: Copied! <pre>mask_xpix, mask_ypix = (\n    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n    &amp; session_key\n    &amp; \"mask_center_z=0\"\n    &amp; \"mask_npix &gt; 130\"\n).fetch(\"mask_xpix\", \"mask_ypix\")\n</pre> mask_xpix, mask_ypix = (     imaging.Segmentation.Mask * imaging.MaskClassification.MaskType     &amp; session_key     &amp; \"mask_center_z=0\"     &amp; \"mask_npix &gt; 130\" ).fetch(\"mask_xpix\", \"mask_ypix\") <p>Using this query, we've fetched the coordinates of segmented masks. We can overlay these masks onto our average image.</p> In\u00a0[44]: Copied! <pre>mask_image = np.zeros(np.shape(average_image), dtype=bool)\nfor xpix, ypix in zip(mask_xpix, mask_ypix):\n    mask_image[ypix, xpix] = True\n</pre> mask_image = np.zeros(np.shape(average_image), dtype=bool) for xpix, ypix in zip(mask_xpix, mask_ypix):     mask_image[ypix, xpix] = True In\u00a0[45]: Copied! <pre>plt.imshow(average_image)\nplt.contour(mask_image, colors=\"white\", linewidths=0.5);\n</pre> plt.imshow(average_image) plt.contour(mask_image, colors=\"white\", linewidths=0.5); <p>One more example using queries - plot fluorescence and deconvolved activity traces:</p> <p>Here we fetch the primary key attributes of the entry with <code>curation_id=0</code> for the current session in the <code>imaging.Curation</code> table.</p> <p>Then, we fetch all cells that fit the restriction criteria from <code>imaging.Segmentation.Mask</code> and <code>imaging.MaskClassification.MaskType</code> as a <code>projection</code>.</p> <p>We then use this projection as a restriction to fetch and plot fluorescence and deconvolved activity traces from the <code>imaging.Fluorescence.Trace</code> and <code>imaging.Activity.Trace</code> tables, respectively.</p> In\u00a0[46]: Copied! <pre>curation_key = (imaging.Curation &amp; session_key &amp; \"curation_id=0\").fetch1(\"KEY\")\nquery_cells = (\n    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n    &amp; curation_key\n    &amp; \"mask_center_z=0\"\n    &amp; \"mask_npix &gt; 130\"\n).proj()\n\nquery_cells\n</pre> curation_key = (imaging.Curation &amp; session_key &amp; \"curation_id=0\").fetch1(\"KEY\") query_cells = (     imaging.Segmentation.Mask * imaging.MaskClassification.MaskType     &amp; curation_key     &amp; \"mask_center_z=0\"     &amp; \"mask_npix &gt; 130\" ).proj()  query_cells Out[46]: <p>subject</p> <p>session_datetime</p> <p>scan_id</p> <p>paramset_idx</p> Unique parameter set ID. <p>curation_id</p> <p>mask</p> <p>mask_classification_method</p> subject1 2021-04-30 12:22:15 0 0 0 0 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 1 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 2 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 3 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 4 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 5 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 6 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 7 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 8 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 9 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 10 suite2p_default_classifiersubject1 2021-04-30 12:22:15 0 0 0 11 suite2p_default_classifier <p>...</p> <p>Total: 129</p> In\u00a0[47]: Copied! <pre>fluorescence_traces = (imaging.Fluorescence.Trace &amp; query_cells).fetch(\n    \"fluorescence\", order_by=\"mask\"\n)\n\nactivity_traces = (imaging.Activity.Trace &amp; query_cells).fetch(\n    \"activity_trace\", order_by=\"mask\"\n)\n\nsampling_rate = (scan.ScanInfo &amp; curation_key).fetch1(\"fps\")\n</pre> fluorescence_traces = (imaging.Fluorescence.Trace &amp; query_cells).fetch(     \"fluorescence\", order_by=\"mask\" )  activity_traces = (imaging.Activity.Trace &amp; query_cells).fetch(     \"activity_trace\", order_by=\"mask\" )  sampling_rate = (scan.ScanInfo &amp; curation_key).fetch1(\"fps\") In\u00a0[48]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(16, 4))\nax2 = ax.twinx()\n\nfor f, a in zip(fluorescence_traces, activity_traces):\n    ax.plot(np.r_[: f.size] * 1 / sampling_rate, f, \"k\", label=\"fluorescence trace\")\n    ax2.plot(\n        np.r_[: a.size] * 1 / sampling_rate,\n        a,\n        \"r\",\n        alpha=0.5,\n        label=\"deconvolved trace\",\n    )\n\n    break\n\nax.tick_params(labelsize=14)\nax2.tick_params(labelsize=14)\n\nax.legend(loc=\"upper left\", prop={\"size\": 14})\nax2.legend(loc=\"upper right\", prop={\"size\": 14})\n\nax.set_xlabel(\"Time (s)\")\nax.set_ylabel(\"Activity (a.u.)\")\nax2.set_ylabel(\"Activity (a.u.)\");\n</pre> fig, ax = plt.subplots(1, 1, figsize=(16, 4)) ax2 = ax.twinx()  for f, a in zip(fluorescence_traces, activity_traces):     ax.plot(np.r_[: f.size] * 1 / sampling_rate, f, \"k\", label=\"fluorescence trace\")     ax2.plot(         np.r_[: a.size] * 1 / sampling_rate,         a,         \"r\",         alpha=0.5,         label=\"deconvolved trace\",     )      break  ax.tick_params(labelsize=14) ax2.tick_params(labelsize=14)  ax.legend(loc=\"upper left\", prop={\"size\": 14}) ax2.legend(loc=\"upper right\", prop={\"size\": 14})  ax.set_xlabel(\"Time (s)\") ax.set_ylabel(\"Activity (a.u.)\") ax2.set_ylabel(\"Activity (a.u.)\"); <p>To run this tutorial notebook on your own data, please use the following steps:</p> <ul> <li>Download the mysql-docker image for DataJoint and run the container according to the instructions provide in the repository.</li> <li>Create a fork of this repository to your GitHub account.</li> <li>Clone the repository and open the files using your IDE.</li> <li>Add a code cell immediately after the first code cell in the notebook - we will setup the local connection using this cell. In this cell, type in the following code.</li> </ul> <pre>import datajoint as dj\ndj.config[\"database.host\"] = \"localhost\"\ndj.config[\"database.user\"] = \"&lt;your-username&gt;\"\ndj.config[\"database.password\"] = \"&lt;your-password&gt;\"\ndj.config[\"custom\"] = {\"imaging_root_data_dir\": \"path/to/your/data/dir\",\n\"database_prefix\": \"&lt;your-username_&gt;\"}\ndj.config.save_local()\ndj.conn()\n</pre> <ul> <li>Run this code cell and proceed with the rest of the notebook.</li> </ul>"}, {"location": "tutorials/tutorial/#process-calcium-imaging-data-with-datajoint-elements", "title": "Process calcium imaging data with DataJoint Elements\u00b6", "text": "<p>This notebook will walk through processing two-photon calcium imaging data collected from ScanImage and processed with Suite2p. While anyone can work through this notebook to process calcium imaging data through DataJoint's element-calcium-imaging pipeline, for a detailed tutorial about the fundamentals of DataJoint including table types, make functions, and querying, please see the DataJoint Tutorial.</p> <p>The DataJoint Python API and Element Calcium Imaging offer a lot of features to support collaboration, automation, reproducibility, and visualizations.</p> <p>For more information on these topics, please visit our documentation:</p> <ul> <li><p>DataJoint Core: General principles</p> </li> <li><p>DataJoint Python and MATLAB APIs: in-depth reviews of specifics</p> </li> <li><p>DataJoint Element Calcium Imaging: A modular pipeline for calcium imaging analysis</p> </li> </ul>"}, {"location": "tutorials/tutorial/#the-basics", "title": "The Basics:\u00b6", "text": "<p>Any DataJoint workflow can be broken down into basic 3 parts:</p> <ul> <li><code>Insert</code></li> <li><code>Populate</code> (or process)</li> <li><code>Query</code></li> </ul> <p>In this demo we will:</p> <ul> <li><code>Insert</code> metadata about an animal subject, recording session, and  parameters related to processing calcium imaging data through Suite2p.</li> <li><code>Populate</code> tables with outputs of image processing including motion correction, segmentation, mask classification, fluorescence traces and deconvolved activity traces.</li> <li><code>Query</code> the processed data from the database and plot calcium activity traces.</li> </ul> <p>Each of these topics will be explained thoroughly in this notebook.</p>"}, {"location": "tutorials/tutorial/#workflow-diagram", "title": "Workflow diagram\u00b6", "text": "<p>This workflow is assembled from 4 DataJoint elements:</p> <ul> <li>element-lab</li> <li>element-animal</li> <li>element-session</li> <li>element-calcium-imaging</li> </ul> <p>Each element declares its own schema in the database. These schemas can be imported like any other Python package. This workflow is composed of schemas from each of the Elements above and correspond to a module within <code>workflow_calcium_imaging.pipeline</code>.</p> <p>The schema diagram is a good reference for understanding the order of the tables within the workflow, as well as the corresponding table type. Let's activate the elements and view the schema diagram.</p>"}, {"location": "tutorials/tutorial/#diagram-breakdown", "title": "Diagram Breakdown\u00b6", "text": "<p>While the diagram above seems complex at first, it becomes more clear when it's approached as a hierarchy of tables that define the order in which the workflow expects to receive data in each of its tables.</p> <ul> <li>Tables with a green, or rectangular shape expect to receive data manually using the</li> </ul> <p><code>insert()</code> function.</p> <ul> <li>The tables higher up in the diagram such as <code>subject.Subject()</code></li> </ul> <p>should be the first to receive data. This ensures data integrity by preventing orphaned data within DataJoint schemas.</p> <ul> <li>Tables with a purple oval or red circle can be automatically filled with relevant data by calling <code>populate()</code>. For example <code>scan.ScanInfo</code> and its part-table <code>scan.ScanInfo.Field</code> are both populated with <code>scan.ScanInfo.populate()</code>.</li> <li>Tables connected by a solid line depend on attributes (entries) in the table above it</li> </ul>"}, {"location": "tutorials/tutorial/#table-types", "title": "Table Types\u00b6", "text": "<p>There are 5 table types in DataJoint. Each of these appear in the diagram above.</p> <ul> <li>Manual table: green box, manually inserted table, expect new entries daily, e.g. Subject, ProbeInsertion.</li> <li>Lookup table: gray box, pre inserted table, commonly used for general facts or parameters. e.g. Strain, ClusteringMethod, ClusteringParamSet.</li> <li>Imported table: blue oval, auto-processing table, the processing depends on the importing of external files. e.g. process of Clustering requires output files from kilosort2.</li> <li>Computed table: red circle, auto-processing table, the processing does not depend on files external to the database, commonly used for</li> <li>Part table: plain text, as an appendix to the master table, all the part entries of a given master entry represent a intact set of the master entry. e.g. Unit of a CuratedClustering.</li> </ul>"}, {"location": "tutorials/tutorial/#starting-the-workflow-insert", "title": "Starting the workflow: Insert\u00b6", "text": ""}, {"location": "tutorials/tutorial/#insert-entries-into-manual-tables", "title": "Insert entries into manual tables\u00b6", "text": "<p>To view details about a table's dependencies and attributes, use functions <code>.describe()</code> and <code>.heading</code>, respectively.</p> <p>Let's start with the first table in the schema diagram (the <code>subject</code> table) and view the table attributes we need to insert. There are two ways you can do this: run each of the two cells below</p>"}, {"location": "tutorials/tutorial/#populate", "title": "Populate\u00b6", "text": ""}, {"location": "tutorials/tutorial/#automatically-populate-tables", "title": "Automatically populate tables\u00b6", "text": "<p><code>scan.ScanInfo</code> is the first table in the pipeline that can be populated automatically. If a table contains a part table, this part table is also populated during the <code>populate()</code> call. <code>populate()</code> takes several arguments including the a session key. This key restricts <code>populate()</code> to performing the operation on the session of interest rather than all possible sessions which could be a time-intensive process for databases with lots of entries.</p> <p>Let's view the <code>scan.ScanInfo</code> and its part table <code>scan.ScanInfo.Field</code> and populate both through a single <code>populate()</code> call.</p>"}, {"location": "tutorials/tutorial/#query", "title": "Query\u00b6", "text": "<p>This section focuses on working with data that is already in the database.</p> <p>DataJoint queries allow you to view and import data from the database into a python variable using the <code>fetch()</code> method.</p> <p>There are several important features supported by <code>fetch()</code>:</p> <ul> <li>By default, an empty <code>fetch()</code> imports a list of dictionaries containing all attributes of all entries in the table that is queried.</li> <li><code>fetch1()</code>, on the other hand, imports a dictionary containing all attributes of one of the entries in the table. By default, if a table has multiple entries, <code>fetch1()</code> imports the first entry in the table.</li> <li>Both <code>fetch()</code> and <code>fetch1()</code> accept table attributes as an argument to query that particular attribute. For example <code>fetch1('fps')</code> will fetch the first value of the <code>fps</code> attribute if it exists in the table.</li> <li>Recommended best practice is to restrict queries by primary key attributes of the table to ensure the accuracy of imported data.<ul> <li>The most common restriction for entries in DataJoint tables is performed using the <code>&amp;</code> operator. For example to fetch all session start times belonging to <code>subject1</code>, a possible query could be <code>subject1_sessions = (session.Session &amp; \"subject = 'subject1'\").fetch(\"session_datetime\")</code>.</li> </ul> </li> <li><code>fetch()</code> can also be used to obtain the primary keys of a table. To fetch the primary keys of a table use <code>&lt;table_name&gt;.fetch(\"KEY\")</code> syntax.</li> </ul> <p>Let's walk through these concepts of querying by moving from simple to more complex queries.</p>"}]}